"""
Singleton Config System - 3ê°œ
í†µí•© í•™ìŠµ ì„¤ì • ê´€ë¦¬
"""

import os
import json
from dataclasses import dataclass, field, asdict
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
import torch


class ConfigSingleton:
    """ì‹±ê¸€í†¤ ë² ì´ìŠ¤ í´ëž˜ìŠ¤"""
    _instances = {}
    
    def __new__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super().__new__(cls)
        return cls._instances[cls]


@dataclass
class PropertyConfig(ConfigSingleton):
    """Property Prediction Transformer ì „ìš© ì„¤ì • (ì‹±ê¸€í†¤) - GPU ìµœì í™”"""
    # ë””ë°”ì´ìŠ¤ ì„¤ì • (CPU í…ŒìŠ¤íŠ¸ìš©)
    device: str = "cpu"  # CPU í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë³€ê²½
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜ - ì²´í¬í¬ì¸íŠ¸ì™€ í˜¸í™˜ë˜ë„ë¡ ì„¤ì •
    d_model: int = 256  # í†µí•©ëœ ê¸°ë³¸ê°’
    n_heads: int = 8
    n_layers: int = 6  # í†µí•©ëœ ê¸°ë³¸ê°’
    d_ff: int = 1024
    dropout: float = 0.1  # í†µí•©ëœ ê¸°ë³¸ê°’
    attention_mode: str = "advanced"
    use_rotary_pe: bool = True
    cross_attention_heads: int = 4  # SOTA ì„¤ì •
    
    # Property íŠ¹í™”
    property_dim: int = 3  # entanglement, fidelity, expressibility (í†µí•©)
    max_qubits: int = 10  # í†µí•©ëœ ê¸°ë³¸ê°’
    max_gates: int = 100  # í†µí•©ëœ ê¸°ë³¸ê°’
    
    # í•™ìŠµ ì„¤ì • (í†µí•©ëœ ê¸°ë³¸ê°’)
    learning_rate: float = 1e-4  # í†µí•©ëœ ê¸°ë³¸ê°’
    min_learning_rate: float = 1e-6  # ìµœì†Œ í•™ìŠµë¥ 
    train_batch_size: int = 32  # í†µí•©ëœ ê¸°ë³¸ê°’
    val_batch_size: int = 64  # í†µí•©ëœ ê¸°ë³¸ê°’
    grad_accum_steps: int = 4  # ê·¸ëž˜ë””ì–¸íŠ¸ ì¶•ì  ìŠ¤í… (effective batch = 16)
    weight_decay: float = 1e-4  # ë” ì•½í•œ ì •ê·œí™”
    num_epochs: int = 100
    scheduler_type: str = "cosine_with_restarts"  # ì£¼ê¸°ì  ìž¬ì‹œìž‘
    warmup_steps: int = 500  # ì›Œë°ì—… ì¶”ê°€
    patience: int = 10  # ì¡°ê¸° ì¢…ë£Œ ë°©ì§€
    
    # ê°€ì¤‘ì¹˜ - ê· í˜• ì¡°ì • (ì ì‘í˜•)
    entanglement_weight: float = 1.0
    fidelity_weight: float = 1.0
    expressibility_weight: float = 2.0  # ë” ì–´ë ¤ìš´ íƒœìŠ¤í¬ì— ë†’ì€ ê°€ì¤‘ì¹˜
    
    # SOTA ì„¤ì •
    consistency_loss_weight: float = 0.1
    numerical_stability: bool = True
    
    # ì ì‘í˜• í•™ìŠµ ì„¤ì •
    use_adaptive_weights: bool = True  # ì†ì‹¤ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
    gradient_clipping: float = 1.0  # ê·¸ëž˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    label_smoothing: float = 0.1  # ë¼ë²¨ ìŠ¤ë¬´ë”©
    
    def get_device(self) -> str:
        """ì‹¤ì œ ë””ë°”ì´ìŠ¤ ë°˜í™˜ (GPU ìš°ì„ )"""
        if self.device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return self.device


@dataclass
class DecisionConfig(ConfigSingleton):
    """Decision Transformer ì „ìš© ì„¤ì • (ì‹±ê¸€í†¤) - GPU ìµœì í™”"""
    # ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ìš°ì„ )
    device: str = "cuda"  # CPU í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë³€ê²½

    entanglement_weight: float = 10.0
    fidelity_weight: float = 0.1
    expressibility_weight: float = 0.1
    
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜
    d_model: int = 256
    n_heads: int = 8
    n_layers: int = 4
    d_ff: int = 1024
    dropout: float = 0.1
    attention_mode: str = "advanced"
    
    # Decision Transformer íŠ¹í™”
    max_seq_len: int = 500
    action_dim: int = 20
    state_dim: int = 50
    max_qubits: int = 50  # GPU ë©”ëª¨ë¦¬ ê³ ë ¤
    n_gate_types: int = 20  # ê²Œì´íŠ¸ íƒ€ìž… ìˆ˜
    
    # í•™ìŠµ ì„¤ì • (GPU ìµœì í™”)
    learning_rate: float = 5e-4
    train_batch_size: int = 256  # ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ê°ì†Œ
    val_batch_size: int = 256
    grad_accum_steps: int = 4  # ê·¸ëž˜ë””ì–¸íŠ¸ ì¶•ì  ìŠ¤í… (effective batch = 16)
    weight_decay: float = 1e-5
    num_epochs: int = 100
    warmup_steps: int = 1000
    gradient_clip_norm: float = 1.0
    scheduler_type: str = "cosine"  # "cosine" or "linear"
    use_rotary_pe: bool = True
    
    # GPU ìµœì í™” ì„¤ì •
    use_amp: bool = True  # Automatic Mixed Precision
    pin_memory: bool = True  # DataLoader ìµœì í™”
    num_workers: int = 4  # ë©€í‹°í”„ë¡œì„¸ì‹±
    
    # Logging ì„¤ì •
    use_wandb: bool = True
    wandb_project: str = "quantum-decision-transformer"
    log_interval: int = 100
    gradient_accumulation_steps: int = 1
    gradient_checkpointing: bool = True
    
    # ê²€ì¦ ë° ì €ìž¥
    log_every_n_steps: int = 100
    val_every_n_steps: int = 500
    save_every_n_steps: int = 1000
    save_dir: str = "checkpoints"
    memory_cleanup_interval = 100
    
    def get_device(self) -> str:
        """ì‹¤ì œ ë””ë°”ì´ìŠ¤ ë°˜í™˜ (GPU ìš°ì„ )"""
        if self.device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return self.device


@dataclass
class DefaultConfig(ConfigSingleton):
    """ê¸°ë³¸ ê³µí†µ ì„¤ì • (ì‹±ê¸€í†¤) - GPU ìµœì í™”"""
    # ë””ë°”ì´ìŠ¤ (CPU í…ŒìŠ¤íŠ¸ìš©)
    device: str = "cpu"  # CPU í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë³€ê²½
    
    # ë°ì´í„°
    train_ratio: float = 0.7
    val_ratio: float = 0.15
    test_ratio: float = 0.15
    
    # CPU í…ŒìŠ¤íŠ¸ ì„¤ì • (GPU ìµœì í™” êµ¬ì¡° ìœ ì§€)
    use_amp: bool = False  # CPUì—ì„œëŠ” AMP ë¹„í™œì„±í™”
    pin_memory: bool = False  # CPUì—ì„œëŠ” pin_memory ë¹„í™œì„±í™”
    num_workers: int = 2  # CPUìš© ê°ì†Œ
    prefetch_factor: int = 2  # ìœ ì§€
    
    # ë°°ì¹˜ í¬ê¸° (CPU ë©”ëª¨ë¦¬ì— ë§žê²Œ ì¡°ì •)
    train_batch_size: int = 16  # CPUìš© ê°ì†Œ
    val_batch_size: int = 16   # CPUìš© ê°ì†Œ
    
    # ìºì‹± (GPU ë©”ëª¨ë¦¬ ê³ ë ¤)
    enable_cache: bool = True
    cache_dir: str = "cache"
    max_cache_size_gb: float = 4.0  # GPU ë©”ëª¨ë¦¬ ì œí•œ
    
    def get_device(self) -> str:
        """ì‹¤ì œ ë””ë°”ì´ìŠ¤ ë°˜í™˜ (GPU ìš°ì„ )"""
        if self.device == "auto":
            if torch.cuda.is_available():
                print(f"ðŸš€ Using GPU: {torch.cuda.get_device_name()}")
                return "cuda"
            else:
                print("âš ï¸ CUDA not available, falling back to CPU")
                return "cpu"
        return self.device


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì ‘ê·¼ í•¨ìˆ˜ë“¤
def get_property_config() -> PropertyConfig:
    """Property ì„¤ì • ì‹±ê¸€í†¤ ë°˜í™˜"""
    return PropertyConfig()

def get_decision_config() -> DecisionConfig:
    """Decision ì„¤ì • ì‹±ê¸€í†¤ ë°˜í™˜"""
    return DecisionConfig()

def get_default_config() -> DefaultConfig:
    """ê¸°ë³¸ ì„¤ì • ì‹±ê¸€í†¤ ë°˜í™˜"""
    return DefaultConfig()


# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ëž˜í¼ë“¤
def create_property_prediction_config(size: str = "medium", attention_mode: str = "advanced") -> PropertyConfig:
    """ì‚¬ìš©ìž ì¸ìžì— ë§žì¶° Property ì„¤ì • ìƒì„±"""
    from config.experiment_configs import MODEL_SIZES
    
    config = get_property_config()
    
    # ì‚¬ìš©ìžê°€ ì§€ì •í•œ í¬ê¸°ì— ë§žì¶° ì„¤ì • ì—…ë°ì´íŠ¸
    if size in MODEL_SIZES:
        size_config = MODEL_SIZES[size]
        config.d_model = size_config["d_model"]
        config.n_heads = size_config["n_heads"] 
        config.n_layers = size_config["n_layers"]
        config.d_ff = size_config["d_ff"]
        config.dropout = size_config["dropout"]
        config.train_batch_size = size_config["batch_size"]
        config.val_batch_size = size_config["batch_size"]
    
    # ì–´í…ì…˜ ëª¨ë“œ ì„¤ì •
    config.attention_mode = attention_mode
    
    return config


# ë ˆê±°ì‹œ í´ëž˜ìŠ¤ë“¤ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
ModelArchitectureConfig = DecisionConfig
TrainingConfig = DecisionConfig
PropertyPredictionConfig = PropertyConfig


@dataclass
class DataConfig:
    """Data configuration"""
    # Dataset paths
    data_path: str = "dummy_experiment_results.json"
    train_split: float = 0.8
    val_split: float = 0.1
    test_split: float = 0.1
    
    # Data processing
    max_circuit_length: int = 1000
    normalize_targets: bool = True
    augment_data: bool = False
    
    # Caching
    use_cache: bool = True
    cache_dir: str = "cache"


@dataclass
class ExperimentConfig:
    """Experiment configuration"""
    # Experiment metadata
    experiment_name: str = "default_experiment"
    description: str = ""
    tags: list = field(default_factory=list)
    
    # Output directories
    output_dir: str = "experiments"
    checkpoint_dir: str = "checkpoints"
    log_dir: str = "logs"
    
    # Reproducibility
    seed: int = 42
    deterministic: bool = True


@dataclass
class UnifiedTrainingConfig:
    """í†µí•© í•™ìŠµ ì„¤ì •"""
    
    # ëª¨ë¸ ì„¤ì •
    model: DecisionConfig = field(default_factory=lambda: get_decision_config())
    
    # í•™ìŠµ ì„¤ì •
    training: DecisionConfig = field(default_factory=lambda: get_decision_config())
    
    # ë°ì´í„° ì„¤ì •
    data: DataConfig = field(default_factory=DataConfig)
    
    # ì‹¤í—˜ ì„¤ì •
    experiment: ExperimentConfig = field(default_factory=ExperimentConfig)

    # RTG ì„¤ì •
    enable_rtg: bool = False
    property_model_size: str = "small"
    property_attention_mode: str = "advanced"
    
    def save(self, path: Union[str, Path]):
        """Save configuration to JSON file"""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'w') as f:
            json.dump(asdict(self), f, indent=2)
    
    @classmethod
    def load(cls, path: Union[str, Path]) -> 'UnifiedTrainingConfig':
        """Load configuration from JSON file"""
        with open(path, 'r') as f:
            data = json.load(f)
        
        # Reconstruct nested dataclasses
        config = cls()
        
        if 'model' in data:
            config.model = ModelArchitectureConfig(**data['model'])
        if 'training' in data:
            config.training = TrainingConfig(**data['training'])
        if 'decision_transformer' in data:
            config.decision_transformer = DecisionTransformerConfig(**data['decision_transformer'])
        if 'property_predictor' in data:
            config.property_predictor = PropertyPredictorConfig(**data['property_predictor'])
        if 'data' in data:
            config.data = DataConfig(**data['data'])
        if 'experiment' in data:
            config.experiment = ExperimentConfig(**data['experiment'])
        
        return config
    
    def update_from_dict(self, updates: Dict[str, Any]):
        """Update configuration from dictionary"""
        for section, values in updates.items():
            if hasattr(self, section):
                section_config = getattr(self, section)
                for key, value in values.items():
                    if hasattr(section_config, key):
                        setattr(section_config, key, value)
    
    def get_model_config_for_decision_transformer(self) -> Dict[str, Any]:
        """Get model configuration for Decision Transformer"""
        return {
            'd_model': self.model.d_model,
            'n_heads': self.model.n_heads,
            'n_layers': self.model.n_layers,
            'd_ff': self.model.d_ff,
            'dropout': self.model.dropout,
            'max_qubits': self.model.max_qubits,
            'n_gate_types': self.model.n_gate_types,
            'attention_mode': self.model.attention_mode,
            'device': self.model.get_device()
        }
    
    def get_model_config_for_property_predictor(self) -> Dict[str, Any]:
        """Get model configuration for Property Predictor"""
        return {
            'd_model': self.model.d_model,
            'n_heads': self.model.n_heads,
            'n_layers': self.model.n_layers,
            'd_ff': self.model.d_ff,
            'dropout': self.model.dropout,
            'max_qubits': self.model.max_qubits,
            'max_gates': self.model.max_gates,
            'device': self.model.get_device()
        }
    
    def setup_directories(self):
        """Create necessary directories"""
        dirs = [
            self.experiment.output_dir,
            self.experiment.checkpoint_dir,
            self.experiment.log_dir,
            self.data.cache_dir
        ]
        
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    def set_seed(self):
        """Set random seeds for reproducibility"""
        import random
        import numpy as np
        
        random.seed(self.experiment.seed)
        np.random.seed(self.experiment.seed)
        torch.manual_seed(self.experiment.seed)
        
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.experiment.seed)
            torch.cuda.manual_seed_all(self.experiment.seed)
        
        if self.experiment.deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False


# Predefined experiment configurations
def get_small_experiment_config() -> UnifiedTrainingConfig:
    """Small experiment for quick testing"""
    config = UnifiedTrainingConfig()
    
    # Small model
    config.model.d_model = 256
    config.model.n_layers = 4
    config.model.n_heads = 4
    config.model.d_ff = 1024
    
    # Fast training
    config.training.num_epochs = 50
    config.training.train_batch_size = 64
    config.training.val_every_n_steps = 100
    config.training.save_every_n_steps = 200
    
    config.experiment.experiment_name = "small_test"
    config.experiment.description = "Small model for quick testing"
    
    return config


def get_medium_experiment_config() -> UnifiedTrainingConfig:
    """Medium experiment for development"""
    config = UnifiedTrainingConfig()
    
    # Medium model (default values are already medium)
    config.experiment.experiment_name = "medium"
    config.experiment.description = "Medium model for development"
    
    return config


def get_large_experiment_config() -> UnifiedTrainingConfig:
    """Large experiment for production"""
    config = UnifiedTrainingConfig()
    
    # Large model
    config.model.d_model = 768
    config.model.n_layers = 12
    config.model.n_heads = 12
    config.model.d_ff = 3072
    config.model.max_qubits = 16
    
    # Intensive training
    config.training.num_epochs = 200
    config.training.train_batch_size = 64
    config.training.learning_rate = 5e-5
    config.training.warmup_steps = 2000
    
    config.experiment.experiment_name = "large_production"
    config.experiment.description = "Large model for production use"
    
    return config


def get_config_by_name(name: str) -> UnifiedTrainingConfig:
    """Get predefined configuration by name"""
    configs = {
        'small': get_small_experiment_config,
        'medium': get_medium_experiment_config,
        'large': get_large_experiment_config
    }
    
    if name not in configs:
        raise ValueError(f"Unknown config name: {name}. Available: {list(configs.keys())}")
    
    return configs[name]()


# Configuration manager class
class ConfigManager:
    """Centralized configuration management"""
    
    def __init__(self, config_dir: str = "configs"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)
    
    def save_config(self, config: UnifiedTrainingConfig, name: str):
        """Save configuration with a name"""
        config_path = self.config_dir / f"{name}.json"
        config.save(config_path)
        print(f"Configuration saved to {config_path}")
    
    def load_config(self, name: str) -> UnifiedTrainingConfig:
        """Load configuration by name"""
        config_path = self.config_dir / f"{name}.json"
        if not config_path.exists():
            raise FileNotFoundError(f"Configuration file not found: {config_path}")
        
        return UnifiedTrainingConfig.load(config_path)
    
    def list_configs(self) -> list:
        """List available configuration files"""
        return [f.stem for f in self.config_dir.glob("*.json")]
    
    def create_experiment_config(self, 
                               base_config: str = "medium",
                               experiment_name: str = None,
                               overrides: Dict[str, Any] = None) -> UnifiedTrainingConfig:
        """Create experiment configuration with overrides"""
        # Get base configuration
        config = get_config_by_name(base_config)
        
        # Set experiment name
        if experiment_name:
            config.experiment.experiment_name = experiment_name
        
        # Apply overrides
        if overrides:
            config.update_from_dict(overrides)
        
        # Setup experiment
        config.setup_directories()
        config.set_seed()
        
        return config


if __name__ == "__main__":
    # Example usage
    print("ðŸ”§ Unified Training Configuration System")
    
    # Create config manager
    manager = ConfigManager()
    
    # Create and save different experiment configs
    small_config = get_small_experiment_config()
    manager.save_config(small_config, "small_test")
    
    medium_config = get_medium_experiment_config()
    manager.save_config(medium_config, "medium_dev")
    
    large_config = get_large_experiment_config()
    manager.save_config(large_config, "large_production")
    
    print(f"Available configurations: {manager.list_configs()}")
    
    # Example of creating custom experiment
    custom_config = manager.create_experiment_config(
        base_config="medium",
        experiment_name="custom_experiment",
        overrides={
            "model": {"d_model": 384, "n_layers": 8},
            "training": {"learning_rate": 2e-4, "num_epochs": 50}
        }
    )
    
    print(f"Custom experiment created: {custom_config.experiment.experiment_name}")
    print(f"Model d_model: {custom_config.model.d_model}")
    print(f"Training epochs: {custom_config.training.num_epochs}")
