"""
Decision Transformer Model
ê°„ë‹¨í•˜ê³  í™•ì¥ì„± ë†’ì€ Decision Transformer êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import math
import numpy as np

# Focal Loss for addressing class imbalance in classification tasks
class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance in classification tasks"""
    
    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, ignore_index: int = -100):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ignore_index = ignore_index
        
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: [N, C] logits
            targets: [N] class indices
        """
        # ê¸°ë³¸ CrossEntropy ê³„ì‚°
        ce_loss = F.cross_entropy(inputs, targets, ignore_index=self.ignore_index, reduction='none')
        
        # p_t ê³„ì‚° (ì •ë‹µ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ )
        pt = torch.exp(-ce_loss)
        
        # Focal Loss ê³„ì‚°: Î± * (1-p_t)^Î³ * CE_loss
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        return focal_loss.mean()

from dataclasses import dataclass
from pathlib import Path

# ë™ì  ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
import sys
from pathlib import Path

# í˜„ì¬ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ì„ ê²½ë¡œì— ì¶”ê°€
current_file = Path(__file__).resolve()
src_dir = current_file.parent.parent
project_root = src_dir.parent.parent
sys.path.insert(0, str(src_dir))
sys.path.insert(0, str(project_root / "quantumcommon"))

# ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from encoding.modular_quantum_attention import ModularQuantumAttention, AttentionMode
except ImportError:
    try:
        from ..encoding.modular_quantum_attention import ModularQuantumAttention, AttentionMode
    except ImportError:
        from OAT_Model.src.encoding.modular_quantum_attention import ModularQuantumAttention, AttentionMode

# Property Prediction ëª¨ë¸ ì„í¬íŠ¸
try:
    from models.unified_property_prediction_transformer import UnifiedPropertyPredictionTransformer
except ImportError:
    try:
        from .unified_property_prediction_transformer import UnifiedPropertyPredictionTransformer
    except ImportError:
        from OAT_Model.src.models.unified_property_prediction_transformer import UnifiedPropertyPredictionTransformer

# ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì„í¬íŠ¸
try:
    from gates import QuantumGateRegistry
except ImportError:
    try:
        from quantumcommon.gates import QuantumGateRegistry
    except ImportError:
        print("Warning: Could not import QuantumGateRegistry")
        QuantumGateRegistry = None

# Debug utilities import
try:
    from utils.debug_utils import debug_print
except ImportError:
    try:
        from ..utils.debug_utils import debug_print
    except ImportError:
        def debug_print(*args, **kwargs):
            print(*args, **kwargs)

# ğŸ—‘ï¸ REMOVED: Legacy MultiHeadAttention class - now using ModularAttention system


class TransformerBlock(nn.Module):
    """íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ (ëª¨ë“ˆëŸ¬ ì–‘ì ì–´í…ì…˜ + í¬ë¡œìŠ¤ ì–´í…ì…˜ ì§€ì›)"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, 
                 attention_mode: str = "standard", enable_cross_attention: bool = False):
        super().__init__()
        
        # ëª¨ë“ˆëŸ¬ ì–‘ì ì–´í…ì…˜ ì‚¬ìš© (Self-Attention)
        mode = AttentionMode.ADVANCED if attention_mode == "advanced" else AttentionMode.STANDARD
        self.self_attention = ModularQuantumAttention(
            d_model=d_model,
            num_heads=n_heads,
            mode=mode
        )
        self.attention_mode = attention_mode
        self.enable_cross_attention = enable_cross_attention
        
        # Cross-Attention ì¶”ê°€ (ì„ íƒì )
        if enable_cross_attention:
            self.cross_attention = nn.MultiheadAttention(
                embed_dim=d_model,
                num_heads=n_heads,
                dropout=dropout,
                batch_first=True
            )
            self.norm_cross = nn.LayerNorm(d_model, eps=1e-6)
            self.dropout_cross = nn.Dropout(dropout)
        
        # ë¬¸ì œ 6 í•´ê²°: í”¼ë“œí¬ì›Œë“œ ì •ê·œí™” ìµœì í™” (ê³¼ë„í•œ ì •ê·œí™” ì œê±°)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),  # ì¤‘ê°„ dropoutë§Œ ìœ ì§€
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # Pre-norm êµ¬ì¡° (ì•ˆì •ì  epsilon)
        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        # ë¬¸ì œ 4 í•´ê²°: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° ë³µì›
        self.scale = nn.Parameter(torch.ones(1) * 0.5)  # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor, 
                graph_data: Optional[Dict] = None, encoder_output: Optional[torch.Tensor] = None,
                encoder_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Remove debug prints that access tensor values during training
        
        # Device synchronization - ensure x is on same device as model
        model_device = next(self.parameters()).device
        if x.device != model_device:
            # Moving x to model device
            x = x.to(model_device)
        
        # Also ensure mask is on same device
        if mask is not None and mask.device != model_device:
            # Moving mask to model device
            mask = mask.to(model_device)
        
        # 1. Self-Attention
        norm_x = self.norm1(x)
        # Remove debug prints that access tensor values during training
        
        # ëª¨ë“ˆëŸ¬ ì–‘ì ì–´í…ì…˜ ì ìš© (Self-Attention)
        self_attn_out = self.self_attention(norm_x, graph_data=graph_data)
        # Remove debug prints that access tensor values during training
        
        dropout_self_attn = self.dropout1(self_attn_out)
        scaled_self_attn = self.scale * dropout_self_attn
        x = x + scaled_self_attn
        # Remove debug prints that access tensor values during training
        
        # 2. Cross-Attention (ì„ íƒì )
        if self.enable_cross_attention and encoder_output is not None:
            norm_x_cross = self.norm_cross(x)
            # Cross-attention processing
            
            cross_attn_out, cross_attn_weights = self.cross_attention(
                query=norm_x_cross,
                key=encoder_output,
                value=encoder_output,
                key_padding_mask=encoder_mask,
                need_weights=True
            )
            # Remove debug prints that access tensor values during training
            
            dropout_cross_attn = self.dropout_cross(cross_attn_out)
            scaled_cross_attn = self.scale * dropout_cross_attn
            x = x + scaled_cross_attn
            # Remove debug prints that access tensor values during training
        
        # 3. Feed Forward
        norm_x = self.norm2(x)
        # Remove debug prints that access tensor values during training
        
        ff_out = self.feed_forward(norm_x)
        # Remove debug prints that access tensor values during training
        
        dropout_ff = self.dropout2(ff_out)
        scaled_ff = self.scale * dropout_ff
        x = x + scaled_ff
        # Remove debug prints that access tensor values during training
        
        return x
    
    def set_attention_mode(self, mode: str):
        """ì–´í…ì…˜ ëª¨ë“œ ë³€ê²½"""
        attention_mode = AttentionMode.ADVANCED if mode.lower() == "advanced" else AttentionMode.STANDARD
        self.attention.set_mode(attention_mode)
        self.attention_mode = mode
        debug_print(f"TransformerBlock attention mode changed to: {mode}")


class DecisionTransformer(nn.Module):
    """Decision Transformer ëª¨ë¸"""
    
    def __init__(
        self,
        d_model: int = 512,
        n_layers: int = 6,
        n_heads: int = 8,
        d_ff: int = 2048,
        n_gate_types: int = None,  # ğŸ† NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •
        max_qubits: int = 50,  # ğŸ† NEW: ìµœëŒ€ íë¹— ìˆ˜
        position_dim: int = None,  # ğŸ† NEW: ìœ„ì¹˜ ì˜ˆì¸¡ ì¶œë ¥ ì°¨ì› (ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±ìš©)
        dropout: float = 0.1,
        attention_mode: str = "advanced",  # ğŸ† NEW: ì–´í…ì…˜ ëª¨ë“œ ì„ íƒ
        device: str = "cpu",  # ğŸ† NEW: ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
        property_prediction_model: Optional[UnifiedPropertyPredictionTransformer] = None
    ):
        super().__init__()
        
        self.d_model = d_model
        self.max_qubits = max_qubits
        self.device = device
        
        # Use the position_dim from the parameter if provided, otherwise use default
        if position_dim is not None:
            self.position_dim = position_dim
        else:
            self.position_dim = d_model // 4
        
        # ğŸ† Gate vocabulary configuration:
        # - Embedding: 22 gates (including special tokens [EOS], [PAD])
        # - Prediction head: 20 gates (excluding special tokens)
        self.gate_registry = QuantumGateRegistry()
        
        # Full vocab for embedding (22 gates including special tokens)
        self.n_embedding_gates = len(self.gate_registry.get_full_gate_vocab())
        
        # Reduced vocab for prediction (20 gates excluding special tokens)  
        self.n_prediction_gates = len(self.gate_registry.get_gate_vocab())
        
        # Use prediction gates for action heads
        if n_gate_types is None:
            self.n_gate_types = self.n_prediction_gates  # 20 gates for prediction
            debug_print(f"ğŸ† DecisionTransformer: Embedding={self.n_embedding_gates}, Prediction={self.n_prediction_gates}")
        else:
            self.n_gate_types = n_gate_types
            debug_print(f" DecisionTransformer: Using manual n_gate_types = {self.n_gate_types}")
        
        self.attention_mode = attention_mode  # NEW: ì–´í…ì…˜ ëª¨ë“œ ì €ì¥
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ë“¤ ìƒì„± (Causal Decoder-Only êµ¬ì¡°)
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(
                d_model, n_heads, d_ff, dropout, attention_mode,
                enable_cross_attention=True  # ì¸ê³¼ì  ë””ì½”ë”ë§Œ ì‚¬ìš©
            )
            for i in range(n_layers)
        ])
        
        # Cross-attention for integrating quantum circuit constraints
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            dropout=dropout,
            batch_first=True
        )
        self.cross_attention_norm = nn.LayerNorm(d_model, eps=1e-6)
        self.cross_attention_dropout = nn.Dropout(dropout)
        
        debug_print(f"ğŸ† DecisionTransformer: Gate prediction with property-based RTG rewards")
        
        # Action prediction heads (ê²Œì´íŠ¸, ìœ„ì¹˜, íŒŒë¼ë¯¸í„°) - RESTORED
        self.action_heads = nn.ModuleDict({
            'gate': nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(),
                nn.Linear(d_model // 2, self.n_gate_types)  # ë¶„ë¥˜: ê²Œì´íŠ¸ íƒ€ì… ì˜ˆì¸¡
            ),
            'position': nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(), 
                nn.Linear(d_model // 2, 2 * self.position_dim)  # ë¶„ë¥˜: [qubit1, qubit2] ì˜ˆì¸¡
            ),
            'parameter': nn.Sequential(
                nn.Linear(d_model, d_model // 4),
                nn.GELU(),
                nn.Linear(d_model // 4, 1)  # íšŒê·€: ê²Œì´íŠ¸ íŒŒë¼ë¯¸í„° ì˜ˆì¸¡
            )
        })
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ë§¤ìš° ë³´ìˆ˜ì )
        self.apply(self._conservative_init_weights)
        
        # Cross-attention ì‹œê°í™” í”Œë˜ê·¸
        self.visualize_cross_attention = False
        
    def _conservative_init_weights(self, module):
        """ë¬¸ì œ 5 í•´ê²°: ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì•ˆì •ì„± + í•™ìŠµ ëŠ¥ë ¥)"""
        if isinstance(module, nn.Linear):
            # ë¬¸ì œ 5 í•´ê²°: gain 0.1 â†’ 0.5ë¡œ ì¦ê°€ (ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë°©ì§€)
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            # ì„ë² ë”© ì´ˆê¸°í™”ë„ ì•½ê°„ ì¦ê°€
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(
        self,
        input_sequence: torch.Tensor,
        attention_mask: torch.Tensor,
        action_prediction_mask: torch.Tensor,
        circuit_constraints: Optional[torch.Tensor] = None,
        num_qubits: Optional[List[int]] = None,
        num_gates: Optional[List[int]] = None
    ) -> torch.Tensor:
        """
        Clean Decision Transformer Forward Pass
        SAR ì‹œí€€ìŠ¤ëŠ” ë°°ì¹˜ ì½œë ˆì´í„°ì—ì„œ ì´ë¯¸ êµ¬ì„±ë¨
        
        Args:
            input_sequence: [batch, sar_seq_len, d_model] - ì´ë¯¸ SAR êµ¬ì„±ëœ ì‹œí€€ìŠ¤
            attention_mask: [batch, sar_seq_len, sar_seq_len]
            action_prediction_mask: [batch, sar_seq_len] - Action ìœ„ì¹˜ë§Œ True
        
        Returns:
            Property predictions for action positions
        """
        # Remove debug prints that access tensor values during training
        
        # ë””ë°”ì´ìŠ¤ ë™ê¸°í™”
        device = input_sequence.device
        attention_mask = attention_mask.to(device)
        action_prediction_mask = action_prediction_mask.to(device)
        
        batch_size, seq_len, _ = input_sequence.shape
        
        # ì…ë ¥ ì‹œí€€ìŠ¤ ì²˜ë¦¬
        x = self.dropout(input_sequence)
        
        # ê·¸ë˜í”„ ë°ì´í„° ì²˜ë¦¬ (ì–‘ì íšŒë¡œ ì œì•½ ì •ë³´) - ë””ë°”ì´ìŠ¤ ë™ê¸°í™”
        graph_data = None
        if circuit_constraints is not None:
            circuit_constraints = circuit_constraints.to(device)  # ë””ë°”ì´ìŠ¤ ë™ê¸°í™”
            graph_data = {
                'num_qubits': num_qubits,
                'num_gates': num_gates
            }
        
        # Transformer blocks with causal attention
        for i, block in enumerate(self.transformer_blocks):
            x = block(x, attention_mask, graph_data)
            
            # Optional circuit constraints integration
            if circuit_constraints is not None:
                x, _ = self.integrate_quantum_constraints(
                    sequence_features=x,
                    constraint_features=circuit_constraints
                )
        
        # Gate prediction heads - predict next gate at action positions
        predictions = {}
        for head_name, head in self.action_heads.items():
            predictions[head_name] = head(x)  # [batch, seq_len, output_dim]
        
        return predictions
    
    
    def _apply_dynamic_qubit_masking(
        self, 
        predictions: Dict[str, torch.Tensor], 
        num_qubits: List[int], 
        action_mask: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        ğŸš€ ë°°ì¹˜ë³„ íë¹— ìˆ˜ì— ë”°ë¥¸ ë™ì  ë§ˆìŠ¤í‚¹ ì ìš©
        
        Args:
            predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
            num_qubits: ê° íšŒë¡œì˜ íë¹— ìˆ˜ [batch_size]
            action_mask: ì•¡ì…˜ ë§ˆìŠ¤í¬ [batch, num_actions]
        
        Returns:
            ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ ì˜ˆì¸¡ ê²°ê³¼
        """
        batch_size = len(num_qubits)
        position_preds = predictions['position_preds']  # [batch, num_actions, max_qubits, 2]
        
        # ë°°ì¹˜ í¬ê¸° ê²€ì¦
        if position_preds.shape[0] != batch_size:
            raise ValueError(
                f"âŒ CRITICAL ERROR: ë°°ì¹˜ í¬ê¸° ë¶ˆì¼ì¹˜!\n"
                f"   position_preds batch size: {position_preds.shape[0]}\n"
                f"   num_qubits length: {batch_size}"
            )
        
        # ê° íšŒë¡œë³„ë¡œ ë™ì  ë§ˆìŠ¤í‚¹ ì ìš©
        for batch_idx, circuit_qubits in enumerate(num_qubits):
            # íë¹— ìˆ˜ ê²€ì¦
            if circuit_qubits <= 0 or circuit_qubits > self.max_qubits:
                raise ValueError(
                    f"âŒ CRITICAL ERROR: íšŒë¡œ {batch_idx}ì˜ íë¹— ìˆ˜ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
                    f"   circuit_qubits: {circuit_qubits}\n"
                    f"   max_qubits: {self.max_qubits}"
                )
            
            # ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ë¥¼ -infë¡œ ë§ˆìŠ¤í‚¹ (softmaxì—ì„œ í™•ë¥  0)
            # position_preds[batch_idx, :, circuit_qubits:, :] = float('-inf')
            if circuit_qubits < self.max_qubits:
                position_preds[batch_idx, :, circuit_qubits:, :] = float('-inf')
        
        predictions['position_preds'] = position_preds
        return predictions
    
    def compute_loss(
        self,
        predictions: Dict[str, torch.Tensor],
        targets: Dict,
        action_prediction_mask: torch.Tensor,
        num_qubits: Optional[List[int]] = None,
        num_gates: Optional[List[int]] = None
    ) -> Dict[str, torch.Tensor]:
        """
        ğŸ¯ ë³„ë„ì˜ ì†ì‹¤ ê³„ì‚° ë©”ì„œë“œ (ì˜ˆì¸¡ê³¼ ë¶„ë¦¬)
        
        Args:
            predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
            targets: íƒ€ê²Ÿ ë°ì´í„°
            action_prediction_mask: ì•¡ì…˜ ì˜ˆì¸¡ ë§ˆìŠ¤í¬
            num_qubits: ë°°ì¹˜ë³„ íë¹— ìˆ˜ ì •ë³´
        
        Returns:
            ì†ì‹¤ ê³„ì‚° ê²°ê³¼ dict
        """
        loss_computer = ActionLossComputer()
        
        # EOS ë§ˆìŠ¤í¬ ìƒì„± (í•„ìš”ì‹œ)
        # Note: For decision transformer, we use reduced vocab (no special tokens)
        # So EOS tokens should not appear in targets, but we keep this for safety
        eos_mask = None
        
        # ë§ˆìŠ¤í¬ ê²°í•©
        combined_mask = action_prediction_mask
        if eos_mask is not None:
            combined_mask = action_prediction_mask & eos_mask
        
        # ì†ì‹¤ ê³„ì‚°
        return loss_computer.compute(
            predictions=predictions,
            targets=targets,
            mask=combined_mask,
            num_qubits=num_qubits,
            num_gates=num_gates
        )
    
    def set_attention_mode(self, mode: str):
        """ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ ì–´í…ì…˜ ëª¨ë“œ ë³€ê²½"""
        self.attention_mode = mode
        for block in self.transformer_blocks:
            block.set_attention_mode(mode)
        debug_print(f"DecisionTransformer attention mode changed to: {mode}")
    
    def enable_cross_attention_visualization(self, enable: bool = True):
        """í¬ë¡œìŠ¤ ì–´í…ì…˜ ì‹œê°í™” í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.visualize_cross_attention = enable
        if enable:
            debug_print("Cross-attention visualization enabled")
        else:
            debug_print("Cross-attention visualization disabled")
    
    def integrate_quantum_constraints(self, 
                                   sequence_features: torch.Tensor,
                                   constraint_features: torch.Tensor,
                                   constraint_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ì–‘ì íšŒë¡œ ì œì•½ ì •ë³´ë¥¼ ì‹œí€€ìŠ¤ íŠ¹ì§•ê³¼ í†µí•©
        
        Args:
            sequence_features: [batch, seq_len, d_model] - ì‹œí€€ìŠ¤ íŠ¹ì§•
            constraint_features: [batch, constraint_len, d_model] - ì œì•½ íŠ¹ì§•
            constraint_mask: [batch, constraint_len] - ì œì•½ ë§ˆìŠ¤í¬
        
        Returns:
            í†µí•©ëœ ì‹œí€€ìŠ¤ íŠ¹ì§•
        """
        # Pre-norm for cross-attention
        sequence_norm = self.cross_attention_norm(sequence_features)
        
        # Cross-attention: query=sequence, key=value=constraints
        integrated_features, attention_weights = self.cross_attention(
            query=sequence_norm,
            key=constraint_features,
            value=constraint_features,
            key_padding_mask=constraint_mask,
            need_weights=True
        )
        
        # Residual connection with dropout
        output = sequence_features + self.cross_attention_dropout(integrated_features)
        
        debug_print(f"Quantum constraints integrated: seq_shape={sequence_features.shape}, constraint_shape={constraint_features.shape}")
        
        return output, attention_weights
    
    def get_attention_mode(self) -> str:
        """í˜„ì¬ ì–´í…ì…˜ ëª¨ë“œ ë°˜í™˜"""
        return self.attention_mode
    
    def compare_attention_modes(self, input_sequence: torch.Tensor, attention_mask: torch.Tensor, 
                              action_prediction_mask: torch.Tensor) -> Dict[str, Dict[str, torch.Tensor]]:
        """ì–´í…ì…˜ ëª¨ë“œë³„ ê²°ê³¼ ë¹„êµ"""
        original_mode = self.attention_mode
        results = {}
        
        for mode in ["standard", "advanced", "hybrid"]:
            self.set_attention_mode(mode)
            with torch.no_grad():
                output = self.forward(input_sequence, attention_mask, action_prediction_mask)
                results[mode] = {
                    'action_logits': output['action_logits'].clone(),
                    'action_predictions': output['action_predictions'].clone()
                }
        
        # ì›ë˜ ëª¨ë“œë¡œ ë³µêµ¬
        self.set_attention_mode(original_mode)
    def predict_next_action(
        self,
        input_sequence: torch.Tensor,
        attention_mask: torch.Tensor,
        circuit_constraints: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """ë‹¤ìŒ ì•¡ì…˜ ì˜ˆì¸¡ (ì¶”ë¡ ìš©) - ë©€í‹°íƒœìŠ¤í¬ ì˜ˆì¸¡ ì§€ì›
        ê¸°ëŒ€ í˜•íƒœ:
          - input_sequence: [1, seq_len, d_model]
          - attention_mask: [1, seq_len, seq_len]
        """
        with torch.no_grad():
            # ë§ˆì§€ë§‰ ìœ„ì¹˜ì—ì„œ ì•¡ì…˜ ì˜ˆì¸¡
            seq_len = input_sequence.shape[1]
            action_prediction_mask = torch.zeros(input_sequence.shape[:2], dtype=torch.bool, device=input_sequence.device)
            for j in range(1, seq_len, 3):
                if j < seq_len:
                    action_prediction_mask[0, j] = True
            action_prediction_mask[0, -1] = True  # ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ ì˜ˆì¸¡
            
            outputs = self.forward(input_sequence, attention_mask, action_prediction_mask, circuit_constraints=circuit_constraints)
            
            # ë§ˆì§€ë§‰ ì•¡ì…˜ ìœ„ì¹˜ì˜ í™•ë¥  ë¶„í¬ ë°˜í™˜
            action_positions = torch.where(action_prediction_mask)
            last_action_pos = int(action_positions[-1])
            
            # Property predictions from the cleaned model
            property_preds = outputs[:, last_action_pos, :]  # [1, 3]
            
            return {
                'property_predictions': property_preds,
                'hidden_states': outputs
            }
    
    def sample_gate_with_constraints(
        self, 
        gate_probs: torch.Tensor,
        temperature: float = 1.0
    ) -> Tuple[int, str, int, int]:
        """ê²Œì´íŠ¸ íƒ€ì…ì„ ìƒ˜í”Œë§í•˜ê³  í•´ë‹¹ ê²Œì´íŠ¸ì˜ íë¹—/íŒŒë¼ë¯¸í„° ìš”êµ¬ì‚¬í•­ ë°˜í™˜
        
        Returns:
            gate_idx: ìƒ˜í”Œë§ëœ ê²Œì´íŠ¸ ì¸ë±ìŠ¤
            gate_name: ê²Œì´íŠ¸ ì´ë¦„
            required_qubits: í•„ìš”í•œ íë¹— ìˆ˜
            required_params: í•„ìš”í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        """
        # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
        if temperature != 1.0:
            gate_probs = gate_probs / temperature
            gate_probs = F.softmax(gate_probs, dim=-1)
        
        # ê²Œì´íŠ¸ ìƒ˜í”Œë§
        gate_idx = torch.multinomial(gate_probs.squeeze(0), 1)
        
        # ê²Œì´íŠ¸ ì •ë³´ ì¡°íšŒ
        gate_vocab = self.gate_registry.get_gate_vocab()
        gate_names = list(gate_vocab.keys())
        
        if gate_idx < len(gate_names):
            gate_name = gate_names[gate_idx]
            gate_def = self.gate_registry.get_gate(gate_name)
            
            if gate_def:
                return gate_idx, gate_name, gate_def.num_qubits, gate_def.num_parameters
        
        # ê¸°ë³¸ê°’ (ì•Œ ìˆ˜ ì—†ëŠ” ê²Œì´íŠ¸)
        return gate_idx, "unknown", 1, 0
    
    def sample_qubits_for_gate(
        self,
        qubit_probs: List[torch.Tensor],
        gate_name: str,
        num_qubits_required: int,
        available_qubits: int,
        temperature: float = 1.0
    ) -> List[int]:
        """ê²Œì´íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” íë¹— ìœ„ì¹˜ë“¤ì„ ìƒ˜í”Œë§
        
        Args:
            qubit_probs: ê° íë¹— ìœ„ì¹˜ë³„ í™•ë¥  ë¶„í¬ ë¦¬ìŠ¤íŠ¸
            gate_name: ê²Œì´íŠ¸ ì´ë¦„
            num_qubits_required: í•„ìš”í•œ íë¹— ìˆ˜
            available_qubits: ì‚¬ìš© ê°€ëŠ¥í•œ ì´ íë¹— ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            
        Returns:
            ì„ íƒëœ íë¹— ì¸ë±ìŠ¤ë“¤ (ìš”êµ¬ì‚¬í•­ 1: [n], [n,n], [n,n,n] í˜•íƒœ)
        """
        selected_qubits = []
        used_qubits = set()
        
        for i in range(min(num_qubits_required, len(qubit_probs))):
            probs = qubit_probs[i].clone()
            
            # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
            if temperature != 1.0:
                probs = probs / temperature
                probs = F.softmax(probs, dim=-1)
            
            # ì´ë¯¸ ì‚¬ìš©ëœ íë¹—ê³¼ "no qubit" í† í° ë§ˆìŠ¤í‚¹
            for used_qubit in used_qubits:
                if used_qubit < probs.shape[-1] - 1:  # -1ì€ "no qubit" í† í°
                    probs[0, used_qubit] = 0.0
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ íë¹— ë²”ìœ„ ë°–ì€ ë§ˆìŠ¤í‚¹
            if available_qubits < probs.shape[-1] - 1:
                probs[0, available_qubits:-1] = 0.0
            
            # í™•ë¥  ì¬ì •ê·œí™”
            probs = probs / probs.sum()
            
            # íë¹— ìƒ˜í”Œë§
            qubit_idx = torch.multinomial(probs.squeeze(0), 1)
            
            # "no qubit" í† í°ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
            if qubit_idx < available_qubits:
                selected_qubits.append(qubit_idx)
                used_qubits.add(qubit_idx)
        
        return selected_qubits
    
    def sample_parameters_for_gate(
        self,
        param_values: List[torch.Tensor],
        gate_name: str,
        num_params_required: int
    ) -> List[float]:
        """ê²Œì´íŠ¸ì— í•„ìš”í•œ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¶”ì¶œ
        
        Args:
            param_values: ì˜ˆì¸¡ëœ íŒŒë¼ë¯¸í„° ê°’ë“¤
            gate_name: ê²Œì´íŠ¸ ì´ë¦„  
            num_params_required: í•„ìš”í•œ íŒŒë¼ë¯¸í„° ìˆ˜
            
        Returns:
            íŒŒë¼ë¯¸í„° ê°’ë“¤ (ìš”êµ¬ì‚¬í•­ 2)
        """
        parameters = []
        
        for i in range(min(num_params_required, len(param_values))):
            param_val = param_values[i].squeeze()
            
            # íŒŒë¼ë¯¸í„° ë²”ìœ„ ì œí•œ (íšŒì „ ê²Œì´íŠ¸ì˜ ê²½ìš° 0 ~ 2Ï€)
            if gate_name.startswith('r') or gate_name in ['p']:  # rx, ry, rz, p ê²Œì´íŠ¸
                param_val = param_val % (2 * math.pi)
            
            parameters.append(param_val)
        
        return parameters

    def generate_autoregressive(
        self,
        prompt_tokens: List[int] = None,
        max_length: int = 100,
        temperature: float = 1.0,
        top_k: int = 50,
        reward_calculator=None,
        target_properties: Dict[str, float] = None,
        num_qubits: int = 4
    ) -> List[Dict]:
        """
        Property-guided Autoregressive ìƒì„± (Decision Transformer ê¸°ë°˜)
        
        Args:
            prompt_tokens: ì´ˆê¸° í† í° ì‹œí€€ìŠ¤ (ì„ íƒì )
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_k: Top-k ìƒ˜í”Œë§
            reward_calculator: ë³´ìƒ ê³„ì‚°ê¸°
            target_properties: ëª©í‘œ ì†ì„±
            num_qubits: íë¹— ìˆ˜
            
        Returns:
            ìƒì„±ëœ ê²Œì´íŠ¸ ì‹œí€€ìŠ¤ [{'gate': str, 'qubits': List[int], 'params': List[float]}]
        """
        self.eval()
        
        # ì´ˆê¸° ì‹œí€€ìŠ¤ ì„¤ì •
        generated_gates = []
        current_context = {
            'states': [],
            'actions': [],
            'rewards': []
        }
        
        # ë³´ìƒ ê°€ì´ë˜ìŠ¤ ì„¤ì •
        use_reward_guidance = reward_calculator is not None and target_properties is not None
        if use_reward_guidance:
            reward_calculator.set_target_properties(target_properties)
            print(f"ğŸ¯ Using reward guidance with targets: {target_properties}")
        
        with torch.no_grad():
            for step in range(max_length):
                print(f"\n--- Generation Step {step} ---")
                
                # 1. í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
                if len(current_context['states']) == 0:
                    # ì´ˆê¸° ìƒíƒœ (ë¹ˆ íšŒë¡œ)
                    device = torch.device(self.device)
                    state_emb = torch.zeros(1, 1, self.d_model, device=device)
                    action_emb = torch.zeros(1, 1, self.d_model, device=device)
                    reward_emb = torch.zeros(1, 1, self.d_model, device=device)
                else:
                    # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ì¬ì„ë² ë”©
                    state_emb = torch.stack(current_context['states'], dim=1)  # [1, seq_len, d_model]
                    action_emb = torch.stack(current_context['actions'], dim=1)
                    reward_emb = torch.stack(current_context['rewards'], dim=1)
                
                # 2. SAR ì‹œí€€ìŠ¤ êµ¬ì„±ì€ ë°°ì¹˜ ì½œë ˆì´í„°ì—ì„œ ì²˜ë¦¬ë¨
                # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ forward passë§Œ ìˆ˜í–‰
                
                # 3. ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
                device = torch.device(self.device)
                mask = torch.ones(1, seq_len * 3, device=device, dtype=torch.bool)
                
                # 4. íŠ¸ëœìŠ¤í¬ë¨¸ forward
                hidden_states = sar_sequence
                for block in self.transformer_blocks:
                    hidden_states = block(hidden_states, mask)
                
                # 5. í˜„ì¬ ìƒíƒœ ì„ë² ë”© ì¶”ì¶œ (ë§ˆì§€ë§‰ state ìœ„ì¹˜)
                if seq_len > 0:
                    current_state_emb = hidden_states[:, (seq_len-1)*3, :]  # ë§ˆì§€ë§‰ state
                else:
                    current_state_emb = hidden_states[:, 0, :]  # ì²« ë²ˆì§¸ ìœ„ì¹˜
                
                # 6. ë³´ìƒ ê³„ì‚° (ì„ íƒì )
                current_reward = 0.0
                if use_reward_guidance:
                    try:
                        reward_info = reward_calculator.calculate_reward_from_state_embedding(
                            current_state_emb, num_qubits=num_qubits
                        )
                        current_reward = reward_info['total_reward']
                        print(f"   Current reward: {current_reward:.4f}")
                        print(f"   Predicted properties: {reward_info['predicted_properties']}")
                        
                        # ë†’ì€ ë³´ìƒ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
                        if current_reward > 0.8:
                            print(f"ğŸ‰ High reward achieved ({current_reward:.4f}), stopping generation")
                            break
                            
                    except Exception as e:
                        print(f"   Reward calculation failed: {e}")
                        current_reward = 0.0
                
                # 7. ë‹¤ìŒ ì•¡ì…˜ ì˜ˆì¸¡
                gate_logits = self.gate_head(current_state_emb)  # [1, num_gates]
                
                # ë³´ìƒ ê¸°ë°˜ ë°”ì´ì–´ìŠ¤ ì ìš©
                if use_reward_guidance and current_reward > 0:
                    reward_bias = current_reward * 2.0  # ë³´ìƒ ìŠ¤ì¼€ì¼ë§
                    gate_logits = gate_logits + reward_bias
                
                # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§ ë° ìƒ˜í”Œë§
                gate_probs = F.softmax(gate_logits / temperature, dim=-1)
                
                # Top-k ìƒ˜í”Œë§
                if top_k > 0:
                    top_k_probs, top_k_indices = torch.topk(gate_probs, min(top_k, gate_probs.shape[-1]))
                    gate_idx = top_k_indices[0, torch.multinomial(top_k_probs[0], 1)]
                else:
                    gate_idx = torch.multinomial(gate_probs[0], 1)
                
                # 8. ê²Œì´íŠ¸ ì •ë³´ ì¶”ì¶œ
                gate_registry = QuantumGateRegistry()
                gate_name = gate_registry.get_gate_name_by_index(gate_idx)
                
                if gate_name is None:
                    print(f"   Invalid gate index: {gate_idx}, stopping generation")
                    break
                
                print(f"   Selected gate: {gate_name} (idx: {gate_idx})")
                
                # 9. íë¹— ìœ„ì¹˜ ì˜ˆì¸¡
                position_logits = self.position_head(current_state_emb)  # [1, max_qubits]
                selected_qubits = self.sample_qubits_for_gate(
                    position_logits, gate_name, num_qubits, temperature
                )
                
                # 10. íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ (í•„ìš”í•œ ê²½ìš°)
                param_values = []
                gate_info = gate_registry.get_gate_info(gate_name)
                if gate_info and gate_info.get('num_params', 0) > 0:
                    for i in range(gate_info['num_params']):
                        param_logit = self.parameter_heads[i](current_state_emb)
                        param_values.append(param_logit)
                    
                    parameters = self.sample_parameters_for_gate(
                        param_values, gate_name, gate_info['num_params']
                    )
                else:
                    parameters = []
                
                # 11. ìƒì„±ëœ ê²Œì´íŠ¸ ì €ì¥
                generated_gate = {
                    'gate': gate_name,
                    'qubits': selected_qubits,
                    'params': parameters
                }
                generated_gates.append(generated_gate)
                
                print(f"   Generated: {generated_gate}")
                
                # 12. ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´)
                # ì˜ˆì¸¡ëœ ê²Œì´íŠ¸ë¥¼ í˜„ì¬ íšŒë¡œì— ì¶”ê°€í•˜ì—¬ ìƒˆë¡œìš´ ìƒíƒœ ì„ë² ë”© ìƒì„±
                predicted_gate_info = {
                    'gate_name': gate_name,
                    'qubits': selected_qubits,
                    'parameter_value': parameters[0] if parameters else 0.0
                }
                
                # í˜„ì¬ê¹Œì§€ì˜ ê²Œì´íŠ¸ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
                current_circuit_gates = []
                for prev_gate in generated_gates[:-1]:  # ë°©ê¸ˆ ì¶”ê°€í•œ ê²Œì´íŠ¸ ì œì™¸
                    current_circuit_gates.append({
                        'gate_name': prev_gate['gate'],
                        'qubits': prev_gate['qubits'],
                        'parameter_value': prev_gate['params'][0] if prev_gate['params'] else 0.0
                    })
                
                # ìƒˆë¡œìš´ í†µí•© Facadeë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
                from encoding.unified_embedding_facade import UnifiedEmbeddingFacade
                facade = UnifiedEmbeddingFacade()
                
                # í˜„ì¬ íšŒë¡œ ìƒíƒœë¥¼ grid matrix í˜•íƒœë¡œ ë³€í™˜
                grid_data = self._convert_circuit_to_grid_data(current_circuit_gates, num_qubits)
                
                # Facadeë¥¼ í†µí•´ ì„ë² ë”© ìƒì„±
                embedding_result = facade.process_for_decision_transformer(
                    grid_data, 
                    original_gate_count=len(current_circuit_gates)
                )
                
                # ë§ˆì§€ë§‰ ìƒíƒœ ì„ë² ë”© ì¶”ì¶œ
                new_state_emb = embedding_result['input_sequence'][-1]
                
                # ì•¡ì…˜ ì„ë² ë”© (ì˜ˆì¸¡ëœ ê²Œì´íŠ¸ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì„ë² ë”©)
                device = torch.device(self.device)
                gate_tensor = torch.tensor([gate_idx, selected_qubits[0], 
                                          selected_qubits[1] if len(selected_qubits) > 1 else selected_qubits[0],
                                          parameters[0] if parameters else 0.0], 
                                         dtype=torch.float32, device=device)
                action_emb = torch.nn.functional.embedding(torch.tensor([gate_idx], device=device), 
                                                         torch.randn(self.n_gate_types, self.d_model, device=device))
                
                # ë³´ìƒ ì„ë² ë”© (ê°„ë‹¨í•œ ì„ í˜• ë³€í™˜)
                reward_tensor = torch.tensor([current_reward], device=device)
                reward_emb_new = torch.nn.Linear(1, self.d_model).to(device)(reward_tensor.unsqueeze(0)).squeeze(0)
                
                # ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                current_context['states'].append(new_state_emb)
                current_context['actions'].append(action_emb)
                current_context['rewards'].append(reward_emb_new)
                
                # ì¢…ë£Œ ì¡°ê±´ ì²´í¬
                if gate_name in ['measure', 'barrier'] or len(generated_gates) >= max_length:
                    break
        
        print(f"\nğŸ¯ Generation completed: {len(generated_gates)} gates generated")
        return generated_gates


class DebugMode:
    """ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •"""
    TENSOR_DIM = "tensor_dim"          # í…ì„œ ì°¨ì› í…ŒìŠ¤íŠ¸
    EMBEDDING = "embedding"            # ì„ë² ë”© ë””ë²„ê·¸
    MODEL_PREDICTION = "model_prediction"  # ëª¨ë¸ ì˜ˆì¸¡ ë””ë²„ê·¸
    MODEL_OUTPUT = "model_output"      # ëª¨ë¸ ì¶œë ¥ ë””ë²„ê·¸
    
    # í˜„ì¬ í™œì„±í™”ëœ ë””ë²„ê·¸ ëª¨ë“œë“¤
    ACTIVE_MODES = {MODEL_OUTPUT}
    
    @staticmethod
    def is_active(mode: str) -> bool:
        """ë””ë²„ê·¸ ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return mode in DebugMode.ACTIVE_MODES


class ActionLossComputer:
    """ì•¡ì…˜ ì˜ˆì¸¡ ì†ì‹¤ ê³„ì‚°ê¸° - ëª¨ë“ˆëŸ¬ êµ¬ì¡°ì— ë§ê²Œ ë¦¬íŒ©í† ë§ëœ ì†ì‹¤ ê³„ì‚° ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, loss_weights: Dict[str, float] = None, ignore_index: int = -100):
        self.weights = loss_weights or {'gate': 0.8, 'position': 0.1, 'parameter': 0.1}
        self.ignore_index = ignore_index
        self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0, ignore_index=ignore_index)
        self.mse_loss = nn.MSELoss()
    
    def compute(self, predictions: Dict, targets: Dict, mask: torch.Tensor, num_qubits: Optional[List[int]] = None, num_gates: Optional[List[int]] = None) -> Dict:
        """í†µí•© ì†ì‹¤ ê³„ì‚° (ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì§€ì›)"""
        device = mask.device
        
        #  DEBUG: ì•¡ì…˜ ë§ˆìŠ¤í¬ ì°¨ì› ë¶„ì„
        # Remove debug prints that access tensor values during training
        # Remove debug prints that access tensor values during training
        
        # ìœ íš¨í•œ ì˜ˆì¸¡ ìœ„ì¹˜ë§Œ ì„ íƒ
        valid_mask = mask.view(-1)
        # Remove debug prints that access tensor values during training
        # Remove debug prints that access tensor values during training
        
        if valid_mask.sum() == 0:
            return self._empty_loss_dict(device)
        
        # ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì ìš©
        if num_qubits is not None and 'position_preds' in predictions:
            predictions = self._apply_dynamic_qubit_masking(predictions, num_qubits, mask)
        
        #  DEBUG: ì†ì‹¤ ê³„ì‚° ì§„í–‰ ìƒí™© ì¶”ì 
        # Remove debug prints that access tensor values during training
        # Remove debug prints that access tensor values during training
        
        # ê° ì†ì‹¤ ê³„ì‚°
        # Gate loss computation
        gate_loss = self._compute_gate_loss(predictions, targets, valid_mask)
        # Gate loss computed
        
        # Position loss computation
        position_loss = self._compute_position_loss(predictions, targets, valid_mask, num_qubits, num_gates)
        # Position loss computed
        
        # Parameter loss computation
        parameter_loss = self._compute_parameter_loss(predictions, targets, valid_mask)
        # Parameter loss computed
        
        #  DEBUG: ì²« ë²ˆì§¸ íšŒë¡œì˜ ì˜ˆì¸¡ vs ì •ë‹µ ë¹„êµ
        #self._debug_first_circuit_predictions(predictions, targets, mask, num_gates)
        
        # ê°€ì¤‘ í•©ê³„
        total_loss = (
            self.weights['gate'] * gate_loss + 
            self.weights['position'] * position_loss + 
            self.weights['parameter'] * parameter_loss
        )
        
        # Remove debug prints that access tensor values during training
        
        losses = {
            'loss': total_loss,
            'gate_loss': gate_loss,
            'position_loss': position_loss,
            'parameter_loss': parameter_loss,
        }
        
        # Loss dictionary created
        
        # ì •í™•ë„ ë° ë¶„ë¥˜ ë©”íŠ¸ë¦­ ì¶”ê°€ (í•˜ìœ„ í˜¸í™˜ì„±)
        if hasattr(self, '_gate_accuracy'):
            losses['gate_accuracy'] = self._gate_accuracy
        if hasattr(self, '_gate_precision'):
            losses['gate_precision'] = self._gate_precision
        if hasattr(self, '_gate_recall'):
            losses['gate_recall'] = self._gate_recall
        if hasattr(self, '_gate_f1'):
            losses['gate_f1'] = self._gate_f1
        
        # Loss computation completed
        return losses
    
    def _debug_first_circuit_predictions(self, predictions: Dict, targets: Dict, mask: torch.Tensor, num_gates: Optional[List[int]] = None) -> None:
        """ì²« ë²ˆì§¸ íšŒë¡œì˜ ì˜ˆì¸¡ vs ì •ë‹µ ë¹„êµ ë””ë²„ê·¸"""
        try:
            print(f" [DEBUG_ENTRY] num_gates: {num_gates}")
            print(f" [DEBUG_ENTRY] mask.shape: {mask.shape}")
            print(f" [DEBUG_ENTRY] predictions keys: {list(predictions.keys())}")
            print(f" [DEBUG_ENTRY] targets keys: {list(targets.keys())}")
            
            if num_gates is None or len(num_gates) == 0:
                print(" [DEBUG_EXIT] num_gatesê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìŒ")
                return
            
            batch_size, seq_len = mask.shape
            first_circuit_gates = num_gates[0]
            print(f" [DEBUG_INFO] first_circuit_gates: {first_circuit_gates}")
            
            # ì²« ë²ˆì§¸ íšŒë¡œì˜ ìœ íš¨í•œ ê²Œì´íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
            first_circuit_mask = mask[0]  # [seq_len]
            valid_positions = torch.where(first_circuit_mask)[0][:first_circuit_gates]  # ì²« Nê°œ ìœ„ì¹˜ë§Œ
            print(f" [DEBUG_INFO] valid_positions: {valid_positions}")
            
            if len(valid_positions) == 0:
                print(" [DEBUG_EXIT] valid_positionsê°€ ë¹„ì–´ìˆìŒ")
                return
            
            print(f"\nğŸ” [DEBUG] ì²« ë²ˆì§¸ íšŒë¡œ ì˜ˆì¸¡ ë¶„ì„ (ê²Œì´íŠ¸ ìˆ˜: {first_circuit_gates})")
            print("=" * 80)
        
            # Gate predictions vs targets
            if 'gate_logits' in predictions and 'gate_targets' in targets:
                gate_logits = predictions['gate_logits'][0]  # [seq_len, 20]
                gate_preds = torch.argmax(gate_logits, dim=-1)  # [seq_len]
                
                # íƒ€ê²Ÿ ì²˜ë¦¬
                gate_targets = targets['gate_targets']
                if gate_targets.dim() == 1:
                    # [batch*seq] í˜•íƒœì¸ ê²½ìš°
                    first_circuit_targets = gate_targets[:first_circuit_gates]
                else:
                    # [batch, seq] í˜•íƒœì¸ ê²½ìš°
                    first_circuit_targets = gate_targets[0, :first_circuit_gates]
                
                print(f"ğŸ¯ Gate ì˜ˆì¸¡ vs ì •ë‹µ (ì²˜ìŒ {min(10, first_circuit_gates)}ê°œ):")
                for i, pos in enumerate(valid_positions[:10]):
                    pred_gate = gate_preds[pos]
                    true_gate = first_circuit_targets[i] if i < len(first_circuit_targets) else -1
                    match = "âœ…" if pred_gate == true_gate else "âŒ"
                    print(f"   ìœ„ì¹˜ {pos:2d}: ì˜ˆì¸¡={pred_gate:2d}, ì •ë‹µ={true_gate:2d} {match}")
            
            # Position predictions vs targets  
            if 'position_preds' in predictions and 'position_targets' in targets:
                position_preds = predictions['position_preds'][0]  # [seq_len, 32, 2]
                position_logits = position_preds[:, 0, :]  # [seq_len, 2] - ì²« ë²ˆì§¸ íë¹—ë§Œ
                
                position_targets = targets['position_targets']
                if position_targets.dim() == 2 and position_targets.shape[1] == 2:
                    # [N, 2] í˜•íƒœ
                    first_circuit_pos_targets = position_targets[:first_circuit_gates]
                else:
                    print("   Position targets í˜•íƒœë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ")
                    first_circuit_pos_targets = None
                
                if first_circuit_pos_targets is not None:
                    print(f"ğŸ¯ Position ì˜ˆì¸¡ vs ì •ë‹µ (ì²˜ìŒ {min(5, first_circuit_gates)}ê°œ):")
                    for i, pos in enumerate(valid_positions[:5]):
                        pred_pos = position_logits[pos]  # [2]
                        true_pos = first_circuit_pos_targets[i] if i < len(first_circuit_pos_targets) else torch.tensor([-1, -1])
                        print(f"   ìœ„ì¹˜ {pos:2d}: ì˜ˆì¸¡=[{pred_pos[0]:.2f}, {pred_pos[1]:.2f}], ì •ë‹µ=[{true_pos[0]}, {true_pos[1]}]")
            
                print("=" * 80)
        
        except Exception as e:
            print(f" [DEBUG_ERROR] ë””ë²„ê·¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def _compute_gate_loss(self, predictions, targets, valid_mask):
        """ê²Œì´íŠ¸ íƒ€ì… ì†ì‹¤ ê³„ì‚°"""
        # Gate prediction logits
        gate_logits = predictions['gate_logits']  # [batch, seq, num_types]
        batch_size, seq_len, num_gate_types = gate_logits.shape
        
        # ë¡œì§“ ì¬êµ¬ì„± (Reshape logits)
        reshaped_logits = gate_logits.reshape(-1, num_gate_types)  # [batch*seq, num_types]
        
        # ë””ë²„ê¹… ì •ë³´
        # Remove debug prints that access tensor values during training
        
        # ë§ˆìŠ¤í¬ê°€ Trueì¸ ìœ„ì¹˜ë§Œ ì„ íƒ (ì§ì ‘ ì¸ë±ì‹±)
        selected_indices = torch.where(valid_mask)[0]
        # Remove debug prints that access tensor values during training
        
        # ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì˜ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        max_idx = reshaped_logits.shape[0] - 1
        valid_indices = selected_indices[selected_indices <= max_idx]
        
        # ë§ˆìŠ¤í¬ ì ìš© - ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        gate_logits = reshaped_logits[valid_indices]
        # Remove debug prints that access tensor values during training
        
        # íƒ€ê²Ÿ ì¸ë±ì‹± - ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        gate_targets = targets['gate_targets'].reshape(-1)
        
        # ğŸ”¥ CRITICAL FIX: Map special tokens to valid prediction range
        # Embedding uses 22 gates (0-21), but prediction head uses 20 gates (0-19)
        # Map special tokens: [EOS]=20 -> 0 (H gate), [PAD]=21 -> 0 (H gate)
        special_token_mask = gate_targets >= 20  # >= 20 for special tokens
        if special_token_mask.any():
            # Special token mapping to H gate
            gate_targets = gate_targets.clone()
            gate_targets[special_token_mask] = 0  # Map special tokens to H gate
        
        # ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì˜ íƒ€ê²Ÿë§Œ ì‚¬ìš©
        if len(gate_targets) > len(valid_indices):
            gate_targets = gate_targets[valid_indices]
        else:
            # íƒ€ê²Ÿ ë°°ì—´ì´ ë” ì‘ì€ ê²½ìš°
            max_target_idx = min(len(gate_targets)-1, max(valid_indices))
            usable_indices = valid_indices[valid_indices <= max_target_idx]
            gate_logits = reshaped_logits[usable_indices]
            gate_targets = gate_targets[usable_indices]
            pass  # Target size limitation warning removed
        
        # Remove debug prints that access tensor values during training
        
        # í…ì„œ íƒ€ì…ì„ Longìœ¼ë¡œ ë³€í™˜ (cross_entropyëŠ” Long íƒ€ì…ì„ ìš”êµ¬í•¨)
        if gate_targets.dtype == torch.bool:
            gate_targets = gate_targets.long()
        elif gate_targets.dtype != torch.long:
            gate_targets = gate_targets.to(torch.long)
        
        # íŒ¨ë”© íƒ€ê²Ÿ(-1) í•„í„°ë§
        valid_target_mask = gate_targets >= 0
        if valid_target_mask.sum() == 0:
            return torch.tensor(0.0, device=gate_logits.device)
        
        final_logits = gate_logits[valid_target_mask]
        final_targets = gate_targets[valid_target_mask]
        
        # ì •í™•ë„ ê³„ì‚° ë° ì €ì¥
        gate_predictions = torch.argmax(final_logits, dim=-1)
        self._gate_accuracy = (gate_predictions == final_targets).float().mean()
        
        # F1, Precision, Recall ê³„ì‚° (ë‹¤ì¤‘ í´ë˜ìŠ¤)
        self._compute_classification_metrics(gate_predictions, final_targets)
        
        return self.focal_loss(final_logits, final_targets)
    
    def _compute_classification_metrics(self, predictions: torch.Tensor, targets: torch.Tensor):
        """F1, Precision, Recall ê³„ì‚° (ë§¤í¬ë¡œ í‰ê· )"""
        predictions_np = predictions.cpu().numpy()
        targets_np = targets.cpu().numpy()
        
        # ê³ ìœ  í´ë˜ìŠ¤ë“¤
        unique_classes = np.unique(np.concatenate([predictions_np, targets_np]))
        
        if len(unique_classes) <= 1:
            # ë‹¨ì¼ í´ë˜ìŠ¤ì¸ ê²½ìš°
            self._gate_precision = 1.0
            self._gate_recall = 1.0
            self._gate_f1 = 1.0
            return
        
        # í´ë˜ìŠ¤ë³„ precision, recall ê³„ì‚°
        precisions = []
        recalls = []
        f1s = []
        
        for cls in unique_classes:
            # True Positive, False Positive, False Negative
            tp = np.sum((predictions_np == cls) & (targets_np == cls))
            fp = np.sum((predictions_np == cls) & (targets_np != cls))
            fn = np.sum((predictions_np != cls) & (targets_np == cls))
            
            # Precision, Recall ê³„ì‚°
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            precisions.append(precision)
            recalls.append(recall)
            f1s.append(f1)
        
        # ë§¤í¬ë¡œ í‰ê· 
        self._gate_precision = np.mean(precisions)
        self._gate_recall = np.mean(recalls)
        self._gate_f1 = np.mean(f1s)
    
    def _compute_position_loss(self, predictions: Dict, targets: Dict, valid_mask: torch.Tensor,
                               num_qubits: Optional[List[int]] = None, num_gates: Optional[List[int]] = None) -> torch.Tensor:
        position_preds = predictions['position_preds']  # [batch, seq_len, max_qubits, 2]
        batch_size, seq_len, max_qubits, pos_dim = position_preds.shape
        
        # Remove debug prints that access tensor values during training
        
        # ì²« ë²ˆì§¸ íë¹— ìœ„ì¹˜ë§Œ ì˜ˆì¸¡ (ê°„ë‹¨í™”)
        position_logits = position_preds[:, :, 0, :]  # [batch, seq_len, 2]
        position_logits_flat = position_logits.reshape(-1, 2)  # [batch*seq_len, 2]
        
        # valid_maskë¡œ ì‹¤ì œ ê²Œì´íŠ¸ ìœ„ì¹˜ë§Œ ì„ íƒ
        valid_position_logits = position_logits_flat[valid_mask]  # [num_valid_gates, 2]
        # Remove debug prints that access tensor values during training
        
        # position_targets ê°€ì ¸ì˜¤ê¸° (qubit_targetsë¡œ ë§¤í•‘)
        if 'qubit_targets' not in targets:
            # Missing qubit_targets key
            return torch.tensor(0.0, device=position_preds.device)
        
        position_targets = targets['qubit_targets']  # [batch, seq_len, 2]
        
        # ğŸ”¥ FIX: Handle case where position_targets is a list instead of tensor
        if isinstance(position_targets, list):
            # Converting position_targets list to tensor
            # Convert list to tensor, handle nested lists
            if len(position_targets) > 0 and isinstance(position_targets[0], list):
                # Nested list case: [[qubits], [qubits], ...]
                max_len = max(len(item) if isinstance(item, list) else 1 for item in position_targets)
                padded_targets = []
                for item in position_targets:
                    if isinstance(item, list):
                        # Pad to max_len with -1
                        padded = item + [-1] * (max_len - len(item))
                        padded_targets.append(padded[:2])  # Take first 2 qubits only
                    else:
                        padded_targets.append([item, -1])  # Single qubit case
                position_targets = torch.tensor(padded_targets, dtype=torch.long)
            else:
                # Simple list case
                position_targets = torch.tensor(position_targets, dtype=torch.long)
        
        # Remove debug prints that access tensor values during training
        
        # position_targetsë¥¼ flatí•˜ê²Œ ë³€í™˜
        position_targets_flat = position_targets.reshape(-1, 2)  # [batch*seq_len, 2]
        # Remove debug prints that access tensor values during training
        
        # valid_maskë¡œ í•„í„°ë§
        valid_position_targets = position_targets_flat[valid_mask]
        # Remove debug prints that access tensor values during training
        
        # íŒ¨ë”©ëœ íƒ€ê²Ÿ(-1) ì œê±°
        non_padding_mask = (valid_position_targets[:, 0] >= 0) & (valid_position_targets[:, 1] >= 0)
        # Remove debug prints that access tensor values during training
        
        if non_padding_mask.sum() == 0:
            # All position targets are padding (-1)
            return torch.tensor(0.0, device=position_preds.device)
        
        final_preds = valid_position_logits[non_padding_mask]
        final_targets = valid_position_targets[non_padding_mask].float()
        
        # ğŸš¨ ê°•ë ¥í•œ íë¹— ë²”ìœ„ í˜ë„í‹° ì ìš©
        position_loss = F.mse_loss(final_preds, final_targets, reduction='mean')
        
        # íë¹— ë²”ìœ„ ìœ„ë°˜ í˜ë„í‹° ê³„ì‚°
        if num_qubits is not None:
            penalty_loss = self._compute_qubit_range_penalty(
                final_preds, final_targets, num_qubits, valid_mask, non_padding_mask
            )
            # í˜ë„í‹°ë¥¼ ê¸°ë³¸ ì†ì‹¤ì— ì¶”ê°€ (ê°•ë ¥í•œ ê°€ì¤‘ì¹˜ ì ìš©)
            position_loss = position_loss + 10.0 * penalty_loss
        
        return position_loss
    
    def _compute_qubit_range_penalty(self, predictions: torch.Tensor, targets: torch.Tensor, 
                                   num_qubits: List[int], valid_mask: torch.Tensor, 
                                   non_padding_mask: torch.Tensor) -> torch.Tensor:
        """
        íë¹— ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì˜ˆì¸¡ì— ëŒ€í•œ ê°•ë ¥í•œ í˜ë„í‹° ê³„ì‚°
        
        Args:
            predictions: ì˜ˆì¸¡ëœ íë¹— ìœ„ì¹˜ [num_valid, 2]
            targets: ì‹¤ì œ íë¹— ìœ„ì¹˜ [num_valid, 2]  
            num_qubits: ê° íšŒë¡œì˜ íë¹— ìˆ˜ [batch_size]
            valid_mask: ìœ íš¨í•œ ìœ„ì¹˜ ë§ˆìŠ¤í¬
            non_padding_mask: íŒ¨ë”©ì´ ì•„ë‹Œ ìœ„ì¹˜ ë§ˆìŠ¤í¬
        
        Returns:
            í˜ë„í‹° ì†ì‹¤ (íë¹— ë²”ìœ„ ìœ„ë°˜ì‹œ í° ê°’)
        """
        device = predictions.device
        penalty_loss = torch.tensor(0.0, device=device)
        
        # ë°°ì¹˜ë³„ë¡œ íë¹— ë²”ìœ„ í™•ì¸
        batch_size = len(num_qubits)
        
        # valid_maskì™€ non_padding_maskë¥¼ í†µí•´ ë°°ì¹˜ ì¸ë±ìŠ¤ ë³µì›
        batch_indices = []
        current_idx = 0
        
        for batch_idx in range(batch_size):
            # ì´ ë°°ì¹˜ì˜ ìœ íš¨í•œ ìœ„ì¹˜ ìˆ˜ ê³„ì‚° (ê·¼ì‚¬ì¹˜)
            batch_valid_count = valid_mask.sum() // batch_size
            
            for _ in range(batch_valid_count):
                if current_idx < len(predictions):
                    batch_indices.append(batch_idx)
                    current_idx += 1
        
        # ì˜ˆì¸¡ê°’ì´ íë¹— ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ”ì§€ í™•ì¸
        total_violations = 0
        total_penalty = 0.0
        
        for i, pred in enumerate(predictions):
            if i < len(batch_indices):
                batch_idx = batch_indices[i]
                max_qubit = num_qubits[batch_idx] - 1  # 0-indexed
                
                # ë‘ íë¹— ìœ„ì¹˜ ëª¨ë‘ í™•ì¸
                qubit1, qubit2 = pred[0], pred[1]
                
                # ë²”ìœ„ ìœ„ë°˜ ê²€ì‚¬
                violation_penalty = 0.0
                
                # íë¹—1 ë²”ìœ„ ìœ„ë°˜
                if qubit1 < 0 or qubit1 > max_qubit:
                    violation_penalty += torch.abs(qubit1 - torch.clamp(qubit1, 0, max_qubit))
                    total_violations += 1
                
                # íë¹—2 ë²”ìœ„ ìœ„ë°˜  
                if qubit2 < 0 or qubit2 > max_qubit:
                    violation_penalty += torch.abs(qubit2 - torch.clamp(qubit2, 0, max_qubit))
                    total_violations += 1
                
                total_penalty += violation_penalty
        
        # ìœ„ë°˜ì´ ìˆìœ¼ë©´ ê°•ë ¥í•œ í˜ë„í‹° ì ìš©
        if total_violations > 0:
            penalty_loss = torch.tensor(total_penalty, device=device)
            # Qubit range violation penalty applied
        
        return penalty_loss
    
    def _compute_parameter_loss(self, predictions: Dict, targets: Dict, valid_mask: torch.Tensor) -> torch.Tensor:
        # íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ ì •ë³´ ì°¨ì› ë””ë²„ê¹…
        param_preds = predictions['parameter_preds']
        debug_print(f" PARAMETER_PREDS_SHAPE: {param_preds.shape}")
        
        # ë¦¬ì„œì´í•‘
        reshaped_preds = param_preds.reshape(-1)
        debug_print(f" RESHAPED_PARAM_PREDS_SHAPE: {reshaped_preds.shape}")
        
        # ì•ˆì „í•˜ê²Œ ë§ˆìŠ¤í¬ ì ìš© (ì°¨ì›ì´ ë§ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ)
        if len(reshaped_preds) != len(valid_mask):
            debug_print(f"âš ï¸ íŒŒë¼ë¯¸í„° ì°¨ì› ë¶ˆì¼ì¹˜ ë°œìƒ! ê°€ì¥ ì‘ì€ ì°¨ì›ìœ¼ë¡œ ì˜ë¼ëƒ„")
            min_len = min(len(reshaped_preds), len(valid_mask))
            valid_mask = valid_mask[:min_len]
            reshaped_preds = reshaped_preds[:min_len]
        
        # ë§ˆìŠ¤í¬ ì ìš©
        param_preds = reshaped_preds[valid_mask]
        
        # parameter_targets í™•ì¸
        if 'parameter_targets' not in targets:
            return torch.tensor(0.0, device=param_preds.device)
        
        # parameter_targetsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ì„œë¡œ ë³€í™˜
        if isinstance(targets['parameter_targets'], list):
            if len(targets['parameter_targets']) == 0:
                return torch.tensor(0.0, device=param_preds.device)
            parameter_targets_tensor = torch.tensor(targets['parameter_targets'], device=param_preds.device)
        else:
            parameter_targets_tensor = targets['parameter_targets']
        
        # íƒ€ê²Ÿ ì¬ìƒì„±
        reshaped_targets = parameter_targets_tensor.reshape(-1)
        if len(reshaped_targets) > len(valid_mask):
            reshaped_targets = reshaped_targets[:len(valid_mask)]
        param_targets = reshaped_targets[valid_mask]
        
        # NaN ì²˜ë¦¬
        non_nan_mask = ~torch.isnan(param_targets)
        if non_nan_mask.sum() > 0:
            return self.mse_loss(param_preds[non_nan_mask], param_targets[non_nan_mask])
        return torch.tensor(0.0, device=param_preds.device)
    
    def _apply_dynamic_qubit_masking(self, predictions: Dict, num_qubits: List[int], mask: torch.Tensor) -> Dict:
        """ğŸš€ ì—„ê²©í•œ ë™ì  íë¹— ë§ˆìŠ¤í‚¹: ê° íšŒë¡œì˜ ì •í™•í•œ íë¹— ìˆ˜ì— ë§ê²Œ ì˜ˆì¸¡ì„ ë§ˆìŠ¤í‚¹"""
        if 'position_preds' not in predictions:
            raise ValueError("âŒ CRITICAL ERROR: position_predsê°€ ì˜ˆì¸¡ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        if num_qubits is None:
            raise ValueError("âŒ CRITICAL ERROR: num_qubits ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì˜ˆì¸¡ ë³µì‚¬ (ì›ë³¸ ìˆ˜ì • ë°©ì§€)
        masked_predictions = predictions.copy()
        position_preds = predictions['position_preds'].clone()  # [batch, num_actions, max_qubits, 2]
        
        batch_size, num_actions, max_qubits, qubit_dims = position_preds.shape
        
        # ğŸš€ ë°°ì¹˜ í¬ê¸° ê²€ì¦
        if len(num_qubits) != batch_size:
            raise ValueError(
                f"âŒ CRITICAL ERROR: ë°°ì¹˜ í¬ê¸° ë¶ˆì¼ì¹˜!\n"
                f"   position_preds ë°°ì¹˜ í¬ê¸°: {batch_size}\n"
                f"   num_qubits ê¸¸ì´: {len(num_qubits)}"
            )
        
        # ğŸš€ ê° ë°°ì¹˜ë³„ë¡œ ì •í™•í•œ ë™ì  ë§ˆìŠ¤í‚¹ ì ìš©
        for batch_idx in range(batch_size):
            circuit_qubits = num_qubits[batch_idx]
            
            # ğŸš€ íë¹— ìˆ˜ ê²€ì¦ ë° ìë™ ì¡°ì •
            if circuit_qubits <= 0:
                raise ValueError(f"âŒ CRITICAL ERROR: íšŒë¡œ {batch_idx}ì˜ íë¹— ìˆ˜ê°€ 0 ì´í•˜ì…ë‹ˆë‹¤: {circuit_qubits}")
            
            if circuit_qubits > max_qubits:
                debug_print(f"âš ï¸ íšŒë¡œ {batch_idx}: íë¹— ìˆ˜ ì´ˆê³¼ ê°ì§€ - ìë™ ì¡°ì •")
                debug_print(f"   ì›ë³¸ íë¹— ìˆ˜: {circuit_qubits}")
                debug_print(f"   ëª¨ë¸ ìµœëŒ€ íë¹—: {max_qubits}")
                debug_print(f"   â†’ {max_qubits}ê°œë¡œ ì œí•œ")
                circuit_qubits = max_qubits
            
            # âœ… ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ë¥¼ -infë¡œ ë§ˆìŠ¤í‚¹ (softmaxì—ì„œ í™•ë¥  0ì´ ë¨)
            for action_idx in range(num_actions):
                for qubit_dim in range(qubit_dims):  # qubit1, qubit2
                    # circuit_qubits ì´ìƒì˜ ì¸ë±ìŠ¤ëŠ” -infë¡œ ë§ˆìŠ¤í‚¹
                    position_preds[batch_idx, action_idx, circuit_qubits:, qubit_dim] = float('-inf')
        
        masked_predictions['position_preds'] = position_preds
        return masked_predictions
    
    def _empty_loss_dict(self, device: torch.device) -> Dict:
        return {
            'total_loss': torch.tensor(0.0, device=device),
            'gate_loss': torch.tensor(0.0, device=device),
            'position_loss': torch.tensor(0.0, device=device),
            'parameter_loss': torch.tensor(0.0, device=device),
            'gate_accuracy': torch.tensor(0.0, device=device)
        }

    def forward(self, predictions, targets, action_prediction_mask, eos_mask=None, num_qubits=None):
        """ í´ë¦°í•œ ì†ì‹¤ ê³„ì‚° (ë³µì¡í•œ ë¡œì§ ì œê±°)"""
        device = action_prediction_mask.device
        
        # ìƒˆë¡œìš´ íƒ€ê²Ÿ êµ¬ì¡° ì²˜ë¦¬
        if 'action_targets' in targets and targets['action_targets']:
            action_targets = targets['action_targets']
            
            # ActionLossComputer ì‚¬ìš©
            loss_computer = ActionLossComputer()
            combined_mask = action_prediction_mask & eos_mask if eos_mask is not None else action_prediction_mask
            
            return loss_computer.compute(predictions, action_targets, combined_mask, num_qubits=num_qubits)
        
        # ğŸ”¥ NEW: legacy íƒ€ê²Ÿ êµ¬ì¡°ë¥¼ ActionLossComputerë¡œ ë³€í™˜
        if 'target_actions' in targets:
            # legacy íƒ€ê²Ÿì„ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë³€í™˜
            action_targets = {
                'gate_targets': targets['target_actions'],
                'position_targets': targets.get('target_qubits'),
                'parameter_targets': targets.get('target_params')
            }
            
            # ActionLossComputer ì‚¬ìš©
            loss_computer = ActionLossComputer()
            combined_mask = action_prediction_mask & eos_mask if eos_mask is not None else action_prediction_mask
            
            return loss_computer.compute(predictions, action_targets, combined_mask, num_qubits=num_qubits)
        
        # FALLBACK: ê¸°ì¡´ ë³µì¡í•œ ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„±)
        return self._legacy_loss_computation(predictions, targets, action_prediction_mask, eos_mask, num_qubits)
    
    def _legacy_loss_computation(self, predictions, targets, action_prediction_mask, eos_mask, num_qubits):
        """ê¸°ì¡´ ë³µì¡í•œ ì†ì‹¤ ê³„ì‚° ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        device = action_prediction_mask.device
        total_loss = torch.tensor(0.0, device=device)
        loss_dict = {}
        
        # EOS í† í° ì´í›„ ìœ„ì¹˜ ë§ˆìŠ¤í‚¹
        eos_mask = self.create_eos_mask(target_gates)
        
        # ë””ë²„ê¹…: ë§ˆìŠ¤í¬ í¬ê¸° í™•ì¸
        if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
            print(f" MASK_DEBUG: action_prediction_mask.shape = {action_prediction_mask.shape}")
            print(f" MASK_DEBUG: eos_mask.shape = {eos_mask.shape}")
            print(f" MASK_DEBUG: target_gates.shape = {target_gates.shape if target_gates is not None else 'None'}")
        
        # ì•¡ì…˜ ì˜ˆì¸¡ ë§ˆìŠ¤í¬ì™€ EOS ë§ˆìŠ¤í¬ ê²°í•© (í¬ê¸° ë¶ˆì¼ì¹˜ ì‹œ ì—ëŸ¬ ë°œìƒ)
        combined_mask = action_prediction_mask & eos_mask
        
        # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì—ì„œë§Œ ì†ì‹¤ ê³„ì‚°
        mask_sum = combined_mask.sum()
        
        if mask_sum == 0:
            if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
                print(" WARNING: No valid predictions to compute loss!")
            return {
                'total_loss': total_loss,
                'gate_loss': torch.tensor(0.0, device=device),
                'qubit_loss': torch.tensor(0.0, device=device),
                'param_loss': torch.tensor(0.0, device=device),
                'gate_accuracy': torch.tensor(0.0, device=device),
                'num_predictions': torch.tensor(0, device=device)
            }
        
        masked_gate_logits = gate_type_logits[combined_mask]
        masked_gate_targets = target_gates[combined_mask]
        
        # ëª¨ë¸ ì˜ˆì¸¡ ë””ë²„ê·¸ (ì„ íƒì  ì¶œë ¥)
        if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
            # íŒ¨ë”© í† í°(0ë²ˆ) ì œì™¸í•˜ê³  ì‹¤ì œ ê²Œì´íŠ¸ë§Œ ë¶„ì„
            non_pad_mask = masked_gate_targets != 0
            non_pad_targets = masked_gate_targets[non_pad_mask]
            
            if len(non_pad_targets) > 0:
                unique_targets = torch.unique(non_pad_targets)
                target_dist = torch.bincount(non_pad_targets)
                print(f" REAL_TARGETS: unique={unique_targets.tolist()}, count={len(unique_targets)}")
                print(f" REAL_DIST: {target_dist.tolist()}")
                # Padding ratio calculation removed to avoid tensor access
            else:
                print(" No non-padding targets found!")
            
            # íë¹— ìœ„ì¹˜ íƒ€ê²Ÿ ë¶„ì„
            if target_qubits:
                for i, qubit_targets in enumerate(target_qubits):
                    if qubit_targets is not None:
                        masked_qubit_targets = qubit_targets[combined_mask]
                        valid_qubits = masked_qubit_targets[masked_qubit_targets >= 0]  # -1 ì œì™¸
                        if len(valid_qubits) > 0:
                            unique_qubits = torch.unique(valid_qubits)
                            print(f" QUBIT_{i}: unique={unique_qubits.tolist()}, count={len(unique_qubits)}")
            
            # íŒŒë¼ë¯¸í„° íƒ€ê²Ÿ ë¶„ì„
            if target_params:
                for i, param_targets in enumerate(target_params):
                    if param_targets is not None:
                        masked_param_targets = param_targets[combined_mask]
                        valid_params = masked_param_targets[~torch.isnan(masked_param_targets)]  # NaN ì œì™¸
                        if len(valid_params) > 0:
                            # Parameter range calculation removed to avoid tensor access
                            print(f"ğŸ¯ PARAM_{i}: range={param_range}, count={len(valid_params)}")
        
        gate_loss = self.cross_entropy(masked_gate_logits, masked_gate_targets)
        total_loss = total_loss + self.gate_weight * gate_loss
        loss_dict['gate_loss'] = gate_loss
        
        with torch.no_grad():
            gate_predictions = torch.argmax(masked_gate_logits, dim=-1)
            gate_accuracy = (gate_predictions == masked_gate_targets).float().mean()
            
            # ëª¨ë¸ ì˜ˆì¸¡ ë””ë²„ê·¸ (ì„ íƒì  ì¶œë ¥)
            if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
                unique_preds = torch.unique(gate_predictions)
                print(f"ğŸ¤– PREDICTIONS: unique={unique_preds.tolist()}, count={len(unique_preds)}")
                print(f"ğŸ“ˆ ACCURACY: gate={gate_accuracy:.4f}")
                
            loss_dict['gate_accuracy'] = gate_accuracy
        
        # 2. íë¹— ìœ„ì¹˜ ì†ì‹¤ (ìš”êµ¬ì‚¬í•­ 1: í†µí•© í…ì„œë¡œ ì²˜ë¦¬)
        if target_qubits is not None:
            # ìƒˆë¡œìš´ í†µí•© í…ì„œ êµ¬ì¡° ì²˜ë¦¬: [batch, num_gates, max_qubits_per_gate]
            if isinstance(target_qubits, torch.Tensor) and target_qubits.dim() == 3:
                # í†µí•© íë¹— ì˜ˆì¸¡: [num_actions, max_qubits_per_gate, max_qubits + 1]
                masked_qubit_logits = qubit_position_logits[combined_mask]
                
                # í†µí•© íë¹— íƒ€ê²Ÿ: [num_actions, max_qubits_per_gate]
                masked_qubit_targets = target_qubits[combined_mask]
                
                # ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì ìš©
                if num_qubits is not None:
                    num_actions = masked_qubit_logits.shape[0]
                    max_qubits_per_gate = masked_qubit_logits.shape[1]
                    
                    for action_idx in range(num_actions):
                        # ê° íšŒë¡œì˜ ì‹¤ì œ íë¹— ìˆ˜ë¥¼ ë„˜ëŠ” ì˜ˆì¸¡ì€ ë§ˆìŠ¤í‚¹
                        circuit_idx = action_idx  # ë°°ì¹˜ ë‚´ íšŒë¡œ ì¸ë±ìŠ¤ (ë‹¨ìˆœí™”)
                        if circuit_idx < len(num_qubits):
                            max_valid_qubit = num_qubits[circuit_idx] if isinstance(num_qubits[circuit_idx], int) else num_qubits[circuit_idx].item()
                            # ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ëŠ” ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
                            masked_qubit_logits[action_idx, qubit_pos, max_valid_qubit+1:] = -1e9
                
                # ìœ íš¨í•œ íƒ€ê²Ÿì´ ìˆëŠ” ìœ„ì¹˜ë§Œ ì†ì‹¤ ê³„ì‚°
                valid_mask = masked_qubit_targets != self.ignore_index
                if valid_mask.sum() > 0:
                    # Flatten for cross entropy: [num_valid_predictions, max_qubits + 1]
                    valid_logits = masked_qubit_logits[valid_mask]  # [num_valid, max_qubits + 1]
                    valid_targets = masked_qubit_targets[valid_mask]  # [num_valid]
                    
                    qubit_loss = self.cross_entropy(valid_logits, valid_targets)
                    total_loss = total_loss + self.qubit_weight * qubit_loss
                    loss_dict['qubit_loss'] = qubit_loss
                else:
                    loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
            
            # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
            elif isinstance(target_qubits, list) and len(target_qubits) > 0:
                # í†µí•© íë¹— ì˜ˆì¸¡: [num_actions, max_qubits_per_gate, max_qubits + 1]
                masked_qubit_logits = qubit_position_logits[combined_mask]
                
                # íƒ€ê²Ÿ íë¹—ë“¤ì„ í†µí•© í…ì„œë¡œ ë³€í™˜: [num_actions, max_qubits_per_gate]
                num_actions = masked_qubit_logits.shape[0]
                max_qubits_per_gate = masked_qubit_logits.shape[1]
                
                # í†µí•© íƒ€ê²Ÿ í…ì„œ ìƒì„±
                unified_qubit_targets = torch.full(
                    (num_actions, max_qubits_per_gate), 
                    self.ignore_index, 
                    dtype=torch.long, 
                    device=device
                )
                
                # ê° íë¹— ìœ„ì¹˜ë³„ íƒ€ê²Ÿì„ í†µí•© í…ì„œì— ë³µì‚¬
                for qubit_idx, qubit_targets in enumerate(target_qubits):
                    if qubit_targets is not None and qubit_idx < max_qubits_per_gate:
                        masked_targets = qubit_targets[combined_mask]
                        unified_qubit_targets[:, qubit_idx] = masked_targets
                
                # ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì ìš©
                if num_qubits is not None:
                    for action_idx in range(num_actions):
                        for qubit_pos in range(max_qubits_per_gate):
                            # ê° íšŒë¡œì˜ ì‹¤ì œ íë¹— ìˆ˜ë¥¼ ë„˜ëŠ” ì˜ˆì¸¡ì€ ë§ˆìŠ¤í‚¹
                            circuit_idx = action_idx  # ë°°ì¹˜ ë‚´ íšŒë¡œ ì¸ë±ìŠ¤ (ë‹¨ìˆœí™”)
                            if circuit_idx < len(num_qubits):
                                max_valid_qubit = num_qubits[circuit_idx]
                                # ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ëŠ” ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
                                masked_qubit_logits[action_idx, qubit_pos, max_valid_qubit+1:] = -1e9
                
                # ìœ íš¨í•œ íƒ€ê²Ÿì´ ìˆëŠ” ìœ„ì¹˜ë§Œ ì†ì‹¤ ê³„ì‚°
                valid_mask = unified_qubit_targets != self.ignore_index
                if valid_mask.sum() > 0:
                    # Flatten for cross entropy: [num_valid_predictions, max_qubits + 1]
                    valid_logits = masked_qubit_logits[valid_mask]  # [num_valid, max_qubits + 1]
                    valid_targets = unified_qubit_targets[valid_mask]  # [num_valid]
                    
                    qubit_loss = self.cross_entropy(valid_logits, valid_targets)
                    total_loss = total_loss + self.qubit_weight * qubit_loss
                    loss_dict['qubit_loss'] = qubit_loss
                else:
                    loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
            else:
                loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
        else:
            loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
        
        # 3. íŒŒë¼ë¯¸í„° ì†ì‹¤ (ìš”êµ¬ì‚¬í•­ 2: í†µí•© í…ì„œë¡œ ì²˜ë¦¬)
        if target_params is not None:
            # ìƒˆë¡œìš´ í†µí•© í…ì„œ êµ¬ì¡° ì²˜ë¦¬: [batch, num_gates, max_params_per_gate]
            if isinstance(target_params, torch.Tensor) and target_params.dim() == 3:
                # í†µí•© íŒŒë¼ë¯¸í„° ì˜ˆì¸¡: [num_actions, max_params_per_gate]
                masked_param_preds = parameter_predictions[combined_mask]
                
                # í†µí•© íŒŒë¼ë¯¸í„° íƒ€ê²Ÿ: [num_actions, max_params_per_gate]
                masked_param_targets = target_params[combined_mask]
                
                # ìœ íš¨í•œ íŒŒë¼ë¯¸í„° íƒ€ê²Ÿë§Œ ì†ì‹¤ ê³„ì‚° (NaN ì œì™¸)
                valid_mask = ~torch.isnan(masked_param_targets)
                non_padding_mask = position_targets != -1
        if non_padding_mask.sum() > 0:
            # ğŸš¨ ê°œë³„ íšŒë¡œ íë¹— ìˆ˜ ê¸°ë°˜ ë™ì  ì •ê·œí™”
            if num_qubits is not None and len(num_qubits) > 0:
                # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì˜ íë¹— ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”
                batch_size = len(num_qubits)
                seq_len = position_targets.size(0) // batch_size
                # í†µí•© íŒŒë¼ë¯¸í„° ì˜ˆì¸¡: [num_actions, max_params_per_gate]
                masked_param_preds = parameter_predictions[combined_mask]
                
                # íƒ€ê²Ÿ íŒŒë¼ë¯¸í„°ë“¤ì„ í†µí•© í…ì„œë¡œ ë³€í™˜: [num_actions, max_params_per_gate]
                num_actions = masked_param_preds.shape[0]
                max_params_per_gate = masked_param_preds.shape[1]
                
                # í†µí•© íƒ€ê²Ÿ í…ì„œ ìƒì„± (NaNìœ¼ë¡œ ì´ˆê¸°í™”)
                unified_param_targets = torch.full(
                    (num_actions, max_params_per_gate), 
                    float('nan'), 
                    dtype=torch.float32, 
                    device=device
                )
                
                # ê° íŒŒë¼ë¯¸í„°ë³„ íƒ€ê²Ÿì„ í†µí•© í…ì„œì— ë³µì‚¬
                for param_idx, param_targets in enumerate(target_params):
                    if param_targets is not None and param_idx < max_params_per_gate:
                        masked_targets = param_targets[combined_mask]
                        unified_param_targets[:, param_idx] = masked_targets
                
                # ìœ íš¨í•œ íŒŒë¼ë¯¸í„° íƒ€ê²Ÿë§Œ ì†ì‹¤ ê³„ì‚° (NaN ì œì™¸)
                valid_mask = ~torch.isnan(unified_param_targets)
                if valid_mask.sum() > 0:
                    valid_preds = masked_param_preds[valid_mask]
                    valid_targets = unified_param_targets[valid_mask]
                    
                    param_loss = self.mse_loss(valid_preds, valid_targets)
                    total_loss = total_loss + self.param_weight * param_loss
                    loss_dict['param_loss'] = param_loss
                else:
                    loss_dict['param_loss'] = torch.tensor(0.0, device=device)
            else:
                loss_dict['param_loss'] = torch.tensor(0.0, device=device)
        else:
            loss_dict['param_loss'] = torch.tensor(0.0, device=device)
        
        # ğŸ”¥ CRITICAL: gate_accuracyê°€ í•­ìƒ í¬í•¨ë˜ë„ë¡ ë³´ì¥
        if 'gate_accuracy' not in loss_dict:
            loss_dict['gate_accuracy'] = torch.tensor(0.0, device=device)
        
        loss_dict.update({
            'total_loss': total_loss,
            'num_predictions': combined_mask.sum()
        })
        
        return loss_dict


# ëª¨ë¸ íŒ©í† ë¦¬ í•¨ìˆ˜
def create_decision_transformer(
    config = None,
    d_model: int = 512,
    n_layers: int = 6,
    n_heads: int = 8,
    n_gate_types: int = 20,
    dropout: float = 0.1,
    property_prediction_model: Optional[UnifiedPropertyPredictionTransformer] = None
) -> DecisionTransformer:
    """Decision Transformer ëª¨ë¸ ìƒì„±"""
    
    # configê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì²˜ë¦¬
    if config is not None:
        if isinstance(config, dict):
            d_model = config.get('d_model')
            n_layers = config.get('n_layers')
            n_heads = config.get('n_heads')
            dropout = config.get('dropout')
            attention_mode = config.get('attention_mode', 'standard')
            d_ff = config.get('d_ff', d_model * 4)
            property_prediction_model = config.get('property_prediction_model', property_prediction_model)
        else:
            # config ê°ì²´ì¸ ê²½ìš°
            d_model = getattr(config, 'd_model')
            n_layers = getattr(config, 'n_layers')
            n_heads = getattr(config, 'n_heads')
            dropout = getattr(config, 'dropout')
            attention_mode = getattr(config, 'attention_mode', 'standard')
            d_ff = getattr(config, 'd_ff', d_model * 4)
    else:
        d_ff = d_model * 4  # í‘œì¤€ ë¹„ìœ¨
        attention_mode = 'standard'
    
    return DecisionTransformer(
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        d_ff=d_ff,
        n_gate_types=n_gate_types,
        dropout=dropout,
        attention_mode=attention_mode,
        property_prediction_model=property_prediction_model
    )


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ìƒì„±
    model = create_decision_transformer(
        d_model=256,
        n_layers=4,
        n_heads=8,
        n_gate_types=20  # ğŸ”§ FIXED: í†µì¼ëœ ê²Œì´íŠ¸ íƒ€ì… ìˆ˜
    )
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    batch_size, seq_len, d_model = 2, 10, 256
    
    input_sequence = torch.randn(batch_size, seq_len, d_model)
    attention_mask = torch.tril(torch.ones(batch_size, seq_len, seq_len, dtype=torch.bool))
    action_prediction_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)
    action_prediction_mask[:, 1::3] = True  # ì•¡ì…˜ ìœ„ì¹˜
    
    # ìˆœì „íŒŒ
    outputs = model(input_sequence, attention_mask, action_prediction_mask)
    
    # Remove debug prints that access tensor values during training
    
    # ğŸ¯ ì†ì‹¤ ê³„ì‚° í…ŒìŠ¤íŠ¸ (ìƒˆë¡œìš´ êµ¬ì¡°)
    # ë”ë¯¸ íƒ€ê²Ÿ ë°ì´í„° ìƒì„±
    targets = {
        'gate_targets': torch.randint(0, 16, (batch_size, seq_len)),
        'position_targets': torch.randn(batch_size, seq_len, 3),
        'parameter_targets': torch.randn(batch_size, seq_len)
    }
    
    # ëª¨ë¸ì˜ compute_loss ë©”ì„œë“œ ì‚¬ìš©
    loss_outputs = model.compute_loss(
        predictions=outputs,
        targets=targets,
        action_prediction_mask=action_prediction_mask
    )
    
    # Loss output printing removed to avoid tensor access during training
