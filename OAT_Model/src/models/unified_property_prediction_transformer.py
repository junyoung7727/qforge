"""
ì™„ì „ í†µí•©ëœ Property Prediction Transformer
ë ˆê±°ì‹œ ì˜ì¡´ì„± ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„ëœ ìµœì¢… ë²„ì „
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
# ë™ì  ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
import sys
from pathlib import Path

# í˜„ì¬ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ì„ ê²½ë¡œì— ì¶”ê°€
current_file = Path(__file__).resolve()
src_dir = current_file.parent.parent
project_root = src_dir.parent
sys.path.insert(0, str(src_dir))
sys.path.insert(0, str(project_root / "quantumcommon"))

# UnifiedEmbeddingFacade ì„í¬íŠ¸
try:
    from encoding.unified_embedding_facade import UnifiedEmbeddingFacade
except ImportError:
    try:
        from ..encoding.unified_embedding_facade import UnifiedEmbeddingFacade
    except ImportError:
        from OAT_Model.src.encoding.unified_embedding_facade import UnifiedEmbeddingFacade

# OptimalPropertyHead ì„í¬íŠ¸
try:
    from models.optimal_property_prediction_head import OptimalPropertyHead, OptimalPropertyLoss
except ImportError:
    try:
        from .optimal_property_prediction_head import OptimalPropertyHead, OptimalPropertyLoss
    except ImportError:
        from OAT_Model.src.models.optimal_property_prediction_head import OptimalPropertyHead, OptimalPropertyLoss

# ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì„í¬íŠ¸
try:
    from gates import QuantumGateRegistry
except ImportError:
    try:
        from quantumcommon.gates import QuantumGateRegistry
    except ImportError:
        print("Warning: Could not import QuantumGateRegistry")
        QuantumGateRegistry = None

# Import unified config instead of defining separately
try:
    from config.unified_training_config import PropertyConfig as UnifiedPropertyPredictionConfig
except ImportError:
    try:
        from ..config.unified_training_config import PropertyConfig as UnifiedPropertyPredictionConfig
    except ImportError:
        from OAT_Model.src.config.unified_training_config import PropertyConfig as UnifiedPropertyPredictionConfig


class TransformerBlock(nn.Module):
    """íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡"""
    def __init__(self, config: UnifiedPropertyPredictionConfig):
        super().__init__()
        self.config = config
        
        # Multi-head attention
        self.attention = nn.MultiheadAttention(
            embed_dim=config.d_model,
            num_heads=config.n_heads,
            dropout=config.dropout,
            batch_first=True
        )
        
        # Feed-forward network
        self.ffn = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_ff, config.d_model),
            nn.Dropout(config.dropout)
        )
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(config.d_model)
        self.norm2 = nn.LayerNorm(config.d_model)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Self-attention with residual connection
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # Feed-forward with residual connection
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        return x


class UnifiedPropertyPredictionTransformer(nn.Module):
    """ì™„ì „ í†µí•©ëœ Property Prediction Transformer"""
    
    def __init__(self, config: UnifiedPropertyPredictionConfig):
        super().__init__()
        self.config = config
        
        # Circuit embedding with unified facade - pass config object
        embedding_config = {
            'd_model': config.d_model,
            'max_qubits': config.max_qubits,
            'max_gates': config.max_gates,
            'num_heads': config.n_heads,
            'dropout': config.dropout
        }
        self.circuit_embedding = UnifiedEmbeddingFacade(embedding_config)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
        self.transformer_layers = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ])
        
        # SOTA Prediction Head
        self.prediction_head = OptimalPropertyHead(
            d_model=config.d_model,
            dropout=config.dropout
        )
        
        # SOTA Loss Function
        self.loss_function = OptimalPropertyLoss(
            property_weights={
                'entanglement': 1.0,
                'fidelity': 1.0,
                'expressibility': 1.0
            }
        )
        
        # ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def _process_circuit_specs(self, circuit_specs):
        """CircuitSpec ê°ì²´ë“¤ì„ í…ì„œ í˜•íƒœë¡œ ë³€í™˜ - gate vocab ì‚¬ìš©"""
        if not isinstance(circuit_specs, list):
            circuit_specs = [circuit_specs]
        
        batch_size = len(circuit_specs)
        max_gates = max(len(spec.gates) for spec in circuit_specs) if circuit_specs else 0
        max_gates = min(max_gates, self.config.max_gates)  # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        
        # Gate registryì—ì„œ í†µì¼ëœ vocab ì‚¬ìš©
        gate_registry = QuantumGateRegistry()
        gate_vocab = gate_registry.get_gate_vocab()
        
        # Get device from model parameters
        device = next(self.parameters()).device
        
        gate_types = torch.zeros(batch_size, max_gates, dtype=torch.long, device=device)
        qubit_indices = torch.zeros(batch_size, max_gates, 2, dtype=torch.long, device=device)
        parameters = torch.zeros(batch_size, max_gates, 3, device=device)
        
        for i, spec in enumerate(circuit_specs):
            for j, gate in enumerate(spec.gates[:max_gates]):
                # ê²Œì´íŠ¸ íƒ€ì… - unified vocab ì‚¬ìš©
                gate_types[i, j] = gate_vocab.get(gate.name.lower(), 0)
                
                # íë¹— ì¸ë±ìŠ¤
                if gate.qubits:
                    qubit_indices[i, j, :len(gate.qubits[:2])] = torch.tensor(gate.qubits[:2], device=device)
                
                # íŒŒë¼ë¯¸í„°
                if gate.parameters:
                    parameters[i, j, :len(gate.parameters[:3])] = torch.tensor(gate.parameters[:3], device=device)
        
        return {
            'gate_types': gate_types,
            'qubit_indices': qubit_indices,
            'parameters': parameters
        }

    def forward(self, circuit_spec, targets: Dict[str, torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """Forward pass"""
        # CircuitSpecì„ í…ì„œ í˜•íƒœë¡œ ë³€í™˜ (ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        if not isinstance(circuit_spec, list):
            circuit_spec = [circuit_spec]
        circuit_data = self._process_circuit_specs(circuit_spec)
        
        # íšŒë¡œ ì„ë² ë”©
        embedding_output = self.circuit_embedding(circuit_data)
        
        # Extract main embeddings tensor from dictionary
        embedded = embedding_output['embeddings'] if isinstance(embedding_output, dict) else embedding_output
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ í†µê³¼
        x = embedded
        for layer in self.transformer_layers:
            x = layer(x)
        
        # ì‹œí€€ìŠ¤ í’€ë§ (í‰ê· )
        pooled = x.mean(dim=1)  # [batch_size, d_model]
        
        # ì†ì„± ì˜ˆì¸¡ (targets ì „ë‹¬í•˜ì—¬ ëŸ¬ë‹ í†µê³„ ì—…ë°ì´íŠ¸)
        # ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” ìë™ìœ¼ë¡œ denormalization ìˆ˜í–‰
        inference_mode = not self.training
        predictions = self.prediction_head(pooled, targets=targets, inference_mode=inference_mode)
        
        # ìˆ˜ì¹˜ ì•ˆì •ì„± ê²€ì¦
        if self.config.numerical_stability:
            predictions = self._ensure_numerical_stability(predictions)
        
        return predictions
    
    def _ensure_numerical_stability(self, predictions: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ìˆ˜ì¹˜ ì•ˆì •ì„± ë³´ì¥"""
        stable_predictions = {}
        
        for key, value in predictions.items():
            if torch.isnan(value).any() or torch.isinf(value).any():
                print(f"âš ï¸ {key}ì—ì„œ NaN/Inf ê°ì§€, 0ìœ¼ë¡œ ëŒ€ì²´")
                stable_predictions[key] = torch.zeros_like(value)
            else:
                stable_predictions[key] = value
        
        return stable_predictions
    
    def compute_loss(self, predictions: Dict[str, torch.Tensor], 
                    targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """SOTA ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©"""
        # Pass expressibility mean and std from prediction_head to loss_function
        exp_mean = getattr(self.prediction_head, 'exp_mean', None)
        exp_std = getattr(self.prediction_head, 'exp_std', None)
        
        total_loss, individual_losses = self.loss_function(
            predictions, 
            targets, 
            exp_mean=exp_mean,
            exp_std=exp_std
        )
        
        return {
            'total': total_loss,
            **individual_losses
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_type': 'UnifiedPropertyPredictionTransformer',
            'architecture': 'SOTA Unified (Graph + Transformer + Cross-Attention)',
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'config': {
                'd_model': self.config.d_model,
                'n_layers': self.config.n_layers,
                'n_heads': self.config.n_heads,
                'cross_attention_heads': self.config.cross_attention_heads,
                'consistency_loss_weight': self.config.consistency_loss_weight,
                'dropout': self.config.dropout
            },
            'features': [
                'Multi-Scale Feature Extraction',
                'Cross-Property Attention',
                'Property-Specific Decoders',
                'Consistency Loss',
                'Advanced Regularization',
                'Numerical Stability'
            ]
        }


# ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
IntegratedPropertyPredictionTransformer = UnifiedPropertyPredictionTransformer
IntegratedPropertyPredictionConfig = UnifiedPropertyPredictionConfig
PropertyPredictionTransformer = UnifiedPropertyPredictionTransformer
PropertyPredictionConfig = UnifiedPropertyPredictionConfig

# ì†ì‹¤ í•¨ìˆ˜ ë³„ì¹­
IntegratedPropertyPredictionLoss = OptimalPropertyLoss
PropertyPredictionLoss = OptimalPropertyLoss


def create_property_prediction_model(config: Optional[UnifiedPropertyPredictionConfig] = None) -> UnifiedPropertyPredictionTransformer:
    """í†µí•©ëœ Property Prediction ëª¨ë¸ ìƒì„±"""
    if config is None:
        config = UnifiedPropertyPredictionConfig()  # í†µí•© ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
    
    model = UnifiedPropertyPredictionTransformer(config)
    
    print("ğŸš€ í†µí•©ëœ Property Prediction Transformer ìƒì„± ì™„ë£Œ!")
    print("=" * 60)
    
    model_info = model.get_model_info()
    print(f"ğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"   â€¢ ì•„í‚¤í…ì²˜: {model_info['architecture']}")
    print(f"   â€¢ ì´ íŒŒë¼ë¯¸í„°: {model_info['total_parameters']:,}")
    print(f"   â€¢ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {model_info['trainable_parameters']:,}")
    print(f"   â€¢ d_model: {model_info['config']['d_model']}")
    print(f"   â€¢ n_layers: {model_info['config']['n_layers']}")
    print(f"   â€¢ n_heads: {model_info['config']['n_heads']}")
    
    print(f"\nğŸ¯ SOTA ê¸°ëŠ¥:")
    for feature in model_info['features']:
        print(f"   âœ… {feature}")
    
    return model


# ë ˆê±°ì‹œ í˜¸í™˜ì„± í•¨ìˆ˜
create_integrated_model = create_property_prediction_model


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ§ª í†µí•©ëœ Property Prediction ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    
    config = UnifiedPropertyPredictionConfig()  # í†µí•© ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
    
    model = create_property_prediction_model(config)
    print("\nâœ… í†µí•©ëœ Property Prediction Transformer í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
