"""
Modular RTG (Return-To-Go) Calculator
íš¨ìœ¨ì ì´ê³  ëª¨ë“ˆí™”ëœ RTG ê³„ì‚° ì‹œìŠ¤í…œ
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional, Any, Union
from collections import deque
from abc import ABC, abstractmethod
from pathlib import Path
import sys
import math
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "src"))
# Removed unused import: EncodingPipelineFactory (legacy graph-based encoding)

# Add project paths
sys.path.append(str(Path(__file__).parent.parent.parent))

from rtg.model_loader import load_property_predictor, find_best_checkpoint


class PropertyPredictor(ABC):
    """Property ì˜ˆì¸¡ì„ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def predict(self, circuit_spec) -> Dict[str, float]:
        """íšŒë¡œ ìŠ¤í™ìœ¼ë¡œë¶€í„° ì†ì„± ì˜ˆì¸¡"""
        pass


class ModelBasedPropertyPredictor(PropertyPredictor):
    """í•™ìŠµëœ ëª¨ë¸ ê¸°ë°˜ Property ì˜ˆì¸¡ê¸°"""
    
    def __init__(self, checkpoint_path: Optional[str] = None, device: str = "auto", loaded_model = None):
        """
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (Noneì´ë©´ ìë™ ê²€ìƒ‰)
            device: ë””ë°”ì´ìŠ¤ ì„¤ì •
            loaded_model: ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ (ìˆëŠ” ê²½ìš° ì‚¬ìš©)
        """
        if loaded_model is not None:
            self.model = loaded_model
            print(f"âœ… ì´ë¯¸ ë¡œë“œëœ Property prediction ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
        else:
            if checkpoint_path is None:
                checkpoint_path = find_best_checkpoint()
                if checkpoint_path is None:
                    raise FileNotFoundError("Property predictor ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            self.model = load_property_predictor(checkpoint_path, device)
        
        self.device = self.model.device if hasattr(self.model, 'device') else torch.device(device)
        
    def predict(self, circuit_spec) -> Dict[str, float]:
        """í˜„ì¬ ì•„í‚¤í…ì²˜ ê¸°ë°˜ íšŒë¡œ ìŠ¤í™ìœ¼ë¡œë¶€í„° ì†ì„± ì˜ˆì¸¡"""
        with torch.no_grad():
            # í˜„ì¬ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ì— ë§ê²Œ ìˆ˜ì •
            if hasattr(circuit_spec, 'to'):
                circuit_spec = circuit_spec.to(self.device)
            
            # ëª¨ë¸ forward í˜¸ì¶œ (eval ëª¨ë“œì—ì„œ ìë™ìœ¼ë¡œ denormalizationë¨)
            self.model.eval()  # ì¶”ë¡  ëª¨ë“œ ì„¤ì •

            outputs = self.model(circuit_spec)

            
            # í˜„ì¬ ì•„í‚¤í…ì²˜ ì¶œë ¥ ì²˜ë¦¬ (Dict[str, torch.Tensor])
            if isinstance(outputs, dict):
                if 'expressibility' in outputs and 'expressibility' in outputs and 'expressibility' in outputs:
                    entanglement = outputs['entanglement'].squeeze().item() 
                    fidelity = outputs['fidelity'].squeeze().item() 
                    expressibility = outputs['expressibility'].squeeze().item()

            # NaN/Inf ê²€ì¦ ë° ì²˜ë¦¬
            if torch.isnan(torch.tensor(entanglement)) or torch.isinf(torch.tensor(entanglement)):
                print("âš ï¸ entanglementì—ì„œ NaN/Inf ê°ì§€, 0ìœ¼ë¡œ ëŒ€ì²´")
                entanglement = 0.0
            if torch.isnan(torch.tensor(fidelity)) or torch.isinf(torch.tensor(fidelity)):
                print("âš ï¸ fidelityì—ì„œ NaN/Inf ê°ì§€, 0ìœ¼ë¡œ ëŒ€ì²´")
                fidelity = 0.0
            if torch.isnan(torch.tensor(expressibility)) or torch.isinf(torch.tensor(expressibility)):
                print("âš ï¸ expressibilityì—ì„œ NaN/Inf ê°ì§€, 0ìœ¼ë¡œ ëŒ€ì²´")
                expressibility = 0.0
            
            # ìˆ˜ì¹˜ ì•ˆì •ì„± ê²€ì¦ (expressibilityëŠ” unbounded)
            result = {
                'entanglement': max(0.0, min(1.0, entanglement)),
                'fidelity': max(0.0, min(1.0, fidelity)),
                'expressibility': max(0.0, expressibility)  # expressibilityëŠ” [0, âˆ) ë²”ìœ„
            }
            
            return result
    
    # Removed _prepare_circuit_data() method - dead code from legacy graph-based architecture
    # Current UnifiedPropertyPredictionTransformer handles CircuitSpec directly via _process_circuit_specs()
            

class AdaptiveGaussianRewardCalculator:
    """ì ì‘ì  ê°€ìš°ì‹œì•ˆ ë¦¬ì›Œë“œ ê³„ì‚°ê¸°"""
    
    def __init__(self, property_weights: Optional[Dict[str, float]] = None,
                 property_sigmas: Optional[Dict[str, float]] = None):
        """
        Args:
            property_weights: ì†ì„±ë³„ ê°€ì¤‘ì¹˜
            property_sigmas: ì†ì„±ë³„ ê¸°ë³¸ ì‹œê·¸ë§ˆ ê°’
        """
        self.property_weights = property_weights or {
            'entanglement': 0.3,
            'expressibility': 0.4,
            'fidelity': 0.3
        }
        self.property_sigmas = property_sigmas or {
            'entanglement': 0.1,
            'expressibility': 0.1,
            'fidelity': 0.1
        }
        self.prediction_history = {}  # ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ ì €ì¥
    
    def calculate_gaussian_reward(self, predicted_properties: Dict[str, float],
                                target_properties: Dict[str, float],
                                prediction_history: Optional[Dict[str, List[float]]] = None) -> float:
        """
        ì ì‘ì  ê°€ìš°ì‹œì•ˆ ë¦¬ì›Œë“œ ê³„ì‚°
        
        Args:
            predicted_properties: ì˜ˆì¸¡ëœ ì†ì„±ê°’ë“¤
            target_properties: íƒ€ê²Ÿ ì†ì„±ê°’ë“¤
            prediction_history: ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ (ì„ íƒì )
            
        Returns:
            ê³„ì‚°ëœ ë¦¬ì›Œë“œ
        """
        total_reward = 0.0
        
        for prop_name in self.property_weights:
            if prop_name in predicted_properties and prop_name in target_properties:
                pred_val = predicted_properties[prop_name]
                target_val = target_properties[prop_name]
                weight = self.property_weights[prop_name]
                
                # ê±°ë¦¬ ê³„ì‚°
                distance = abs(pred_val - target_val)
                
                # ì ì‘ì  sigma ê³„ì‚° (ì„ íƒì )
                if prediction_history and prop_name in prediction_history:
                    # ìµœê·¼ ì˜ˆì¸¡ ì˜¤ì°¨ë“¤ì˜ í‘œì¤€í¸ì°¨ë¥¼ sigmaë¡œ ì‚¬ìš©
                    recent_errors = prediction_history[prop_name][-10:]  # ìµœê·¼ 10ê°œ
                    adaptive_sigma = max(np.std(recent_errors), 0.01)  # ìµœì†Œê°’ ë³´ì¥
                else:
                    # ê¸°ë³¸ sigma ì‚¬ìš©
                    adaptive_sigma = self.property_sigmas.get(prop_name, 0.1)
                
                # ê°€ìš°ì‹œì•ˆ ë¦¬ì›Œë“œ ê³„ì‚°
                gaussian_reward = math.exp(-(distance**2) / (2 * adaptive_sigma**2))
                
                # ê°€ì¤‘ì¹˜ ì ìš©
                reward = weight * gaussian_reward
                total_reward += reward
        
        return total_reward
    
    def calculate_reward(self, predicted_properties: Dict[str, float],
                        target_properties: Dict[str, float]) -> float:
        """ê¸°ë³¸ ë¦¬ì›Œë“œ ê³„ì‚° (í•­ìƒ ì ì‘í˜• ì‚¬ìš©)"""
        return self.calculate_gaussian_reward(predicted_properties, target_properties, self.prediction_history)
    
    def update_prediction_history(self, prop_name: str, error: float):
        """ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        if prop_name not in self.prediction_history:
            self.prediction_history[prop_name] = []
        self.prediction_history[prop_name].append(error)
        # ìµœëŒ€ 50ê°œê¹Œì§€ë§Œ ìœ ì§€
        if len(self.prediction_history[prop_name]) > 50:
            self.prediction_history[prop_name] = self.prediction_history[prop_name][-50:]
    
    def calculate_step_reward(self,
                            current_properties: Dict[str, float],
                            previous_properties: Dict[str, float],
                            target_properties: Dict[str, float]) -> float:
        """
        ìŠ¤í…ë³„ ë¦¬ì›Œë“œ ê³„ì‚° (ê°œì„ ë„ ê¸°ë°˜, ê°€ìš°ì‹œì•ˆ ì ìš©)
        
        Args:
            current_properties: í˜„ì¬ ìŠ¤í…ì˜ ì†ì„±ê°’ë“¤
            previous_properties: ì´ì „ ìŠ¤í…ì˜ ì†ì„±ê°’ë“¤
            target_properties: íƒ€ê²Ÿ ì†ì„±ê°’ë“¤
            
        Returns:
            ê³„ì‚°ëœ ìŠ¤í… ë¦¬ì›Œë“œ
        """
        # í˜„ì¬ì™€ ì´ì „ ìŠ¤í…ì˜ ê°€ìš°ì‹œì•ˆ ë¦¬ì›Œë“œ ê³„ì‚°
        current_reward = self.calculate_gaussian_reward(current_properties, target_properties, self.prediction_history)
        previous_reward = self.calculate_gaussian_reward(previous_properties, target_properties, self.prediction_history)
        
        # ê°œì„ ë„ ê¸°ë°˜ ë¦¬ì›Œë“œ (ê°œì„ ë˜ë©´ ì–‘ìˆ˜, ì•…í™”ë˜ë©´ ìŒìˆ˜)
        improvement = current_reward - previous_reward
        
        # ë² ì´ìŠ¤ë¼ì¸ ë¦¬ì›Œë“œ ì¶”ê°€ (0 ìˆ˜ë ´ ë°©ì§€)
        baseline_reward = 0.1
        step_reward = improvement + baseline_reward
        
        # ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        for prop_name in target_properties:
            if prop_name in current_properties:
                error = abs(current_properties[prop_name] - target_properties[prop_name])
                self.update_prediction_history(prop_name, error)
        
        return step_reward


class RTGCalculator:
    """ëª¨ë“ˆí™”ëœ RTG ê³„ì‚°ê¸°"""
    
    def __init__(self, 
                 property_predictor: PropertyPredictor,
                 reward_calculator: Optional[AdaptiveGaussianRewardCalculator] = None):
        """
        Args:
            property_predictor: ì†ì„± ì˜ˆì¸¡ê¸°
            reward_calculator: ë¦¬ì›Œë“œ ê³„ì‚°ê¸°
        """
        self.property_predictor = property_predictor
        self.reward_calculator = reward_calculator or AdaptiveGaussianRewardCalculator()
    
    def calculate_rtg_sequence(self,
                             circuit_specs: List,
                             target_properties: Dict[str, float],
                             gamma: float = 0.99) -> List[float]:
        """
        íšŒë¡œ ì‹œí€€ìŠ¤ì— ëŒ€í•œ RTG ê³„ì‚°
        
        Args:
            circuit_specs: íšŒë¡œ ìŠ¤í™ ë¦¬ìŠ¤íŠ¸
            target_properties: íƒ€ê²Ÿ ì†ì„±ê°’ë“¤
            gamma: í• ì¸ ì¸ì
            
        Returns:
            RTG ê°’ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        # ê° ìŠ¤í…ë³„ ì†ì„± ì˜ˆì¸¡
        predicted_properties_sequence = []
        total_circuits = len(circuit_specs)
        for spec in circuit_specs:
            # ë¹ˆ íšŒë¡œ(ê²Œì´íŠ¸ 0ê°œ)ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© - NaN ë°©ì§€
            if not hasattr(spec, 'gates') or len(spec.gates) == 0:
                props = {'entanglement': 0.0, 'fidelity': 0.0, 'expressibility': 0.0}
            else:
                # UnifiedPropertyPredictionTransformerëŠ” forward ë©”ì„œë“œ ì‚¬ìš©
                if hasattr(self.property_predictor, 'predict'):
                    props = self.property_predictor.predict(spec)
                else:
                    # forward ë©”ì„œë“œ ì‚¬ìš© (UnifiedPropertyPredictionTransformer)
                    with torch.no_grad():
                        output = self.property_predictor.forward(spec)
                        # ì¶œë ¥ì—ì„œ ì†ì„±ê°’ ì¶”ì¶œ
                        if isinstance(output, dict):
                            props = {
                                'entanglement': float(output.get('entanglement', 0.5)),
                                'fidelity': float(output.get('fidelity', 0.5)),
                                'expressibility': float(output.get('expressibility', 0.5))
                            }
                        else:
                            # ê¸°ë³¸ê°’ ì‚¬ìš©
                            props = {'entanglement': 0.5, 'fidelity': 0.5, 'expressibility': 0.5}
            
            predicted_properties_sequence.append(props)
        
        # ë¦¬ì›Œë“œ ê³„ì‚° (ì ì‘í˜• íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ í¬í•¨)
        rewards = []
        for i, props in enumerate(predicted_properties_sequence):
            # ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ëª¨ë“  ìŠ¤í…ì—ì„œ)
            for prop_name in target_properties:
                if prop_name in props:
                    error = abs(props[prop_name] - target_properties[prop_name])
                    self.reward_calculator.update_prediction_history(prop_name, error)
            
            if i == 0:
                # ì²« ë²ˆì§¸ ìŠ¤í…ì€ ì ˆëŒ€ ë¦¬ì›Œë“œ (ì ì‘í˜• ì‚¬ìš©)
                reward = self.reward_calculator.calculate_gaussian_reward(
                    props, target_properties, self.reward_calculator.prediction_history
                )
            else:
                # ì´í›„ ìŠ¤í…ì€ ê°œì„ ë„ ê¸°ë°˜ ë¦¬ì›Œë“œ (ì ì‘í˜• ì‚¬ìš©)
                reward = self.reward_calculator.calculate_step_reward(
                    props, predicted_properties_sequence[i-1], target_properties
                )
            rewards.append(reward)
        
        # RTG ê³„ì‚° (ì—­ìˆœìœ¼ë¡œ) - ë§ˆì§€ë§‰ ìŠ¤í…ì—ì„œ ì‹œì‘í•˜ì—¬ ëˆ„ì 
        rtg_values = []
        rtg = 0.0
        
        for reward in reversed(rewards):
            rtg = reward + gamma * rtg
            rtg_values.append(rtg)
        
        # ìˆœì„œ ë³µì› (ì²« ë²ˆì§¸ ìŠ¤í…ì´ ê°€ì¥ ë†’ì€ RTG ê°’ì„ ê°€ì§)
        rtg_values.reverse()
        
        # RTG ê°’ì´ ëª¨ë‘ 0ì— ìˆ˜ë ´í•˜ì§€ ì•Šë„ë¡ ìµœì†Œê°’ ë³´ì¥
        min_rtg = 0.1
        rtg_values = [max(rtg, min_rtg) for rtg in rtg_values]
        
        return rtg_values
    
    def calculate_episode_rtg(self,
                            episode_data: Dict[str, Any],
                            target_properties: Dict[str, float]) -> Dict[str, List[float]]:
        """
        ì—í”¼ì†Œë“œ ì „ì²´ì— ëŒ€í•œ RTG ê³„ì‚°
        
        Args:
            episode_data: ì—í”¼ì†Œë“œ ë°ì´í„°
            target_properties: íƒ€ê²Ÿ ì†ì„±ê°’ë“¤
            
        Returns:
            RTG ê°’ë“¤ê³¼ ë©”íƒ€ë°ì´í„°
        """
        circuit_specs = episode_data.get('circuit_specs', [])
        
        if not circuit_specs:
            return {'rtg_values': [], 'rewards': [], 'properties': []}
        
        # RTG ê³„ì‚°
        rtg_values = self.calculate_rtg_sequence(circuit_specs, target_properties)
        
        # ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
        properties_sequence = []
        for spec in circuit_specs:
            props = self.property_predictor.predict(spec)
            properties_sequence.append(props)
        
        # ë¦¬ì›Œë“œ ì‹œí€€ìŠ¤ ê³„ì‚° (ì ì‘í˜• íˆìŠ¤í† ë¦¬ ì‚¬ìš©)
        rewards = []
        for i, props in enumerate(properties_sequence):
            # ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            for prop_name in target_properties:
                if prop_name in props:
                    error = abs(props[prop_name] - target_properties[prop_name])
                    self.reward_calculator.update_prediction_history(prop_name, error)
            
            if i == 0:
                reward = self.reward_calculator.calculate_gaussian_reward(
                    props, target_properties, self.reward_calculator.prediction_history
                )
            else:
                reward = self.reward_calculator.calculate_step_reward(
                    props, properties_sequence[i-1], target_properties
                )
            rewards.append(reward)
        
        return {
            'rtg_values': rtg_values,
            'rewards': rewards,
            'properties': properties_sequence,
            'target_properties': target_properties
        }


def create_rtg_calculator(checkpoint_path: Optional[str] = None,
                         property_weights: Optional[Dict[str, float]] = None,
                         device: str = "cpu",
                         loaded_model = None) -> RTGCalculator:
    """
    RTG ê³„ì‚°ê¸°ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Args:
        checkpoint_path: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        property_weights: ì†ì„±ë³„ ê°€ì¤‘ì¹˜
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        loaded_model: ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ (ìˆëŠ” ê²½ìš° ì‚¬ìš©)
        
    Returns:
        ì„¤ì •ëœ RTG ê³„ì‚°ê¸°
    """
    # Property ì˜ˆì¸¡ê¸° ìƒì„± - ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    if loaded_model is not None:
        print("\u2705 ì´ë¯¸ ë¡œë“œëœ property prediction ëª¨ë¸ ì¬ì‚¬ìš©")
        property_predictor = ModelBasedPropertyPredictor(checkpoint_path, device, loaded_model)
    else:
        property_predictor = ModelBasedPropertyPredictor(checkpoint_path, device)
    
    # ë¦¬ì›Œë“œ ê³„ì‚°ê¸° ìƒì„±
    reward_calculator = AdaptiveGaussianRewardCalculator(property_weights)
    
    # RTG ê³„ì‚°ê¸° ìƒì„±
    rtg_calculator = RTGCalculator(property_predictor, reward_calculator)
    
    return rtg_calculator


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª RTG Calculator í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    rtg_calc = create_rtg_calculator()
    print("âœ… RTG Calculator ìƒì„± ì„±ê³µ")
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    dummy_target = {
        'entanglement': 0.8,
        'expressibility': 0.7,
        'fidelity': 0.9
    }
    
    print(f"ğŸ¯ íƒ€ê²Ÿ ì†ì„±: {dummy_target}")
    print("âœ… RTG Calculator í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
