"""
Property Predictor Model Loader for RTG Calculation
κ°€μ¤‘μΉλ§μΌλ΅ λ¨λΈμ„ λ΅λ“ν•λ” ν¨μ¨μ μΈ λ΅λ”
"""

import torch
import torch.nn as nn
import pickle
import os
from pathlib import Path
from typing import Optional, Dict, Any
import sys

# Add project paths
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))
sys.path.append(str(Path(__file__).parent.parent))

try:
    from src.models.unified_property_prediction_transformer import UnifiedPropertyPredictionTransformer
    from src.config.unified_training_config import PropertyConfig as UnifiedPropertyPredictionConfig
    from src.config.unified_training_config import UnifiedTrainingConfig
    from src.config.experiment_configs import MODEL_SIZES
except ImportError:
    # Fallback imports for different path contexts
    try:
        from models.unified_property_prediction_transformer import UnifiedPropertyPredictionTransformer
        from config.unified_training_config import PropertyConfig as UnifiedPropertyPredictionConfig
        from config.unified_training_config import UnifiedTrainingConfig
        from config.experiment_configs import MODEL_SIZES
    except ImportError:
        # Final fallback - use relative imports from OAT_Model.src
        from OAT_Model.src.models.unified_property_prediction_transformer import UnifiedPropertyPredictionTransformer
        from OAT_Model.src.config.unified_training_config import PropertyConfig as UnifiedPropertyPredictionConfig
        from OAT_Model.src.config.unified_training_config import UnifiedTrainingConfig
        from OAT_Model.src.config.experiment_configs import MODEL_SIZES


class PropertyPredictorLoader:
    """μλ™μΌλ΅ λ¨λΈκ³Ό μ„¤μ •μ„ λ΅λ“ν•λ” κ°„μ†ν™”λ ν”„λ΅νΌν‹° ν”„λ¦¬λ”•ν„° λ΅λ”"""
    
    def __init__(self, checkpoint_path: str = None, device: str = "auto"):
        """
        Args:
            checkpoint_path: μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅ (Noneμ΄λ©΄ κΈ°λ³Έ κ²½λ΅μ—μ„ μ°ΎκΈ°)
            device: λ””λ°”μ΄μ¤ ("auto", "cuda", "cpu")
        """
        if checkpoint_path is None:
            checkpoint_path = find_best_checkpoint()
            if checkpoint_path is None:
                raise FileNotFoundError("No checkpoint found in the default directory")
                
        self.checkpoint_path = Path(checkpoint_path)
        self.device = self._get_device(device)
        self.model = None
        self.config = None
        self.checkpoint_data = None
        
    def _get_device(self, device: str) -> torch.device:
        """λ””λ°”μ΄μ¤ μ„¤μ •"""
        if device == "auto":
            return torch.device("cuda" if torch.cuda.is_available() else "cpu")
        return torch.device(device)
    
    def load_model(self) -> UnifiedPropertyPredictionTransformer:
        """μ²΄ν¬ν¬μΈνΈμ—μ„ λ¨λΈ λ΅λ“ - ν•­μƒ SOTA μ•„ν‚¤ν…μ² μ‚¬μ©"""
        if not self.checkpoint_path.exists():
            raise FileNotFoundError(f"μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {self.checkpoint_path}")
        
        # μ²΄ν¬ν¬μΈνΈ λ΅λ“
        print(f"π“¦ μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ¤‘: {self.checkpoint_path}")
        try:
            self.checkpoint_data = self._safe_load_checkpoint(str(self.checkpoint_path))
        except Exception as e:
            raise RuntimeError(f"μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {str(e)}") from e
        
        # μ„¤μ • μ •λ³΄ μ¶”μ¶ λ° κ²€μ¦
        config_data = self.checkpoint_data.get('config', {})
        if not config_data:
            print("β οΈ μ²΄ν¬ν¬μΈνΈμ— μ„¤μ • μ •λ³΄κ°€ μ—†μ, κΈ°λ³Έ μ„¤μ • μ‚¬μ©")
        
        # μ„¤μ • μƒμ„± λ° κ²€μ¦
        if isinstance(config_data, dict) and config_data:
            # DecisionConfig κµ¬μ΅° μ²λ¦¬
            if 'model' in config_data and hasattr(config_data['model'], '__dict__'):
                # DecisionConfig κ°μ²΄μ—μ„ PropertyConfig νΈν™ ν•„λ“ μ¶”μ¶
                decision_config = config_data['model']
                
                # μ²΄ν¬ν¬μΈνΈμ—μ„ μ‹¤μ  μ•„ν‚¤ν…μ² νλΌλ―Έν„° μ¶”μ¶
                state_dict = self._extract_state_dict(self.checkpoint_data)
                detected_arch = self._detect_architecture_from_checkpoint(self.checkpoint_data)
                
                # FFN ν¬κΈ°λ¥Ό state_dictμ—μ„ μ§μ ‘ μ¶”μ¶
                ffn_weight_key = 'transformer_layers.0.ffn.0.weight'
                if ffn_weight_key in state_dict:
                    d_ff_actual = state_dict[ffn_weight_key].shape[0]
                else:
                    d_ff_actual = getattr(decision_config, 'd_ff', 512)
                
                # Position embeddingμ—μ„ max_gates μ¶”μ¶
                pos_emb_key = 'circuit_embedding.position_embedding.weight'
                if pos_emb_key in state_dict:
                    max_gates_actual = state_dict[pos_emb_key].shape[0]
                else:
                    max_gates_actual = getattr(decision_config, 'max_gates', 100)
                
                config_data = {
                    'd_model': detected_arch.get('d_model', getattr(decision_config, 'd_model', 256)),
                    'n_heads': detected_arch.get('n_heads', getattr(decision_config, 'n_heads', 4)),
                    'n_layers': detected_arch.get('n_layers', getattr(decision_config, 'n_layers', 3)),
                    'd_ff': d_ff_actual,
                    'dropout': detected_arch.get('dropout', getattr(decision_config, 'dropout', 0.1)),
                    'attention_mode': getattr(decision_config, 'attention_mode', 'advanced'),
                    'use_rotary_pe': getattr(decision_config, 'use_rotary_pe', True),
                    'learning_rate': getattr(decision_config, 'learning_rate', 1e-4),
                    'train_batch_size': getattr(decision_config, 'train_batch_size', 32),
                    'val_batch_size': getattr(decision_config, 'val_batch_size', 64),
                    'weight_decay': getattr(decision_config, 'weight_decay', 1e-5),
                    'max_qubits': getattr(decision_config, 'max_qubits', 10),
                    'property_dim': 3,
                    'max_gates': max_gates_actual,
                    'cross_attention_heads': detected_arch.get('cross_attention_heads', 4),
                    'consistency_loss_weight': 0.1,
                    'numerical_stability': True,
                    'gradient_clipping': 1.0
                }
                print(f"β… μ²΄ν¬ν¬μΈνΈμ—μ„ μ¶”μ¶λ μ•„ν‚¤ν…μ²: d_model={config_data['d_model']}, d_ff={config_data['d_ff']}, max_gates={config_data['max_gates']}")
                print(f"β… DecisionConfigμ—μ„ PropertyConfig νΈν™ μ„¤μ • μ¶”μ¶ μ™„λ£")
                
                # PropertyConfig μƒμ„± λ° λ¨λΈ μΈμ¤ν„΄μ¤ν™”
                self.config = UnifiedPropertyPredictionConfig(**config_data)
                self.model = UnifiedPropertyPredictionTransformer(self.config)
                self.model = self.model.to(self.device)
                
            elif isinstance(config_data, dict) and config_data:
                # λ κ±°μ‹ ν‚¤ μ κ±° (PropertyConfigμ™€ νΈν™λμ§€ μ•λ” ν‚¤λ“¤)
                incompatible_keys = [
                    'model', 'data', 'training', 'evaluation', 'logging',
                    'enable_rtg', 'property_attention_mode', 'property_model_size',
                    'train_batch_size', 'val_batch_size', 'test_batch_size',
                    'learning_rate', 'weight_decay', 'warmup_steps',
                    'entanglement_weight', 'fidelity_weight', 'expressibility_weight',
                    'robust_fidelity_weight',
                    'use_wandb', 'wandb_project', 'wandb_run_name',
                    'save_interval', 'eval_interval', 'max_epochs',
                    'early_stopping_patience', 'gradient_clip_val','experiment'
                ]
                for key in incompatible_keys:
                    config_data.pop(key, None)
                
                # PropertyConfigμ—μ„ μ§€μ›ν•μ§€ μ•λ” μ¶”κ°€ ν‚¤λ“¤ μ κ±°
                additional_incompatible = [
                    'optimizer', 'scheduler', 'loss_function', 'metrics',
                    'checkpoint_dir', 'log_dir', 'experiment_name',
                    'resume_from_checkpoint', 'auto_lr_find', 'precision',
                    'limit_train_batches', 'limit_val_batches', 'limit_test_batches',
                    'fast_dev_run', 'profiler', 'deterministic', 'benchmark'
                ]
                for key in additional_incompatible:
                    config_data.pop(key, None)
            
            # property_model_sizeμ™€ attention_mode μ²λ¦¬ (μ΄λ―Έ μ κ±°λμ—μΌλ―€λ΅ μ›λ³Έμ—μ„ κ°€μ Έμ¤κΈ°)
            original_config = self.checkpoint_data.get('config', {})
            model_size = original_config.get('property_model_size', 'medium')  # κΈ°λ³Έκ°’μ„ mediumμΌλ΅ λ³€κ²½
            attention_mode = original_config.get('property_attention_mode', 'advanced')
            
            # λ¨λΈ μ‚¬μ΄μ¦μ— λ”°λΌ μ μ ν• νλΌλ―Έν„° μ„¤μ • (experiment_configs.pyμ—μ„ κ°€μ Έμ¤κΈ°)
            if model_size in MODEL_SIZES:
                size_config = MODEL_SIZES[model_size]
                d_model = size_config['d_model']
                n_layers = size_config['n_layers']
                n_heads = size_config['n_heads']
                d_ff = size_config['d_ff']
                dropout = size_config['dropout']
                
            # κΈ°λ³Έκ°’ μ„¤μ • (μ‚¬μ©μ μ„¤μ •μ„ μ°μ„ μ μΌλ΅ μ μ§€)
            config_data['d_model'] = config_data.get('d_model', d_model)
            config_data['n_layers'] = config_data.get('n_layers', n_layers)
            config_data['n_heads'] = config_data.get('n_heads', n_heads)
            config_data['d_ff'] = config_data.get('d_ff', d_ff)
            config_data['dropout'] = config_data.get('dropout', dropout)
            config_data['attention_mode'] = attention_mode
            
            # SOTA μ„¤μ • μ μ© (λ κ±°μ‹ ν”λκ·Έ μ κ±°)
            config_data.update({
                'cross_attention_heads': config_data.get('cross_attention_heads', 4),
                'consistency_loss_weight': config_data.get('consistency_loss_weight', 0.1),
                'dropout': config_data.get('dropout', 0.1),
                'numerical_stability': config_data.get('numerical_stability', True),
                'gradient_clipping': config_data.get('gradient_clipping', 1.0)
            })
            
            self.config = UnifiedPropertyPredictionConfig(**config_data)
        else:
            # κΈ°λ³Έ SOTA μ„¤μ • (medium ν¬κΈ° μ‚¬μ©)
            medium_config = MODEL_SIZES['medium']
            detected_arch = self._detect_architecture_from_checkpoint(self.checkpoint_data)
            self.config = UnifiedPropertyPredictionConfig(
                d_model=detected_arch['d_model'],
                n_layers=detected_arch['n_layers'],
                n_heads=detected_arch['n_heads'],
                d_ff=detected_arch['d_ff'],
                dropout=detected_arch['dropout'],
                cross_attention_heads=4,
                consistency_loss_weight=0.1,
                numerical_stability=True,
                gradient_clipping=1.0
            )
            print(f"π”„ λ¨λΈ μ¬μƒμ„±: {detected_arch}")
            self.model = UnifiedPropertyPredictionTransformer(self.config)
        print(f"β… ν†µν•© μ•„ν‚¤ν…μ² (UnifiedPropertyPredictionTransformer)λ΅ λ΅λ“")
        print(f"π” λ¨λΈ ν΄λμ¤: {self.model.__class__.__name__}")
        print(f"π” μ„¤μ • ν΄λμ¤: {self.config.__class__.__name__}")
        
        # κ°€μ¤‘μΉ λ΅λ“ μ „μ— μ‹¤μ  μ•„ν‚¤ν…μ² κ°μ§€
        state_dict = self._extract_state_dict(self.checkpoint_data)
        detected_arch = self._detect_architecture_from_checkpoint(self.checkpoint_data)
        
        # κ°μ§€λ μ•„ν‚¤ν…μ²μ™€ ν„μ¬ μ„¤μ •μ΄ λ‹¤λ¥΄λ©΄ λ¨λΈ μ¬μƒμ„±
        if detected_arch.get('d_model') and detected_arch['d_model'] != self.config.d_model:
            print(f"π”„ μ•„ν‚¤ν…μ² λ¶μΌμΉ κ°μ§€: μ²΄ν¬ν¬μΈνΈ d_model={detected_arch['d_model']}, ν„μ¬ μ„¤μ • d_model={self.config.d_model}")
            print("π”§ μ²΄ν¬ν¬μΈνΈμ— λ§μ¶° λ¨λΈ μ¬μƒμ„± μ¤‘...")
            
            # κ°μ§€λ μ•„ν‚¤ν…μ²λ΅ μ„¤μ • μ—…λ°μ΄νΈ
            self.config.d_model = detected_arch['d_model']
            self.config.n_layers = detected_arch.get('n_layers', self.config.n_layers)
            self.config.n_heads = detected_arch.get('n_heads', self.config.n_heads)
            self.config.d_ff = detected_arch.get('d_ff', self.config.d_ff)
            
            # λ¨λΈ μ¬μƒμ„±
            self.model = UnifiedPropertyPredictionTransformer(self.config)
            self.model = self.model.to(self.device)
            print(f"β… λ¨λΈ μ¬μƒμ„± μ™„λ£: d_model={self.config.d_model}")
        
        try:
            # λ κ±°μ‹ μ²΄ν¬ν¬μΈνΈ νΈν™μ„±μ„ μ„ν•΄ κ΄€λ€ν• λ΅λ”©
            missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
            
            # λ„λ½λ count λ²„νΌλ“¤μ„ μ΄κΈ°ν™” (backward compatibility)
            count_buffers_to_init = [
                'prediction_head.ent_count',
                'prediction_head.fid_count', 
                'prediction_head.exp_count'
            ]
            
            initialized_buffers = []
            missing_keys_list = list(missing_keys)  # Convert to list for safe removal
            for buffer_name in count_buffers_to_init:
                if buffer_name in missing_keys_list:
                    try:
                        # count λ²„νΌλ¥Ό 0μΌλ΅ μ΄κΈ°ν™”
                        if hasattr(self.model, 'prediction_head'):
                            pred_head = self.model.prediction_head
                            buffer_attr = buffer_name.split('.')[-1]  # 'ent_count', 'fid_count', 'exp_count'
                            if hasattr(pred_head, buffer_attr):
                                getattr(pred_head, buffer_attr).fill_(0)
                                initialized_buffers.append(buffer_name)
                                missing_keys_list.remove(buffer_name)
                    except Exception as e:
                        print(f"β οΈ {buffer_name} μ΄κΈ°ν™” μ‹¤ν¨: {e}")
            
            missing_keys = set(missing_keys_list)  # Convert back to set
            
            print(f"π“ κ°€μ¤‘μΉ λ΅λ”© κ²°κ³Ό:")
            print(f"   - λ„λ½λ ν‚¤: {len(missing_keys)}κ°")
            print(f"   - μμƒμΉ λ»ν• ν‚¤: {len(unexpected_keys)}κ°")
            
            if initialized_buffers:
                print(f"   - μ΄κΈ°ν™”λ count λ²„νΌ: {initialized_buffers}")
            
            if missing_keys:
                print(f"   - λ„λ½λ ν‚¤ μμ‹: {list(missing_keys)[:5]}")
            if unexpected_keys:
                print(f"   - μμƒμΉ λ»ν• ν‚¤ μμ‹: {list(unexpected_keys)[:5]}")
                
            
            print("β… λ¨λΈ κ°€μ¤‘μΉ λ΅λ“ μ™„λ£ (λ κ±°μ‹ νΈν™ λ¨λ“)")
            
        except Exception as e:
            raise RuntimeError(f"λ¨λΈ κ°€μ¤‘μΉ λ΅λ“ μ‹¤ν¨: {str(e)}") from e
        
        # λ””λ°”μ΄μ¤λ΅ μ΄λ™ λ° ν‰κ°€ λ¨λ“
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # λ²„νΌ ν†µκ³„ λ³µμ› (expressibility κ΄€λ ¨)
        if hasattr(self.model, 'prediction_head'):
            exp_mean = getattr(self.model.prediction_head, 'exp_mean', None)
            exp_std = getattr(self.model.prediction_head, 'exp_std', None)
            if exp_mean is not None and exp_std is not None:
                print(f"β… Expressibility ν†µκ³„ λ³µμ›λ¨: mean={exp_mean.item():.6f}, std={exp_std.item():.6f}")
        
        print(f"β… λ¨λΈ λ΅λ“ μ™„λ£ - λ””λ°”μ΄μ¤: {self.device}")
        return self.model
    
    def _extract_config_params(self, checkpoint, detected_config) -> dict:
        """μ²΄ν¬ν¬μΈνΈμ—μ„ μ„¤μ • νλΌλ―Έν„° μ¶”μ¶"""
        if isinstance(checkpoint, dict) and 'config' in checkpoint:
            config_data = checkpoint['config']
            if isinstance(config_data, dict):
                # μ ν¨ν• PropertyPredictionConfig νλΌλ―Έν„°λ§ ν•„ν„°λ§
                valid_params = {}
                config_fields = set(UnifiedPropertyPredictionConfig.__dataclass_fields__.keys())
                for key, value in config_data.items():
                    if key in config_fields:
                        valid_params[key] = value
                
                # κ°μ§€λ μ•„ν‚¤ν…μ²λ΅ λ®μ–΄μ“°κΈ°
                valid_params.update(detected_config)
                return valid_params
            else:
                # config κ°μ²΄μΈ κ²½μ°
                valid_params = {}
                config_fields = set(UnifiedPropertyPredictionConfig.__dataclass_fields__.keys())
                for field in config_fields:
                    if hasattr(config_data, field):
                        valid_params[field] = getattr(config_data, field)
                valid_params.update(detected_config)
                return valid_params
        else:
            # κΈ°λ³Έ μ„¤μ • + κ°μ§€λ μ•„ν‚¤ν…μ²
            return self._create_default_config_dict(detected_config)
    
    def _create_default_config_dict(self, detected_config: dict) -> dict:
        """κΈ°λ³Έ μ„¤μ • λ”•μ…”λ„λ¦¬ μƒμ„±"""
        default_config = {
            'd_model': 256,
            'n_heads': 8,
            'n_layers': 6,
            'd_ff': 2048,
            'dropout': 0.1,
            'max_qubits': 50,
            'property_dim': 3,
            'attention_mode': 'advanced',
            'prediction_head_hidden_dim': 64,
        }
        default_config.update(detected_config)
        return default_config
    
    def _extract_state_dict(self, checkpoint_data):
        """μ²΄ν¬ν¬μΈνΈμ—μ„ state_dict μ¶”μ¶"""
        if 'model_state_dict' in checkpoint_data:
            return checkpoint_data['model_state_dict']
        elif 'model' in checkpoint_data:
            return checkpoint_data['model']
        elif 'state_dict' in checkpoint_data:
            return checkpoint_data['state_dict']
        else:
            raise ValueError("μ²΄ν¬ν¬μΈνΈμ—μ„ state_dictλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤")
    
    def _detect_architecture_from_checkpoint(self, checkpoint_data):
        """μ²΄ν¬ν¬μΈνΈμ—μ„ μ •ν™•ν• μ•„ν‚¤ν…μ² νλΌλ―Έν„° μ¶”μ¶"""
        detected = {}
        
        # 1μμ„: model_infoμ—μ„ μ§μ ‘ μ¶”μ¶ (κ°€μ¥ μ •ν™•ν•¨)
        if 'model_info' in checkpoint_data:
            model_info = checkpoint_data['model_info']
            if 'config' in model_info:
                config_info = model_info['config']
                detected.update({
                    'd_model': config_info.get('d_model'),
                    'n_layers': config_info.get('n_layers'),
                    'n_heads': config_info.get('n_heads'),
                    'd_ff': config_info.get('d_ff'),
                    'dropout': config_info.get('dropout'),
                    'cross_attention_heads': config_info.get('cross_attention_heads')
                })
                print(f"β… model_infoμ—μ„ μ•„ν‚¤ν…μ² μ¶”μ¶: {detected}")
                return detected
        
        # 2μμ„: config λ”•μ…”λ„λ¦¬μ—μ„ μ§μ ‘ μ¶”μ¶
        if 'config' in checkpoint_data:
            config = checkpoint_data['config']
            detected.update({
                'd_model': config.get('d_model'),
                'n_layers': config.get('n_layers'), 
                'n_heads': config.get('n_heads'),
                'd_ff': config.get('d_ff'),
                'dropout': config.get('dropout')
            })
            # None κ°’ μ κ±°
            detected = {k: v for k, v in detected.items() if v is not None}
            if detected:
                print(f"β… configμ—μ„ μ•„ν‚¤ν…μ² μ¶”μ¶: {detected}")
                return detected
        
        # 3μμ„: state_dictμ—μ„ μ¶”λ΅  (μµν›„μ μλ‹¨)
        state_dict = self._extract_state_dict(checkpoint_data)
        
        # d_model κ°μ§€ (feature_extractorμ μ²« λ²μ§Έ λ μ΄μ–΄μ—μ„)
        for key, tensor in state_dict.items():
            if 'prediction_head.feature_extractor.0.weight' in key:
                detected['d_model'] = tensor.shape[1]  # input dimension
                break
            elif 'gate_embedding.weight' in key:
                detected['d_model'] = tensor.shape[1]  # embedding dimension
                break
        
        # n_layers κ°μ§€ (transformer_layers κ°μ)
        layer_indices = set()
        for key in state_dict.keys():
            if 'transformer_layers.' in key:
                try:
                    layer_idx = int(key.split('transformer_layers.')[1].split('.')[0])
                    layer_indices.add(layer_idx)
                except (ValueError, IndexError):
                    continue
        
        if layer_indices:
            detected['n_layers'] = max(layer_indices) + 1
        
        # n_heads κ°μ§€ (MultiHeadAttentionμ κ°€μ¤‘μΉμ—μ„)
        for key, tensor in state_dict.items():
            if 'self_attn.in_proj_weight' in key and detected.get('d_model'):
                d_model = detected['d_model']
                # in_proj_weight: [3*d_model, d_model] (query, key, value κ²°ν•©)
                if tensor.shape[0] == 3 * d_model and tensor.shape[1] == d_model:
                    # n_headsλ” μΌλ°μ μΌλ΅ d_modelμ μ•½μ
                    for n_heads in [4, 6, 8, 12, 16, 32]:
                        if d_model % n_heads == 0:
                            detected['n_heads'] = n_heads
                            break
                break
        
        if detected:
            print(f"π” state_dictμ—μ„ μ•„ν‚¤ν…μ² μ¶”λ΅ : {detected}")
        else:
            print("β οΈ μ•„ν‚¤ν…μ² κ°μ§€ μ‹¤ν¨")
        
        return detected
    
    def _safe_load_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ μ•μ „ λ΅λ”©"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            if not isinstance(checkpoint, dict):
                raise ValueError(f"μ²΄ν¬ν¬μΈνΈκ°€ λ”•μ…”λ„λ¦¬κ°€ μ•„λ‹: {type(checkpoint)}")
            return checkpoint
        except Exception as e:
            raise RuntimeError(f"μ²΄ν¬ν¬μΈνΈ νμΌ μ½κΈ° μ‹¤ν¨: {checkpoint_path}") from e
    
    def get_model(self) -> Optional[UnifiedPropertyPredictionTransformer]:
        """λ΅λ“λ λ¨λΈ λ°ν™"""
        return self.model
    
    
    def get_config(self) -> Optional[UnifiedPropertyPredictionConfig]:
        """λ¨λΈ μ„¤μ • λ°ν™"""
        return self.config


def load_property_predictor(checkpoint_path: str = None, device: str = "auto") -> UnifiedPropertyPredictionTransformer:
    """
    μ²΄ν¬ν¬μΈνΈμ—μ„ μλ™μΌλ΅ λ¨λΈ κµ¬μ„±μ„ κ°μ§€ν•μ—¬ λ΅λ“ν•λ” ν—¬νΌ ν•¨μ
    
    Args:
        checkpoint_path: μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅ (μ—†μ„ κ²½μ° κΈ°λ³Έ κ²½λ΅μ—μ„ κ²€μƒ‰)
        device: λ””λ°”μ΄μ¤ μ„¤μ • ("auto", "cuda", "cpu")
        
    Returns:
        λ΅λ“λ λ¨λΈ (PropertyPredictionTransformer νΉμ€ IntegratedPropertyPredictionTransformer)
    """
    loader = PropertyPredictorLoader(checkpoint_path, device)
    model = loader.load_model()
    
    print("\nβ… λ¨λΈ μλ™ λ΅λ“ μ™„λ£!")
    return model


def find_best_checkpoint(checkpoint_dir: str = "checkpoints") -> Optional[str]:
    """
    μµμ μ μ²΄ν¬ν¬μΈνΈ νμΌμ„ μλ™μΌλ΅ μ°Ύλ” ν•¨μ
    
    Args:
        checkpoint_dir: μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬ (κΈ°λ³Έκ°’: "checkpoints")
        
    Returns:
        μµμ  μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅ λλ” None
    """
    # λ””λ ‰ν† λ¦¬ κ²½λ΅ μ €μ¥
    checkpoint_path = Path(checkpoint_dir)
    
    # λ””λ ‰ν† λ¦¬κ°€ μ΅΄μ¬ν•μ§€ μ•λ” κ²½μ° μƒμ„ κ²½λ΅λ΅ ν™•μ¥
    if not checkpoint_path.exists() and not checkpoint_path.is_absolute():
        parent_dirs = [Path("."), Path("..")] 
        for parent in parent_dirs:
            alt_path = parent / checkpoint_path
            if alt_path.exists():
                checkpoint_path = alt_path
                print(f"π” μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬ λ°κ²¬: {alt_path}")
                break
    
    if not checkpoint_path.exists():
        print(f"β οΈ μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬κ°€ μ—†μµλ‹λ‹¤: {checkpoint_dir}")
        return None
    
    # μ°μ„ μμ„ κΈ°λ° μ²΄ν¬ν¬μΈνΈ νμΌ νƒ€μ…
    priority_filenames = [
        "best_model.pt",          # μµμƒμ„ μ°μ„ μμ„ (ν„μ¬ μ•„ν‚¤ν…μ²)
        "best_enhanced_model.pt", # λ κ±°μ‹
        "best_property_model.pt", # λ‹¤μ μ°μ„ μμ„
        "final_model.pt",         # λ‹¤μ μ°μ„ μμ„
        "latest_model.pt"         # λ‹¤μ μ°μ„ μμ„
    ]
    
    # μ°μ„ μμ„ λ¦¬μ¤νΈμ—μ„ μ°ΎκΈ° 
    for filename in priority_filenames:
        candidate = checkpoint_path / filename
        if candidate.exists():
            print(f"π… μ°μ„ μμ„ μ²΄ν¬ν¬μΈνΈ λ°κ²¬: {candidate}")
            return str(candidate)
    
    # ν¨ν„΄ κΈ°λ° κ²€μƒ‰ (λ‹¤μ–‘ν• μ²΄ν¬ν¬μΈνΈ ν•νƒ κ³ λ ¤)
    pattern_searches = [
        "*best*.pt",       # bestλ΅ μ‹μ‘ν•λ” λ¨λ“  νμΌ
        "checkpoint*.pt", # checkpointλ΅ μ‹μ‘ν•λ” λ¨λ“  νμΌ
        "model_*.pt",     # model_λ΅ μ‹μ‘ν•λ” λ¨λ“  νμΌ
        "*.pt"           # λ¨λ“  .pt νμΌ (λ§μ§€λ§‰ μ„ νƒμ‚¬ν•­)
    ]
    
    # κ° ν¨ν„΄μΌλ΅ κ²€μƒ‰
    for pattern in pattern_searches:
        matching_files = list(checkpoint_path.glob(pattern))
        if matching_files:
            # μμ • μ‹κ°„ κΈ°μ¤€ μ •λ ¬
            latest = max(matching_files, key=lambda p: p.stat().st_mtime)
            print(f"π“… μµμ‹  μ²΄ν¬ν¬μΈνΈ λ°κ²¬ ({pattern}): {latest}")
            return str(latest)
    
    # ν•μ„ λ””λ ‰ν† λ¦¬λ¥Ό ν¬ν•¨ν• κ²€μƒ‰ (μ „μ²΄ λ¦¬μ»¤μ‹λΈ κ²€μƒ‰)
    all_pt_files = []
    for root, _, files in os.walk(checkpoint_path):
        for file in files:
            if file.endswith(".pt"):
                all_pt_files.append(Path(root) / file)
    
    if all_pt_files:
        latest = max(all_pt_files, key=lambda p: p.stat().st_mtime)
        print(f"π” ν•μ„ λ””λ ‰ν† λ¦¬ ν¬ν•¨ κ²€μƒ‰ κ²°κ³Ό: {latest}")
        return str(latest)
    
    print("β μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤.")
    return None


if __name__ == "__main__":
    # ν…μ¤νΈ μ½”λ“
    checkpoint_path = find_best_checkpoint()
    model = load_property_predictor(checkpoint_path)
    print(f"β… λ¨λΈ λ΅λ“ ν…μ¤νΈ μ„±κ³µ: {type(model).__name__}")
