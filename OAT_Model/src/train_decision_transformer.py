"""
Decision Transformer í›ˆë ¨ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì¸ìë¥¼ ë°›ì•„ í›ˆë ¨ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import json
import torch
from pathlib import Path
from torch.utils.data import DataLoader

from models.decision_transformer import create_decision_transformer
from training.decision_transformer_trainer import DecisionTransformerTrainer
from rtg.core.rtg_calculator import RTGCalculator, create_rtg_calculator
from data.simple_dt_collator import SimpleDecisionTransformerCollator
from data.streamlined_dt_dataset import StreamlinedDTDataset
from data.quantum_circuit_dataset import DatasetManager, DecisionTransformerDataset
from config.model_configs import get_model_config, print_model_info, get_available_sizes


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="Decision Transformer Training")
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument('--data_path', type=str, default=r'C:\Users\jungh\Documents\GitHub\Kaist\OAT_Model\raw_data\merged_data.json', help='Training data path')
    parser.add_argument('--output_dir', type=str, default='./checkpoints', help='Output directory')
    
    # ëª¨ë¸ ì‚¬ì´ì¦ˆ (í‘œì¤€í™”)
    parser.add_argument('--model_size', type=str, default='M', choices=['S', 'M', 'L'], 
                       help='Model size: S(Small), M(Medium), L(Large)')
    
    # ì–´í…ì…˜ ëª¨ë“œ
    parser.add_argument('--attention_mode', type=str, default='advanced', 
                       choices=['standard', 'advanced'], 
                       help='Attention mode: standard or advanced')
    
    # í”„ë¡œí¼í‹° ëª¨ë¸ ê°€ì¤‘ì¹˜
    parser.add_argument('--property_model_path', type=str, 
                       help='Path to pre-trained property prediction model weights')
    
    # ì„ë² ë”© ëª¨ë“œ
    parser.add_argument('--embedding_mode', type=str, default='gnn', 
                       choices=['gnn', 'transformer', 'hybrid', 'simple'], 
                       help='Embedding mode: gnn, transformer, hybrid, or simple')
    
    # ë°ì´í„° ì¦ê°• ì˜µì…˜
    parser.add_argument('--use_augmentation', default=True,action='store_true', 
                       help='Enable data augmentation for training dataset')
    parser.add_argument('--noise_samples', type=int, default=500,
                       help='Number of noise augmentation samples per circuit')
    parser.add_argument('--param_random_samples', type=int, default=1000,
                       help='Number of parameter randomization samples per circuit')
    
    # í›ˆë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='Weight decay')
    
    # ë””ë°”ì´ìŠ¤ ë° ê¸°íƒ€
    parser.add_argument('--device', type=str, default='cuda', help='Device to use')
    parser.add_argument('--num_workers', type=int, default=0, help='Number of data loader workers')
    parser.add_argument('--resume', type=str, help='Resume from checkpoint')
    parser.add_argument('--wandb', action='store_true', help='Use Weights & Biases logging')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    return parser.parse_args()


def set_seed(seed: int):
    """ì‹œë“œ ì„¤ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def get_embedding_config(embedding_mode, base_config):
    """ì„ë² ë”© ëª¨ë“œë³„ ì„¤ì • ìƒì„±"""
    embedding_config = {}
    
    if embedding_mode == 'gnn':
        embedding_config = {
            'type': 'gnn',
            'hidden_dim': base_config['d_model'],
            'num_layers': 3,
            'dropout': 0.1,
            'aggregation': 'mean',
            'use_edge_features': True
        }
    elif embedding_mode == 'transformer':
        embedding_config = {
            'type': 'transformer',
            'hidden_dim': base_config['d_model'],
            'num_heads': base_config['n_heads'],
            'num_layers': 2,
            'dropout': 0.1
        }
    elif embedding_mode == 'hybrid':
        embedding_config = {
            'type': 'hybrid',
            'gnn_layers': 2,
            'transformer_layers': 1,
            'hidden_dim': base_config['d_model'],
            'num_heads': base_config['n_heads'],
            'dropout': 0.1
        }
    elif embedding_mode == 'simple':
        embedding_config = {
            'type': 'simple',
            'hidden_dim': base_config['d_model'],
            'num_layers': 2,
            'dropout': 0.1
        }
    else:
        raise ValueError(f"Unknown embedding mode: {embedding_mode}")
    
    return embedding_config


def create_model(model_config, property_model_path=None, embedding_mode='gnn'):
    """ëª¨ë¸ ìƒì„±"""
    print("ğŸ¤– Decision Transformer ëª¨ë¸ ìƒì„± ì¤‘...")
    
    # Property prediction model ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
    property_prediction_model = None
    if property_model_path:
        try:
            from rtg.model_loader import load_property_predictor
            property_prediction_model = load_property_predictor(property_model_path)
            print(f"âœ… Property prediction model ë¡œë“œ ì™„ë£Œ: {property_model_path}")
        except Exception as e:
            print(f"âš ï¸ Property prediction model ë¡œë“œ ì‹¤íŒ¨: {e}")
            property_prediction_model = None
    
    # SAR ì‹œí€€ìŠ¤ëŠ” 3ë°° ê¸¸ì´
    dt_config = model_config.copy()
    dt_config['max_seq_length'] = model_config['max_seq_length'] * 3
    
    # ì„ë² ë”© ëª¨ë“œ ì²˜ë¦¬
    embedding_config = get_embedding_config(embedding_mode, dt_config)
    
    # Decision Transformer ìƒì„±
    dt_config['property_prediction_model'] = property_prediction_model
    dt_config['embedding_mode'] = embedding_mode
    dt_config['embedding_config'] = embedding_config
    model = create_decision_transformer(dt_config)
    
    print(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
    
    # ëª¨ë¸ê³¼ í•¨ê»˜ property_prediction_modelë„ ë°˜í™˜í•˜ì—¬ ì¬ì‚¬ìš©
    return model, property_prediction_model


def create_real_data_loaders(args, model_config, property_prediction_model=None):
    """ì‹¤ì œ ë°ì´í„° ë¡œë” ìƒì„±"""
    print("ğŸ“Š ì‹¤ì œ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    if not Path(args.data_path).exists():
        raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data_path}")
    
    # Quantum dataset ë¡œë“œ (í•œ ë²ˆë§Œ)
    from data.quantum_circuit_dataset import DatasetManager
    manager = DatasetManager(unified_data_path=args.data_path)
    circuit_data = manager.merge_data()
    quantum_dataset = manager
    
    # RTG ê³„ì‚°ê¸° ìƒì„± - ì´ë¯¸ ë¡œë“œëœ property_prediction_model ì¬ì‚¬ìš©
    rtg_calculator = create_rtg_calculator(
        checkpoint_path=args.property_model_path,
        device=args.device,
        loaded_model=property_prediction_model
    )
    
    # ê°„ì†Œí™”ëœ ë°ì´í„°ì…‹ ìƒì„± - ì§ì ‘ circuit_data ì‚¬ìš©
    target_properties = {'entanglement': 0.8, 'expressibility': 0.7}
    base_dataset = StreamlinedDTDataset(
        circuit_data_list=circuit_data,
        rtg_calculator=rtg_calculator,
        target_properties=target_properties,
        max_seq_length=model_config['max_seq_length'],
        d_model=model_config.get('d_model', 512)
    )
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    train_size = int(0.8 * len(base_dataset))
    val_size = len(base_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        base_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # ë°ì´í„° ì¦ê°• ì ìš© (í›ˆë ¨ ë°ì´í„°ë§Œ)
    if args.use_augmentation:
        print(f"ğŸ”„ ë°ì´í„° ì¦ê°• ì ìš© ì¤‘...")
        print(f"   - ë…¸ì´ì¦ˆ ìƒ˜í”Œ: {args.noise_samples}")
        print(f"   - íŒŒë¼ë¯¸í„° ëœë¤ ìƒ˜í”Œ: {args.param_random_samples}")
        
        # Create augmented dataset from base dataset before splitting
        augmented_base_dataset = AugmentedDecisionTransformerDataset(
            base_dataset,
            noise_samples=args.noise_samples,
            param_random_samples=args.param_random_samples
        )
        
        # Re-split the augmented dataset
        aug_train_size = int(0.8 * len(augmented_base_dataset))
        aug_val_size = len(augmented_base_dataset) - aug_train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            augmented_base_dataset, [aug_train_size, aug_val_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        print(f"   - ì¦ê°•ëœ í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset):,}")
        print(f"   - ì¦ê°•ëœ ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset):,}")
    else:
        print("   - ë°ì´í„° ì¦ê°• ë¹„í™œì„±í™”")
    
    # ê°„ì†Œí™”ëœ ì½œë ˆì´í„° ì‚¬ìš©
    collate_fn = base_dataset.get_collate_fn()
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=True if args.device == "cuda" else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=True if args.device == "cuda" else False
    )
    
    print(f"   - í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset):,}")
    print(f"   - ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset):,}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    
    return train_loader, val_loader


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()
    
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸš€ Decision Transformer í›ˆë ¨ ì‹œì‘")
    print(f"   - ëª¨ë¸ ì‚¬ì´ì¦ˆ: {args.model_size}")
    print(f"   - ì–´í…ì…˜ ëª¨ë“œ: {args.attention_mode}")
    print(f"   - ì„ë² ë”© ëª¨ë“œ: {args.embedding_mode}")
    print(f"   - ë°ì´í„° ê²½ë¡œ: {args.data_path}")
    print(f"   - ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(f"   - ë””ë°”ì´ìŠ¤: {args.device}")
    print(f"   - ì—í¬í¬: {args.epochs}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    if args.property_model_path:
        print(f"   - í”„ë¡œí¼í‹° ëª¨ë¸: {args.property_model_path}")
    
    # ì„ë² ë”© ëª¨ë“œë³„ ì„¸ë¶€ ì„¤ì • ì¶œë ¥
    embedding_details = {
        'gnn': 'Graph Neural Network ê¸°ë°˜ íšŒë¡œ ì„ë² ë”©',
        'transformer': 'Transformer ê¸°ë°˜ ì‹œí€€ì…œ ì„ë² ë”©', 
        'hybrid': 'GNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”©',
        'simple': 'ë‹¨ìˆœ ì„ í˜• ì„ë² ë”©'
    }
    print(f"ğŸ“Š ì„ë² ë”© ëª¨ë“œ: {args.embedding_mode} - {embedding_details[args.embedding_mode]}")
    print()
    
    # ëª¨ë¸ ì„¤ì • ë¡œë“œ
    model_config = get_model_config(args.model_size)
    model_config['attention_mode'] = args.attention_mode
    print_model_info(args.model_size)
    print(f"   - Attention Mode: {args.attention_mode}")
    print(f"   - Embedding Mode: {args.embedding_mode}")
    if args.property_model_path:
        print(f"   - Property Model: {args.property_model_path}")
    
    # í›ˆë ¨ ì„¤ì • ì—…ë°ì´íŠ¸
    training_config = {
        'lr': args.learning_rate,
        'weight_decay': args.weight_decay,
        'max_epochs': args.epochs,
        'min_lr': 1e-6,
        'batch_size': args.batch_size
    }
    
    # ëª¨ë¸ ìƒì„± (property_prediction_model ë¨¼ì € ë¡œë“œ)
    model, property_prediction_model = create_model(model_config, args.property_model_path, args.embedding_mode)
    
    # ì‹¤ì œ ë°ì´í„° ë¡œë” ìƒì„± (ì´ë¯¸ ë¡œë“œëœ property_prediction_model ì¬ì‚¬ìš©)
    train_loader, val_loader = create_real_data_loaders(args, model_config, property_prediction_model)
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = DecisionTransformerTrainer(
        model=model,
        config=training_config,
        device=args.device,
        use_wandb=args.wandb
    )
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
    if args.resume:
        print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {args.resume}")
        trainer.load_checkpoint(args.resume)
    
    # í›ˆë ¨ ì‹¤í–‰
    try:
        trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            num_epochs=args.epochs,
            save_dir=args.output_dir
        )
        
        print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # í˜„ì¬ ìƒíƒœ ì €ì¥
        checkpoint_path = output_dir / "interrupted_checkpoint.pt"
        trainer.save_checkpoint(str(checkpoint_path))
        print(f"ğŸ’¾ ì¤‘ë‹¨ëœ ìƒíƒœê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {checkpoint_path}")
    
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
