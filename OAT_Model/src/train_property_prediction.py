#!/usr/bin/env python3
"""
Property Prediction Transformer ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

CircuitSpecìœ¼ë¡œë¶€í„° ì–½í˜ë„, fidelity, robust fidelityë¥¼ ì˜ˆì¸¡í•˜ëŠ” 
íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python train_property_prediction.py --data_path path/to/data.json --epochs 100
"""

import argparse
import torch
import json
from pathlib import Path
import sys

# Add src to path
sys.path.append(str(Path(__file__).parent / "src"))

from models.integrated_property_prediction_transformer import (
    IntegratedPropertyPredictionTransformer,
    IntegratedPropertyPredictionConfig,
    create_property_prediction_model
)
from training.property_prediction_trainer import (
    PropertyPredictionTrainer,
    create_datasets
)

def main():
    parser = argparse.ArgumentParser(description='Property Prediction Transformer í•™ìŠµ')
    
    # Data arguments
    parser.add_argument('--data_path', type=str, 
                       default='C:\\Users\\jungh\\Documents\\GitHub\\Kaist\\OAT_Model\\raw_data\\merged_data.json',
                       help='í•™ìŠµ ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ')
    
    # Model arguments
    parser.add_argument('--d_model', type=int, default=256, help='ëª¨ë¸ ì°¨ì›')
    parser.add_argument('--n_heads', type=int, default=8, help='ì–´í…ì…˜ í—¤ë“œ ìˆ˜')
    parser.add_argument('--n_layers', type=int, default=6, help='íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜')
    parser.add_argument('--dropout', type=float, default=0.1, help='ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨')
    
    # Training arguments
    parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í­ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='ê°€ì¤‘ì¹˜ ê°ì‡ ')
    parser.add_argument('--batch_size', type=int, default=16, help='ë°°ì¹˜ í¬ê¸°')
    
    # Other arguments
    parser.add_argument('--save_dir', type=str, default='property_prediction_checkpoints', 
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--train_ratio', type=float, default=0.8, help='í•™ìŠµ ë°ì´í„° ë¹„ìœ¨')
    parser.add_argument('--val_ratio', type=float, default=0.1, help='ê²€ì¦ ë°ì´í„° ë¹„ìœ¨')
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ')
    
    args = parser.parse_args()
    
    # Set random seed
    torch.manual_seed(args.seed)
    
    print("ğŸ§¬ Property Prediction Transformer í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“Š ë°ì´í„° ê²½ë¡œ: {args.data_path}")
    print(f"ğŸ—ï¸  ëª¨ë¸ ì„¤ì •: d_model={args.d_model}, n_heads={args.n_heads}, n_layers={args.n_layers}")
    print(f"ğŸ“š í•™ìŠµ ì„¤ì •: epochs={args.epochs}, lr={args.learning_rate}, batch_size={args.batch_size}")
    print("=" * 60)
    
    # Check if data file exists
    data_path = Path(args.data_path)
    if not data_path.exists():
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        print("ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        possible_paths = [
            "./raw_data/merged_data.json"
        ]
        for path in possible_paths:
            if Path(path).exists():
                print(f"âœ… ë°œê²¬: {path}")
            else:
                print(f"âŒ ì—†ìŒ: {path}")
        return
    
    try:
        # Create model configuration
        config = PropertyPredictionConfig(
            d_model=args.d_model,
            n_heads=args.n_heads,
            n_layers=args.n_layers,
            dropout=args.dropout,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            train_batch_size=args.batch_size,
            val_batch_size=args.batch_size,
            test_batch_size=args.batch_size,
        )
        
        # Create model
        print("ğŸ—ï¸  ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_property_prediction_model(config)
        
        # Create datasets
        print("ğŸ“Š ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        train_dataset, val_dataset, test_dataset = create_datasets(
            str(data_path), 
            train_ratio=args.train_ratio, 
            val_ratio=args.val_ratio
        )
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
        print(f"   - í•™ìŠµ: {len(train_dataset)} ìƒ˜í”Œ")
        print(f"   - ê²€ì¦: {len(val_dataset)} ìƒ˜í”Œ") 
        print(f"   - í…ŒìŠ¤íŠ¸: {len(test_dataset)} ìƒ˜í”Œ")
        
        # Create trainer
        print("ğŸ¯ íŠ¸ë ˆì´ë„ˆ ìƒì„± ì¤‘...")
        trainer = PropertyPredictionTrainer(
            model=model,
            config=config,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            save_dir=args.save_dir
        )
        
        # Start training
        print("ğŸš€ í•™ìŠµ ì‹œì‘!")
        trainer.train(num_epochs=args.epochs)
        
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {args.save_dir}")
        
        # Test evaluation (optional)
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€...")
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            collate_fn=trainer.train_loader.collate_fn
        )
        
        # Load best model for testing
        best_model_path = Path(args.save_dir) / 'best_model.pt'
        if best_model_path.exists():
            checkpoint = torch.load(best_model_path, map_location=trainer.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… ìµœê³  ëª¨ë¸ ë¡œë“œ: {best_model_path}")
            
            # Simple test evaluation
            model.eval()
            test_losses = []
            
            with torch.no_grad():
                for batch in test_loader:
                    try:
                        circuit_specs = batch['circuit_specs']
                        targets = {k: v.to(trainer.device) for k, v in batch['targets'].items()}
                        
                        predictions = model(circuit_specs)
                        for key in predictions:
                            predictions[key] = predictions[key].to(trainer.device)
                        
                        losses = trainer.criterion(predictions, targets)
                        test_losses.append(losses['total'].item())
                        
                    except Exception as e:
                        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ì‹¤íŒ¨: {e}")
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}") from e
            
            if test_losses:
                avg_test_loss = sum(test_losses) / len(test_losses)
                print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì†ì‹¤: {avg_test_loss:.4f}")
            else:
                print("âš ï¸ í…ŒìŠ¤íŠ¸ í‰ê°€ ì‹¤íŒ¨")
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
