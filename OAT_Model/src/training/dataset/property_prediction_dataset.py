"""
Property Prediction Dataset Module

CircuitSpecìœ¼ë¡œë¶€í„° ì–½í˜ë„, fidelity, expressibilityë¥¼ ì˜ˆì¸¡í•˜ëŠ” 
íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë°ì´í„°ì…‹ ì²˜ë¦¬ ëª¨ë“ˆ
"""
import torch
from typing import Dict, List, Any
from data.quantum_circuit_dataset import QuantumCircuitDataset, CircuitData


class PropertyPredictionDataset:
    """ì–‘ì íšŒë¡œ íŠ¹ì„± ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„°ì…‹ ë˜í¼"""
    
    def __init__(self, quantum_dataset: QuantumCircuitDataset):
        """
        Args:
            quantum_dataset: QuantumCircuitDataset ì¸ìŠ¤í„´ìŠ¤
        """
        self.quantum_dataset = quantum_dataset
        
        print(f"[INIT] Property Prediction ë°ì´í„°ì…‹ ì´ˆê¸°í™”: {len(self.quantum_dataset)} ìƒ˜í”Œ")
    
    def __len__(self) -> int:
        return len(self.quantum_dataset)
    
    def __getitem__(self, idx: int) -> Dict:
        """CircuitDataë¥¼ Property Prediction í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        circuit_data: CircuitData = self.quantum_dataset[idx]
        
        # Check if measurement result exists
        if circuit_data.measurement_result is None:
            raise ValueError(f"No measurement result for circuit {circuit_data.circuit_id}")
            
        measurement = circuit_data.measurement_result
        
        # Validate required fields
        if measurement.fidelity is None:
            raise ValueError(f"Missing fidelity for circuit {circuit_data.circuit_id}")
        
        # Extract expressibility (KL divergence only)
        expressibility_value = 0.0
        if measurement.expressibility and isinstance(measurement.expressibility, dict):
            kl_div = measurement.expressibility.get('kl_divergence', 0.0)
            # Use KL divergence directly as expressibility
            expressibility_value = float(kl_div)
        
        targets = {
            'entanglement': float(measurement.entanglement) if measurement.entanglement is not None else 0.0,
            'fidelity': float(measurement.fidelity),
            'expressibility': float(expressibility_value)
        }
        
        # Combined target vector (3 properties only)
        targets['combined'] = torch.tensor([
            targets['entanglement'],
            targets['fidelity'], 
            targets['expressibility']
        ], dtype=torch.float32)
        
        return {
            'circuit_spec': circuit_data.circuit_spec,
            'targets': targets,
            'metadata': {
                'num_qubits': circuit_data.num_qubits,
                'num_gates': len(circuit_data.gates),
                'circuit_id': circuit_data.circuit_id,
                'depth': measurement.depth
            }
        }


def collate_fn(batch: List[Dict]) -> Dict:
    """ë°°ì¹˜ ë°ì´í„° collation"""
    # Filter out None items from batch
    valid_batch = [item for item in batch if item is not None]
    
    if not valid_batch:
        raise ValueError("[EMPTY] - No valid items in batch")
    
    circuit_specs = [item['circuit_spec'] for item in valid_batch]
    
    # íƒ€ê²Ÿ ê°’ë“¤ì„ í…ì„œë¡œ ë³€í™˜
    targets = {}
    for key in ['entanglement', 'fidelity', 'expressibility']:
        targets[key] = torch.tensor([item['targets'][key] for item in valid_batch], dtype=torch.float32)
    
    targets['combined'] = torch.stack([item['targets']['combined'] for item in valid_batch])
    
    # ë©”íƒ€ë°ì´í„°
    metadata = [item['metadata'] for item in valid_batch]
    
    return {
        'circuit_specs': circuit_specs,
        'targets': targets,
        'metadata': metadata
    }


def create_datasets(data_path: str, train_ratio: float = 0.7, val_ratio: float = 0.15, 
                   enable_augmentation: bool = True):
    """merged_data.jsonì„ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ ë¶„í•  ìƒì„± (ì¦ê°• ì§€ì›)"""
    from data.quantum_circuit_dataset import DatasetManager
    from typing import Tuple
    
    # Create dataset manager
    manager = DatasetManager(unified_data_path=data_path)
    
    # Split quantum datasets
    train_quantum, val_quantum, test_quantum = manager.split_dataset(
        train_ratio=train_ratio,
        val_ratio=val_ratio,
        test_ratio=1.0 - train_ratio - val_ratio
    )
    
    # Apply augmentation to training set if enabled
    if enable_augmentation:
        try:
            from data.augmented_dataset import create_augmented_datasets
            train_quantum, val_quantum, test_quantum = create_augmented_datasets(
                train_quantum, val_quantum, test_quantum,
                noise_samples=500,
                param_random_samples=1000
            )
        except ImportError:
            print("[WARNING] Augmentation module not available, using original datasets")
    
    # Wrap with PropertyPredictionDataset
    train_dataset = PropertyPredictionDataset(train_quantum)
    val_dataset = PropertyPredictionDataset(val_quantum)
    test_dataset = PropertyPredictionDataset(test_quantum)
    
    print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
    print(f"  - Train: {len(train_dataset)} ìƒ˜í”Œ")
    print(f"  - Validation: {len(val_dataset)} ìƒ˜í”Œ")
    print(f"  - Test: {len(test_dataset)} ìƒ˜í”Œ")
    
    return train_dataset, val_dataset, test_dataset
