"""
Decision Transformer ì „ìš© íŠ¸ë ˆì´ë„ˆ
ì™„ì „íˆ ìƒˆë¡œìš´ ê¹”ë”í•œ êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
from typing import Dict, Any, List, Optional, Tuple
import time
import json
from torch import optim
from pathlib import Path
import wandb

from models.decision_transformer import DecisionTransformer
from rtg.core.rtg_calculator import RTGCalculator
from utils.device_manager import get_device_manager, cleanup_memory, get_memory_info


class DecisionTransformerTrainer:
    """Decision Transformer ì „ìš© íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(
        self,
        model: DecisionTransformer,
        config: Dict,
        device: str = "cuda",
        use_wandb: bool = True
    ):
        # Device manager ì´ˆê¸°í™”
        self.device_manager = get_device_manager(device)
        self.device = self.device_manager.device
        self.model = self.device_manager.move_model_to_device(model)
        self.config = config
        self.use_wandb = use_wandb
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ ì„¤ì •
        self.cleanup_interval = config.get('memory_cleanup_interval', 50)  # 50 ë°°ì¹˜ë§ˆë‹¤
        self.memory_threshold_gb = config.get('memory_threshold_gb', 18.0)  # 8GB ì„ê³„ê°’
        self.last_cleanup_batch = 0
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë¹„í™œì„±í™”ë¡œ backward ì˜¤ë¥˜ ë°©ì§€)
        self.accumulate_grad_batches = 1  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë¹„í™œì„±í™”
        self.mixed_precision = config.get('mixed_precision', False)  # í˜¼í•© ì •ë°€ë„ ë¹„í™œì„±í™”
        
        # Mixed precision scaler
        if self.mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()
        else:
            self.scaler = None
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.get('learning_rate', 1e-4),
            weight_decay=config.get('weight_decay', 0.01),
            betas=(0.9, 0.95)
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.get('max_epochs', 100),
            eta_min=config.get('min_lr', 1e-6)
        )
        
        # Huber Loss for property prediction
        self.huber_loss = nn.HuberLoss(delta=1.0)
        
        # í›ˆë ¨ ìƒíƒœ
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.global_step = 0
        
        # WandB ì´ˆê¸°í™”
        if use_wandb:
            wandb.init(
                project="decision-transformer-quantum",
                name=f"dt_clean_{int(time.time())}",
                config=config,
                tags=["decision_transformer", "quantum_circuit", "gate_prediction"]
            )
            # ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¡œê¹…
            wandb.watch(self.model, log="all", log_freq=100)
        
        print(f"âœ… Decision Transformer Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def train_epoch(self, train_loader: DataLoader) -> float:
        """í•œ ì—í¬í¬ í›ˆë ¨ - ì™„ì „ ë¦¬íŒ©í† ë§"""
        self.model.train()
        total_loss = 0.0
        
        pbar = tqdm(train_loader, desc=f"Training Epoch {self.current_epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            # ë‹¨ìˆœí•œ í›ˆë ¨ ìŠ¤í…
            loss_value = self._train_step(batch)
            total_loss += loss_value
            
            # Progress bar ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'loss': f'{loss_value:.4f}', 
                'avg_loss': f'{total_loss/(batch_idx+1):.4f}'
            })
            
            # ë¡œê¹…
            if self.use_wandb and batch_idx % 10 == 0:
                self._log_metrics(loss_value)
            
            self.global_step += 1
        
        return total_loss / len(train_loader)
    
    def validate(self, val_loader: DataLoader) -> float:
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        num_batches = len(val_loader)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(val_loader, desc="Validation")):
                try:
                    input_sequence = batch['input_sequence'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    action_prediction_mask = batch['action_prediction_mask'].to(self.device)
                    target_properties = batch['target_properties'].to(self.device)
                    
                    # Forward pass
                    predictions = self.model(
                        input_sequence=input_sequence,
                        attention_mask=attention_mask,
                        action_prediction_mask=action_prediction_mask
                    )
                    
                    # Loss ê³„ì‚°
                    loss = self._compute_loss(predictions, batch, action_prediction_mask)
                    total_loss += loss.detach().item()
                    
                except Exception as e:
                    print(f"âŒ Validation error in batch {batch_idx}: {e}")
                    continue
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def _train_step(self, batch: Dict[str, torch.Tensor]) -> float:
        """ë‹¨ì¼ í›ˆë ¨ ìŠ¤í… - ê¹”ë”í•œ êµ¬í˜„"""
        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        self.optimizer.zero_grad()
        
        # ì…ë ¥ ì¤€ë¹„
        inputs = self._prepare_inputs(batch)
        
        # Forward pass
        predictions = self.model(**inputs)
        
        # Loss ê³„ì‚°
        loss = self._compute_loss(predictions, batch, inputs['action_prediction_mask'])
        
        # Backward pass - detach loss to prevent graph reuse
        if loss.requires_grad:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
        
        return loss.detach().item()
    
    def _compute_gate_loss(self, gate_pred: torch.Tensor, actions: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """ê²Œì´íŠ¸ ì˜ˆì¸¡ ì†ì‹¤"""
        actions = actions.to(self.device)
        valid_positions = mask.bool()
        
        if not valid_positions.any():
            return None
            
        # ìœ íš¨í•œ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ê°’
        gate_logits = gate_pred[valid_positions]
        
        # íƒ€ê²Ÿ ì¤€ë¹„ - SAR ë§¤í•‘, ìŠ¤ì¹¼ë¼ ê°’ë§Œ ì¶”ì¶œ
        targets = []
        batch_size, sar_seq_len = mask.shape
        _, action_seq_len = actions.shape[:2]
        
        for b in range(batch_size):
            for sar_idx in range(sar_seq_len):
                if mask[b, sar_idx]:
                    action_idx = sar_idx // 3
                    if action_idx < action_seq_len:
                        # ê²Œì´íŠ¸ IDëŠ” ìŠ¤ì¹¼ë¼ ê°’ì´ì–´ì•¼ í•¨
                        if len(actions.shape) > 2:
                            targets.append(actions[b, action_idx, 0])  # ì²« ë²ˆì§¸ ìš”ì†Œ (ê²Œì´íŠ¸ ID)
                        else:
                            targets.append(actions[b, action_idx])
        
        if len(targets) == 0:
            return None
            
        targets_tensor = torch.stack(targets).to(self.device).long()
        
        # Debug: Check target values range
        max_target = targets_tensor.max().item()
        min_target = targets_tensor.min().item()
        vocab_size = gate_logits.shape[-1]
        
        if max_target >= vocab_size or min_target < 0:
            print(f"âš ï¸ Target out of bounds: min={min_target}, max={max_target}, vocab_size={vocab_size}")
            # Clamp targets to valid range
            targets_tensor = torch.clamp(targets_tensor, 0, vocab_size - 1)
        
        return F.cross_entropy(gate_logits, targets_tensor)
    
    def _compute_position_loss(self, pos_pred: torch.Tensor, actions: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """ìœ„ì¹˜ ì˜ˆì¸¡ ì†ì‹¤ - actionsì—ì„œ qubit ìœ„ì¹˜ ì¶”ì¶œ"""
        actions = actions.to(self.device)
        valid_positions = mask.bool()
        
        if not valid_positions.any():
            return None
            
        # ìœ íš¨í•œ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ê°’ (ì²˜ìŒ 2ì°¨ì›ë§Œ ì‚¬ìš©)
        pos_logits = pos_pred[valid_positions][:, :2]
        
        # íƒ€ê²Ÿ ì¤€ë¹„ - actionsì—ì„œ qubit ìœ„ì¹˜ ì¶”ì¶œ [qubit1, qubit2]
        targets = []
        batch_size, sar_seq_len = mask.shape
        _, action_seq_len = actions.shape[:2]
        
        for b in range(batch_size):
            for sar_idx in range(sar_seq_len):
                if mask[b, sar_idx]:
                    action_idx = sar_idx // 3
                    if action_idx < action_seq_len:
                        if len(actions.shape) > 2:
                            # actions[gate_id, qubit1, qubit2, param]ì—ì„œ qubit1, qubit2 ì¶”ì¶œ
                            qubit_positions = actions[b, action_idx, 1:3]  # [qubit1, qubit2]
                            targets.append(qubit_positions)
                        else:
                            # 2Dì¸ ê²½ìš° ì „ì²´ ì‚¬ìš© (fallback)
                            targets.append(actions[b, action_idx])
        
        if len(targets) == 0:
            return None
            
        targets_tensor = torch.stack(targets).to(self.device)
        return F.mse_loss(pos_logits, targets_tensor.float())
    
    def _compute_parameter_loss(self, param_pred: torch.Tensor, actions: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ ì†ì‹¤ - actionsì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        actions = actions.to(self.device)
        valid_positions = mask.bool()
        
        if not valid_positions.any():
            return None
            
        # ìœ íš¨í•œ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ê°’
        param_logits = param_pred[valid_positions].squeeze(-1)
        
        # íƒ€ê²Ÿ ì¤€ë¹„ - actionsì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        targets = []
        batch_size, sar_seq_len = mask.shape
        _, action_seq_len = actions.shape[:2]
        
        for b in range(batch_size):
            for sar_idx in range(sar_seq_len):
                if mask[b, sar_idx]:
                    action_idx = sar_idx // 3
                    if action_idx < action_seq_len:
                        if len(actions.shape) > 2:
                            # actions[gate_id, qubit1, qubit2, param]ì—ì„œ param ì¶”ì¶œ
                            param_value = actions[b, action_idx, 3]  # ë„¤ ë²ˆì§¸ ìš”ì†Œ (íŒŒë¼ë¯¸í„°)
                            targets.append(param_value)
                        else:
                            # 2Dì¸ ê²½ìš° ì „ì²´ ì‚¬ìš© (fallback)
                            targets.append(actions[b, action_idx])
        
        if len(targets) == 0:
            return None
            
        targets_tensor = torch.stack(targets).to(self.device)
        return F.mse_loss(param_logits, targets_tensor.float())
    
    def _extract_targets(self, data: torch.Tensor, mask: torch.Tensor) -> List[torch.Tensor]:
        """SAR ì‹œí€€ìŠ¤ì—ì„œ íƒ€ê²Ÿ ì¶”ì¶œ"""
        targets = []
        batch_size, sar_seq_len = mask.shape
        _, data_seq_len = data.shape[:2]
        
        for b in range(batch_size):
            for sar_idx in range(sar_seq_len):
                if mask[b, sar_idx]:
                    action_idx = sar_idx // 3  # SAR -> Action ë§¤í•‘
                    if action_idx < data_seq_len:
                        if len(data.shape) > 2:
                            # ë‹¤ì°¨ì› ë°ì´í„°ì—ì„œ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì¶”ì¶œ (ìŠ¤ì¹¼ë¼)
                            if data.shape[2] == 1:
                                targets.append(data[b, action_idx, 0])
                            else:
                                # ë²¡í„°ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œë§Œ (gate predictionìš©)
                                targets.append(data[b, action_idx, 0])
                        else:
                            # 2D ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                            targets.append(data[b, action_idx])
        
        return targets
    
    def _log_metrics(self, loss_value: float):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if not self.use_wandb:
            return
            
        log_dict = {
            'train/loss': loss_value,
            'train/lr': self.optimizer.param_groups[0]['lr'],
            'train/step': self.global_step,
            'train/epoch': self.current_epoch
        }
        
        # ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        memory_info = self.get_memory_status()
        if 'allocated_gb' in memory_info:
            log_dict['system/gpu_memory_gb'] = memory_info['allocated_gb']
            
        wandb.log(log_dict, step=self.global_step)
    
    def _prepare_inputs(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ì…ë ¥ í…ì„œ ì¤€ë¹„"""
        return {
            'input_sequence': batch['input_sequence'].to(self.device),
            'attention_mask': batch['attention_mask'].to(self.device),
            'action_prediction_mask': batch['action_prediction_mask'].to(self.device)
        }
    
    def _compute_loss(self, predictions: Dict[str, torch.Tensor], 
                     batch: Dict[str, torch.Tensor], 
                     mask: torch.Tensor) -> torch.Tensor:
        """ì†ì‹¤ ê³„ì‚° - ë‹¨ìˆœí™”"""
        losses = []
        
        # Gate prediction loss
        if 'actions' in batch and batch['actions'] is not None:
            gate_loss = self._compute_gate_loss(predictions['gate'], batch['actions'], mask)
            if gate_loss is not None:
                losses.append(gate_loss)
        
        # Position prediction loss - actionsì—ì„œ qubit ìœ„ì¹˜ ì¶”ì¶œ
        if 'actions' in batch and batch['actions'] is not None:
            pos_loss = self._compute_position_loss(predictions['position'], batch['actions'], mask)
            if pos_loss is not None:
                losses.append(pos_loss)
        
        # Parameter prediction loss - actionsì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        if 'actions' in batch and batch['actions'] is not None:
            param_loss = self._compute_parameter_loss(predictions['parameter'], batch['actions'], mask)
            if param_loss is not None:
                losses.append(param_loss)
        
        if losses:
            return torch.stack(losses).mean()
        else:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def save_checkpoint(self, filepath: str, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'global_step': self.global_step,
            'config': self.config
        }
        
        torch.save(checkpoint, filepath)
        
        if is_best:
            best_path = str(Path(filepath).parent / 'best_model.pt')
            torch.save(checkpoint, best_path)
            print(f"âœ… Best model saved to {best_path}")
    
    def load_checkpoint(self, filepath: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.current_epoch = checkpoint['epoch']
        self.best_val_loss = checkpoint['best_val_loss']
        self.global_step = checkpoint['global_step']
        
        print(f"âœ… Checkpoint loaded from {filepath}")
        print(f"   - Epoch: {self.current_epoch}")
        print(f"   - Best Val Loss: {self.best_val_loss:.4f}")
    
    def train(
        self, 
        train_loader: DataLoader, 
        val_loader: DataLoader,
        num_epochs: int,
        save_dir: str = "checkpoints"
    ):
        """ì „ì²´ í›ˆë ¨ ë£¨í”„"""
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True)
        
        print(f"ğŸš€ Decision Transformer í›ˆë ¨ ì‹œì‘")
        print(f"   - ì—í¬í¬: {num_epochs}")
        print(f"   - ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {save_path}")
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            
            # í›ˆë ¨
            train_loss = self.train_epoch(train_loader)
            
            # ê²€ì¦
            val_loss = self.validate(val_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs}")
            print(f"   - Train Loss: {train_loss:.4f}")
            print(f"   - Val Loss: {val_loss:.4f}")
            print(f"   - LR: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # WandB ì—í¬í¬ ë¡œê¹… (ìƒì„¸)
            if self.use_wandb:
                epoch_log = {
                    'epoch': epoch,
                    'train/epoch_loss': train_loss,
                    'val/epoch_loss': val_loss,
                    'train/lr': self.optimizer.param_groups[0]['lr'],
                    'val/loss_improvement': self.best_val_loss - val_loss if val_loss < self.best_val_loss else 0,
                    'train/best_val_loss': self.best_val_loss
                }
                
                # ëª¨ë¸ íŒŒë¼ë¯¸í„° í†µê³„
                param_stats = self._get_model_parameter_stats()
                epoch_log.update(param_stats)
                
                wandb.log(epoch_log)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
            
            checkpoint_path = save_path / f"checkpoint_epoch_{epoch+1}.pt"
            self.save_checkpoint(str(checkpoint_path), is_best)
            
            # Early stopping (ì„ íƒì )
            if hasattr(self.config, 'early_stopping_patience'):
                # êµ¬í˜„ ê°€ëŠ¥
                pass
        
        print(f"ğŸ‰ í›ˆë ¨ ì™„ë£Œ! Best Val Loss: {self.best_val_loss:.4f}")
        
        if self.use_wandb:
            wandb.finish()
    
    def _smart_memory_cleanup(self, batch_idx: int):
        """íš¨ìœ¨ì ì¸ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬"""
        # CUDAê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
        if not torch.cuda.is_available():
            return
        
        # ì •ë¦¬ ê°„ê²© ë° ë©”ëª¨ë¦¬ ì„ê³„ê°’ í™•ì¸
        should_cleanup = False
        
        # 1. ì£¼ê¸°ì  ì •ë¦¬ (ë„ˆë¬´ ìì£¼ í•˜ì§€ ì•Šê²Œ)
        if batch_idx - self.last_cleanup_batch >= self.cleanup_interval:
            should_cleanup = True
            reason = f"interval ({self.cleanup_interval} batches)"
        
        # 2. ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì •ë¦¬
        if not should_cleanup:
            memory_info = get_memory_info()
            if 'allocated_gb' in memory_info and memory_info['allocated_gb'] > self.memory_threshold_gb:
                should_cleanup = True
                reason = f"memory threshold ({memory_info['allocated_gb']:.1f}GB > {self.memory_threshold_gb}GB)"
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰
        if should_cleanup:
            cleanup_memory()
            self.last_cleanup_batch = batch_idx
            
            # ë””ë²„ê·¸ ì •ë³´ (ê°€ë” ì¶œë ¥)
            if batch_idx % (self.cleanup_interval * 2) == 0:  # ëœ ë¹ˆë²ˆí•˜ê²Œ ì¶œë ¥
                memory_after = get_memory_info()
                if 'allocated_gb' in memory_after:
                    print(f"ğŸ§¹ Memory cleanup at batch {batch_idx} ({reason}) - Memory: {memory_after['allocated_gb']:.1f}GB")
    
    def get_memory_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        return get_memory_info()
    
    def force_memory_cleanup(self):
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        cleanup_memory()
        print(f"ğŸ§¹ Forced memory cleanup - Memory: {get_memory_info().get('allocated_gb', 'N/A')}GB")
    
    def _get_model_parameter_stats(self) -> Dict[str, float]:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° í†µê³„ ìˆ˜ì§‘"""
        stats = {}
        
        # ì „ì²´ íŒŒë¼ë¯¸í„° í†µê³„
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        stats['model/total_parameters'] = total_params
        stats['model/trainable_parameters'] = trainable_params
        
        # ê°€ì¤‘ì¹˜ í†µê³„
        weights = []
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None:
                weights.extend(param.data.flatten().cpu().numpy())
        
        if weights:
            import numpy as np
            weights = np.array(weights)
            stats['model/weight_mean'] = float(np.mean(weights))
            stats['model/weight_std'] = float(np.std(weights))
            stats['model/weight_min'] = float(np.min(weights))
            stats['model/weight_max'] = float(np.max(weights))
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„
        grads = []
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None:
                grads.extend(param.grad.data.flatten().cpu().numpy())
        
        if grads:
            import numpy as np
            grads = np.array(grads)
            stats['model/grad_mean'] = float(np.mean(grads))
            stats['model/grad_std'] = float(np.std(grads))
            stats['model/grad_min'] = float(np.min(grads))
            stats['model/grad_max'] = float(np.max(grads))
        
        return stats
