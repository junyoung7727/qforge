"""
ì—í¬í¬ ë ˆë²¨ ë°ì´í„° ìºì‹± ì‹œìŠ¤í…œ v2
ì²« ë²ˆì§¸ ì—í¬í¬ì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ì´í›„ ì—í¬í¬ì—ì„œ ì¬ì‚¬ìš©
ë²„ì „ ê´€ë¦¬ì™€ ë©”íƒ€ë°ì´í„° ê²€ì¦ì„ í¬í•¨í•œ ì•ˆì •ì ì¸ ìºì‹± ì‹œìŠ¤í…œ
"""

import os
import pickle
import hashlib
import json
import time
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import torch


class EpochCache:
    """ì—í¬í¬ ë ˆë²¨ ë°ì´í„° ìºì‹± ì‹œìŠ¤í…œ v2"""
    
    CACHE_VERSION = "2.0"  # ìºì‹œ ë²„ì „
    METADATA_FILE = "cache_metadata.json"  # ë©”íƒ€ë°ì´í„° íŒŒì¼ëª…
    
    def __init__(self, cache_dir: str = "cache", max_cache_size_gb: float = 2.0):
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ ëª…í™•í•œ ìœ„ì¹˜ ì§€ì •
        if not os.path.isabs(cache_dir):
            cache_dir = os.path.abspath(cache_dir)
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_cache_size_bytes = int(max_cache_size_gb * 1024 * 1024 * 1024)
        self.metadata_path = self.cache_dir / self.METADATA_FILE
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ë¹ ë¥¸ ì ‘ê·¼ìš©)
        self._memory_cache: Dict[str, Any] = {}
        self._cache_stats = {
            'memory_hits': 0,
            'disk_hits': 0,
            'misses': 0,
            'saves': 0,
            'invalidations': 0
        }
        
        # ìºì‹œ ì´ˆê¸°í™” ë° ë²„ì „ ê²€ì¦
        self._initialize_cache()
        
    def _initialize_cache(self):
        """ìºì‹œ ì´ˆê¸°í™” ë° ë²„ì „ ê²€ì¦ - êµ¬ë²„ì „ ìºì‹œ ì™„ì „ ì œê±°"""
        try:
            # êµ¬ë²„ì „ ìºì‹œ íŒŒì¼ë“¤ ë¨¼ì € ì •ë¦¬
            self._cleanup_old_version_cache()
            
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            if self.metadata_path.exists():
                with open(self.metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                # ë²„ì „ í˜¸í™˜ì„± ê²€ì‚¬
                if metadata.get('version') != self.CACHE_VERSION:
                    self._clear_all_cache()
                    self._create_metadata()
            else:
                # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ìƒì„±
                self._create_metadata()
                
        except Exception as e:
            print(f"âŒ Cache initialization failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Cache initialization failed: {e}") from e
    
    def _create_metadata(self):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±"""
        metadata = {
            'version': self.CACHE_VERSION,
            'created_at': time.time(),
            'cache_entries': {}
        }
        
        with open(self.metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"ğŸ“ ìºì‹œ ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {self.metadata_path}")
    
    def _update_metadata(self, cache_key: str, cache_info: Dict[str, Any]):
        """ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            if self.metadata_path.exists():
                with open(self.metadata_path, 'r') as f:
                    metadata = json.load(f)
            else:
                metadata = {'version': self.CACHE_VERSION, 'cache_entries': {}}
            
            metadata['cache_entries'][cache_key] = cache_info
            
            with open(self.metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
                
        except Exception as e:
            print(f"âŒ Metadata update failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Metadata update failed: {e}") from e
    
    def _generate_dataset_hash(self, dataloader) -> str:
        """ë°ì´í„°ë¡œë”ì˜ ê³ ìœ  í•´ì‹œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        # ê¸°ë³¸ ë°ì´í„°ì…‹ ì •ë³´
        dataset_info = {
            'dataset_size': len(dataloader.dataset),
            'batch_size': dataloader.batch_size,
            'shuffle': getattr(dataloader, 'shuffle', False),
            'cache_version': self.CACHE_VERSION  # ë²„ì „ í¬í•¨
        }
        
        # ì•ˆì „í•œ ì²« ë²ˆì§¸ ë°°ì¹˜ ìƒ˜í”Œë§
        try:
            # ë°ì´í„°ë¡œë” ìƒíƒœ ë³´ì¡´ì„ ìœ„í•œ ì„ì‹œ ì´í„°ë ˆì´í„° ìƒì„±
            temp_iter = iter(dataloader)
            first_batch = next(temp_iter)
            
            if isinstance(first_batch, dict):
                # ë°°ì¹˜ êµ¬ì¡° ì •ë³´ ì¶”ê°€
                batch_keys = sorted(first_batch.keys())
                dataset_info['batch_structure'] = batch_keys
                
                # íšŒë¡œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì¶”ê°€
                if 'circuit_specs' in first_batch:
                    specs = first_batch['circuit_specs'][:3]  # ì²˜ìŒ 3ê°œë§Œ
                    try:
                        gate_counts = [len(spec.gates) if hasattr(spec, 'gates') else 0 for spec in specs]
                        dataset_info['sample_gate_counts'] = gate_counts
                    except Exception as e:
                        print(f"âŒ Gate count extraction failed: {e}")
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"Gate count extraction failed: {e}") from e
                        
                # íŒ¨ë”© ì •ë³´ ì¶”ê°€ (padded_421 ê´€ë ¨)
                if 'input_sequence' in first_batch:
                    seq_shape = first_batch['input_sequence'].shape if hasattr(first_batch['input_sequence'], 'shape') else None
                    if seq_shape:
                        dataset_info['sequence_shape'] = list(seq_shape)
                        
        except Exception as e:
            print(f"âŒ Dataset hash generation failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Dataset hash generation failed: {e}") from e
            
        # ì•ˆì •ì ì¸ í•´ì‹œ ìƒì„±
        data_str = json.dumps(dataset_info, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()[:16]
    
    def _get_cache_path(self, dataset_hash: str, epoch: int) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„± (ë²„ì „ í¬í•¨)"""
        return self.cache_dir / f"epoch_{epoch}_{dataset_hash}_v{self.CACHE_VERSION.replace('.', '_')}.pkl"
    
    def has_cached_epoch(self, dataloader, epoch: int) -> bool:
        """ìºì‹œëœ ì—í¬í¬ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (êµ¬ë²„ì „ ìºì‹œ ìë™ ì œê±°)"""
        try:
            dataset_hash = self._generate_dataset_hash(dataloader)
            cache_key = f"{dataset_hash}_epoch_{epoch}"
            
            # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            if cache_key in self._memory_cache:
                return True
                
            # ë””ìŠ¤í¬ ìºì‹œ í™•ì¸ (í˜„ì¬ ë²„ì „ë§Œ)
            cache_path = self._get_cache_path(dataset_hash, epoch)
            if cache_path.exists():
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ìœ íš¨ì„± í™•ì¸
                return self._validate_cache_entry(cache_key, cache_path)
            
            return False
            
        except Exception as e:
            print(f"âŒ Cache validation failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Cache validation failed: {e}") from e
    
    def _validate_cache_entry(self, cache_key: str, cache_path: Path) -> bool:
        """ìºì‹œ ì—”íŠ¸ë¦¬ ìœ íš¨ì„± ê²€ì¦ - êµ¬ë²„ì „ ìºì‹œ ì™„ì „ ì°¨ë‹¨"""
        try:
            # íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ í™•ì¸
            if not cache_path.name.endswith(f"_v{self.CACHE_VERSION.replace('.', '_')}.pkl"):
                # êµ¬ë²„ì „ ìºì‹œ íŒŒì¼ì€ ì¡°ìš©íˆ ì‚­ì œ
                self._invalidate_cache_file(cache_path)
                return False
                
            if not self.metadata_path.exists():
                return False
                
            with open(self.metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # ë©”íƒ€ë°ì´í„° ë²„ì „ í™•ì¸
            if metadata.get('version') != self.CACHE_VERSION:
                self._invalidate_cache_file(cache_path)
                return False
            
            # ë©”íƒ€ë°ì´í„°ì— ì—”íŠ¸ë¦¬ê°€ ìˆëŠ”ì§€ í™•ì¸
            if cache_key not in metadata.get('cache_entries', {}):
                self._invalidate_cache_file(cache_path)
                return False
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì¬í™•ì¸
            if not cache_path.exists():
                return False
            
            return True
            
        except Exception as e:
            print(f"âŒ Cache entry validation failed: {e}")
            import traceback
            traceback.print_exc()
            self._invalidate_cache_file(cache_path)
            raise RuntimeError(f"Cache entry validation failed: {e}") from e
    
    def _invalidate_cache_file(self, cache_path: Path):
        """ê°œë³„ ìºì‹œ íŒŒì¼ ë¬´íš¨í™”"""
        try:
            if cache_path.exists():
                cache_path.unlink()
                self._cache_stats['invalidations'] += 1
        except Exception as e:
            print(f"âŒ Cache file deletion failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Cache file deletion failed: {e}") from e
    
    def clear_cache(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ (ì‚¬ìš©ì í˜¸ì¶œìš©)"""
        self._clear_all_cache()
        print("ğŸ§¹ ëª¨ë“  ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _cleanup_old_version_cache(self):
        """êµ¬ë²„ì „ ìºì‹œ íŒŒì¼ë“¤ ì¡°ìš©íˆ ì‚­ì œ"""
        try:
            current_version_suffix = f"_v{self.CACHE_VERSION.replace('.', '_')}.pkl"
            deleted_count = 0
            
            # êµ¬ë²„ì „ ìºì‹œ íŒŒì¼ íŒ¨í„´ ê²€ì‚¬
            old_patterns = [
                "*_padded_*.pkl",  # êµ¬ë²„ì „ padded íŒ¨í„´
                "*_v1_*.pkl",      # v1 ë²„ì „
                "*_421.pkl"        # íŠ¹ì • ê¸¸ì´ íŒ¨í„´
            ]
            
            for pattern in old_patterns:
                for cache_file in self.cache_dir.glob(pattern):
                    try:
                        cache_file.unlink(missing_ok=True)
                        deleted_count += 1
                        print(f"âš ï¸  ìºì‹œ ë¬´íš¨í™”: êµ¬ë²„ì „ ë©”íƒ€ë°ì´í„° êµ¬ì¡° - {cache_file.name}")
                    except Exception as e:
                        print(f"âŒ Cache file cleanup failed: {e}")
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"Cache file cleanup failed: {e}") from e
            
            # í˜„ì¬ ë²„ì „ì´ ì•„ë‹Œ íŒŒì¼ë“¤ ì‚­ì œ
            for cache_file in self.cache_dir.glob("*.pkl"):
                if not cache_file.name.endswith(current_version_suffix):
                    try:
                        cache_file.unlink(missing_ok=True)
                        deleted_count += 1
                    except Exception as e:
                        print(f"âŒ Cache file deletion failed: {e}")
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"Cache file deletion failed: {e}") from e
                
        except Exception as e:
            print(f"âŒ Cache cleanup failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Cache cleanup failed: {e}") from e
    
    def _clear_all_cache(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ (ë‚´ë¶€ ì‚¬ìš©)"""
        # ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ
        self._memory_cache.clear()
        
        try:
            # ë””ìŠ¤í¬ ìºì‹œ ì‚­ì œ (ëª¨ë“  íŒ¨í„´)
            deleted_count = 0
            cache_patterns = ["*.pkl", "*.json"]
            
            for pattern in cache_patterns:
                for cache_file in self.cache_dir.glob(pattern):
                    try:
                        cache_file.unlink(missing_ok=True)
                        deleted_count += 1
                    except Exception as e:
                        print(f"âŒ Cache file deletion failed: {e}")
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"Cache file deletion failed: {e}") from e
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì‚­ì œ
            if self.metadata_path.exists():
                try:
                    self.metadata_path.unlink()
                except Exception as e:
                    print(f"âŒ Metadata file deletion failed: {e}")
                    import traceback
                    traceback.print_exc()
                    raise RuntimeError(f"Metadata file deletion failed: {e}") from e
                
        except Exception as e:
            print(f"âŒ Clear all cache failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Clear all cache failed: {e}") from e
    
    def save_epoch_data(self, dataloader, epoch: int, processed_batches: List[Dict[str, Any]]):
        """ì—í¬í¬ ì²˜ë¦¬ ë°ì´í„° ì €ì¥"""
        dataset_hash = self._generate_dataset_hash(dataloader)
        cache_key = f"{dataset_hash}_epoch_{epoch}"
        cache_path = self._get_cache_path(dataset_hash, epoch)
        
        try:
            # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
            self._memory_cache[cache_key] = processed_batches
            
            # ë””ìŠ¤í¬ì— ì €ì¥
            with open(cache_path, 'wb') as f:
                pickle.dump(processed_batches, f)
            
            self._cache_stats['saves'] += 1
            
            # ìºì‹œ í¬ê¸° ê´€ë¦¬
            self._cleanup_old_cache()
            
            print(f" ì—í¬í¬ {epoch} ë°ì´í„° ìºì‹œ ì €ì¥ ì™„ë£Œ ({len(processed_batches)} ë°°ì¹˜)")
            
        except Exception as e:
            print(f"âŒ Epoch cache save failed: {e}")
            import traceback
            traceback.print_exc()
            if cache_key in self._memory_cache:
                del self._memory_cache[cache_key]
    
    def load_epoch_data(self, dataloader, epoch: int) -> Optional[List[Dict[str, Any]]]:
        """ìºì‹œëœ ì—í¬í¬ ë°ì´í„° ë¡œë“œ"""
        dataset_hash = self._generate_dataset_hash(dataloader)
        cache_key = f"{dataset_hash}_epoch_{epoch}"
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if cache_key in self._memory_cache:
            self._cache_stats['memory_hits'] += 1
            print(f" ì—í¬í¬ {epoch} ë°ì´í„° ë©”ëª¨ë¦¬ ìºì‹œ íˆíŠ¸!")
            return self._memory_cache[cache_key]
        
        # ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
        cache_path = self._get_cache_path(dataset_hash, epoch)
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    processed_batches = pickle.load(f)
                
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                self._memory_cache[cache_key] = processed_batches
                
                self._cache_stats['disk_hits'] += 1
                print(f"ğŸ“‚ ì—í¬í¬ {epoch} ë°ì´í„° ë””ìŠ¤í¬ ìºì‹œ íˆíŠ¸!")
                return processed_batches
                
            except Exception as e:
                print(f"âŒ Cache file load failed: {e}")
                import traceback
                traceback.print_exc()
                self._invalidate_cache_file(cache_path)
                raise RuntimeError(f"Cache file load failed: {e}") from e
        
        self._cache_stats['misses'] += 1
        return None
    
    def _cleanup_old_cache(self):
        """ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ ì‚­ì œ (í¬ê¸° ì œí•œ ê¸°ë°˜)"""
        try:
            cache_files = list(self.cache_dir.glob("*.pkl"))
            if not cache_files:
                return
            
            # íŒŒì¼ í¬ê¸° ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
            total_size = 0
            valid_files = []
            for f in cache_files:
                try:
                    if f.exists():
                        total_size += f.stat().st_size
                        valid_files.append(f)
                except (OSError, FileNotFoundError) as e:
                    print(f"âŒ File access failed: {e}")
                    import traceback
                    traceback.print_exc()
                    raise RuntimeError(f"File access failed: {e}") from e
            
            if total_size > self.max_cache_size_bytes:
                # ì˜¤ë˜ëœ íŒŒì¼ë¶€í„° ì‚­ì œ (ìˆ˜ì • ì‹œê°„ ê¸°ì¤€)
                valid_files.sort(key=lambda f: f.stat().st_mtime)
                
                deleted_size = 0
                for cache_file in valid_files:
                    if total_size - deleted_size <= self.max_cache_size_bytes * 0.8:
                        break
                    
                    try:
                        if cache_file.exists():
                            file_size = cache_file.stat().st_size
                            cache_file.unlink(missing_ok=True)
                            deleted_size += file_size
                    except Exception as e:
                        print(f"âŒ Individual file deletion failed: {e}")
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"Individual file deletion failed: {e}") from e
                        
        except Exception as e:
            print(f"âŒ Cache cleanup failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Cache cleanup failed: {e}") from e
    
    def clear_cache(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        self._memory_cache.clear()
        
        try:
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            print("ğŸ§¹ ëª¨ë“  ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ Cache deletion failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Cache deletion failed: {e}") from e
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = sum(self._cache_stats.values()) - self._cache_stats['saves']
        hit_rate = 0.0
        
        if total_requests > 0:
            total_hits = self._cache_stats['memory_hits'] + self._cache_stats['disk_hits']
            hit_rate = (total_hits / total_requests) * 100
        
        return {
            **self._cache_stats,
            'hit_rate_percent': hit_rate,
            'total_requests': total_requests,
            'memory_cache_size': len(self._memory_cache)
        }
    
    def print_cache_stats(self):
        """ìºì‹œ í†µê³„ ì¶œë ¥"""
        stats = self.get_cache_stats()
        print(f" ì—í¬í¬ ìºì‹œ í†µê³„:")
        print(f"   ë©”ëª¨ë¦¬ íˆíŠ¸: {stats['memory_hits']}")
        print(f"   ë””ìŠ¤í¬ íˆíŠ¸: {stats['disk_hits']}")
        print(f"   ë¯¸ìŠ¤: {stats['misses']}")
        print(f"   ì €ì¥: {stats['saves']}")
        print(f"   íˆíŠ¸ìœ¨: {stats['hit_rate_percent']:.1f}%")
        print(f"   ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸°: {stats['memory_cache_size']}")


# ìºì‹œ í…ŒìŠ¤íŠ¸ ë©”ì¸ ì½”ë“œ
if __name__ == "__main__":
    import torch
    from torch.utils.data import DataLoader, TensorDataset
    
    print("ğŸ§ª ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°ì…‹ ìƒì„±
    class DummyDataset:
        def __init__(self, size=100):
            self.size = size
            
        def __len__(self):
            return self.size
            
        def __getitem__(self, idx):
            return {
                'input_sequence': torch.randn(421),  # padded_421 ì‹œë®¬ë ˆì´ì…˜
                'target_actions': torch.randint(0, 10, (50,)),
                'attention_mask': torch.ones(421),
                'action_prediction_mask': torch.ones(50),
                'circuit_specs': [{'gates': ['H', 'CNOT', 'RZ'] * (idx % 5 + 1)}]
            }
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±
    dataset = DummyDataset(size=50)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)
    
    # ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    cache = EpochCache(cache_dir="test_cache", max_cache_size_gb=0.1)
    
    print("\n1ï¸âƒ£ ìºì‹œ ì´ˆê¸° ìƒíƒœ í™•ì¸")
    print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {cache.cache_dir}")
    print(f"ë©”íƒ€ë°ì´í„° íŒŒì¼: {cache.metadata_path}")
    cache.print_cache_stats()
    
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°°ì¹˜ ë°ì´í„°
    dummy_batches = [
        {'batch_id': i, 'data': f'batch_{i}_data', 'processed': True}
        for i in range(5)
    ]
    
    print("\n2ï¸âƒ£ ìºì‹œ ì €ì¥ í…ŒìŠ¤íŠ¸")
    epoch = 1
    has_cache_before = cache.has_cached_epoch(dataloader, epoch)
    print(f"ì—í¬í¬ {epoch} ìºì‹œ ì¡´ì¬ ì—¬ë¶€ (ì €ì¥ ì „): {has_cache_before}")
    
    # ìºì‹œ ì €ì¥
    cache.save_epoch_data(dataloader, epoch, dummy_batches)
    
    has_cache_after = cache.has_cached_epoch(dataloader, epoch)
    print(f"ì—í¬í¬ {epoch} ìºì‹œ ì¡´ì¬ ì—¬ë¶€ (ì €ì¥ í›„): {has_cache_after}")
    
    print("\n3ï¸âƒ£ ìºì‹œ ë¡œë“œ í…ŒìŠ¤íŠ¸")
    loaded_data = cache.load_epoch_data(dataloader, epoch)
    if loaded_data:
        print(f"âœ… ìºì‹œ ë¡œë“œ ì„±ê³µ! ë°°ì¹˜ ìˆ˜: {len(loaded_data)}")
        print(f"ì²« ë²ˆì§¸ ë°°ì¹˜: {loaded_data[0]}")
    else:
        print("âŒ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨")
    
    print("\n4ï¸âƒ£ ë©”ëª¨ë¦¬ ìºì‹œ í…ŒìŠ¤íŠ¸")
    # ë©”ëª¨ë¦¬ì—ì„œ ë‹¤ì‹œ ë¡œë“œ
    loaded_data_memory = cache.load_epoch_data(dataloader, epoch)
    if loaded_data_memory:
        print("âœ… ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ë¡œë“œ ì„±ê³µ!")
    
    print("\n5ï¸âƒ£ ë‹¤ë¥¸ ì—í¬í¬ í…ŒìŠ¤íŠ¸")
    epoch2 = 2
    dummy_batches2 = [
        {'batch_id': i, 'data': f'epoch2_batch_{i}_data', 'processed': True}
        for i in range(3)
    ]
    cache.save_epoch_data(dataloader, epoch2, dummy_batches2)
    
    print("\n6ï¸âƒ£ ìºì‹œ í†µê³„ í™•ì¸")
    cache.print_cache_stats()
    
    print("\n7ï¸âƒ£ ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸")
    if cache.metadata_path.exists():
        with open(cache.metadata_path, 'r') as f:
            metadata = json.load(f)
        print(f"ë©”íƒ€ë°ì´í„° ë²„ì „: {metadata.get('version')}")
        print(f"ìºì‹œ ì—”íŠ¸ë¦¬ ìˆ˜: {len(metadata.get('cache_entries', {}))}")
        for key, info in metadata.get('cache_entries', {}).items():
            print(f"  - {key}: {info.get('batch_count')}ê°œ ë°°ì¹˜, {info.get('file_size', 0)/1024:.1f}KB")
    
    print("\n8ï¸âƒ£ ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸")
    # ë‹¤ë¥¸ ë°ì´í„°ë¡œë”ë¡œ í…ŒìŠ¤íŠ¸ (ë‹¤ë¥¸ í•´ì‹œ ìƒì„±)
    dataset_different = DummyDataset(size=30)  # ë‹¤ë¥¸ í¬ê¸°
    dataloader_different = DataLoader(dataset_different, batch_size=8, shuffle=False)
    
    has_cache_different = cache.has_cached_epoch(dataloader_different, epoch)
    print(f"ë‹¤ë¥¸ ë°ì´í„°ë¡œë”ì˜ ì—í¬í¬ {epoch} ìºì‹œ ì¡´ì¬ ì—¬ë¶€: {has_cache_different}")
    
    print("\n9ï¸âƒ£ ìºì‹œ ì •ë¦¬ í…ŒìŠ¤íŠ¸")
    print("ìºì‹œ ì •ë¦¬ ì „:")
    cache_files_before = list(cache.cache_dir.glob("*.pkl"))
    print(f"ìºì‹œ íŒŒì¼ ìˆ˜: {len(cache_files_before)}")
    
    # ìºì‹œ ì •ë¦¬ (í¬ê¸° ì œí•œì„ ë§¤ìš° ì‘ê²Œ ì„¤ì •)
    cache.max_cache_size_bytes = 1024  # 1KBë¡œ ì œí•œ
    cache._cleanup_old_cache()
    
    print("ìºì‹œ ì •ë¦¬ í›„:")
    cache_files_after = list(cache.cache_dir.glob("*.pkl"))
    print(f"ìºì‹œ íŒŒì¼ ìˆ˜: {len(cache_files_after)}")
    
    print("\nğŸ”Ÿ ìµœì¢… í†µê³„")
    cache.print_cache_stats()
    
    print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ì •ë¦¬")
    cache.clear_cache()
    
    print("\nâœ… ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 50)
