"""
SOTA Property Prediction Transformer Training Pipeline
IntegratedPropertyPredictionTransformer ì „ìš© ëª¨ë“ˆí™”ëœ íŠ¸ë ˆì´ë„ˆ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import time
import numpy as np
from tqdm import tqdm
import wandb
import gc

# Import SOTA model only
from models.integrated_property_prediction_transformer import IntegratedPropertyPredictionTransformer, IntegratedPropertyPredictionConfig

# Import circuit interface
sys.path.append(str(Path(__file__).parent.parent.parent / "quantumcommon"))


class PropertyPredictionTrainer:
    """SOTA Property Prediction Transformer ì „ìš© í•™ìŠµê¸°"""
    
    def __init__(
        self,
        model: IntegratedPropertyPredictionTransformer,
        config: IntegratedPropertyPredictionConfig,
        device: str = "auto",
        use_wandb: bool = True
    ):
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._get_device(device)
        
        # ëª¨ë¸ ë° ì„¤ì • (SOTA ëª¨ë¸ë§Œ ì§€ì›)
        self.model = model.to(self.device)
        self.config = config
        
        # WandB ì„¤ì •
        self.use_wandb = use_wandb
        if use_wandb:
            wandb.init(
                project="quantum-property-prediction",
                name=f"training_{int(time.time())}",
                config=config.__dict__ if hasattr(config, '__dict__') else {}
            )
        
        # í•™ìŠµ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._setup_training_components()
        
        # í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”
        self.current_epoch = 0
        self.current_step = 0
        self.best_val_loss = float('inf')
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        if self.device.type == 'cuda':
            torch.cuda.set_per_process_memory_fraction(0.8)
            torch.backends.cudnn.benchmark = False
            torch.backends.cudnn.deterministic = True
        
        print(f"âœ… SOTA Property Prediction Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ëª¨ë¸ íƒ€ì…: SOTA í†µí•© ëª¨ë¸")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def _get_device(self, device: str) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            return torch.device("cuda" if torch.cuda.is_available() else "cpu")
        return torch.device(device)
    
    def _setup_training_components(self):
        """ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜, ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate if hasattr(self.config, 'learning_rate') else 1e-4,
            weight_decay=self.config.weight_decay if hasattr(self.config, 'weight_decay') else 1e-5
        )
        
        # ì›Œë°ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        from torch.optim.lr_scheduler import LinearLR, SequentialLR
        
        warmup_steps = getattr(self.config, 'warmup_steps', 100)
        total_epochs = getattr(self.config, 'num_epochs', 100)
        
        # ì›Œë°ì—… ìŠ¤ì¼€ì¤„ëŸ¬ (ì²˜ìŒ 100ìŠ¤í… ë™ì•ˆ ì„ í˜• ì¦ê°€)
        warmup_scheduler = LinearLR(
            self.optimizer,
            start_factor=0.1,
            end_factor=1.0,
            total_iters=warmup_steps
        )
        
        # ë©”ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ (ì½”ì‚¬ì¸ ì–´ë‹ë§ with restarts)
        from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
        
        scheduler_type = getattr(self.config, 'scheduler_type', 'cosine')
        min_lr = getattr(self.config, 'min_learning_rate', 1e-6)
        
        if scheduler_type == "cosine_with_restarts":
            main_scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=20,  # ì²« ë²ˆì§¸ ì¬ì‹œì‘ ì£¼ê¸°
                T_mult=2,  # ì£¼ê¸° ë°°ìˆ˜
                eta_min=min_lr  # ìµœì†Œ í•™ìŠµë¥ 
            )
        else:
            main_scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=total_epochs - warmup_steps,
                eta_min=min_lr
            )
        
        # ìˆœì°¨ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ê²°í•©
        self.scheduler = SequentialLR(
            self.optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[warmup_steps]
        )
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.current_step = 0
        self.best_val_loss = float('inf')
        
    
    def _prepare_batch_data(self, batch):
        """ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„ - ìœ ë‹ˆíŒŒì´ë“œ ëŸ¬ë„ˆì—ì„œ ì´ë™"""
        circuit_specs = batch.get('circuit_specs', [])
        
        if not circuit_specs:
            return {
                'circuit_data': None,
                'target_properties': None
            }
        
        # íƒ€ê²Ÿ ì†ì„± ì¶”ì¶œ
        targets_dict = batch.get('targets')
        target_properties = None
        
        if targets_dict:
            batch_size = len(circuit_specs)
            target_properties = torch.zeros(batch_size, 3, device=self.device)
            
            # íƒ€ê²Ÿ ê°’ ì¶”ì¶œ ë° í…ì„œ ë³€í™˜
            entanglement_vals = targets_dict.get('entanglement', torch.zeros(batch_size))
            expressibility_vals = targets_dict.get('expressibility', torch.zeros(batch_size))
            fidelity_vals = targets_dict.get('fidelity', torch.ones(batch_size))
            
            # ì ì ˆí•œ í¬ê¸°ë¡œ ë³€í™˜
            if entanglement_vals.dim() == 0:
                entanglement_vals = entanglement_vals.unsqueeze(0).expand(batch_size)
            if expressibility_vals.dim() == 0:
                expressibility_vals = expressibility_vals.unsqueeze(0).expand(batch_size)
            if fidelity_vals.dim() == 0:
                fidelity_vals = fidelity_vals.unsqueeze(0).expand(batch_size)
            
            target_properties[:, 0] = entanglement_vals.to(self.device)
            target_properties[:, 1] = expressibility_vals.to(self.device)
            target_properties[:, 2] = fidelity_vals.to(self.device)
        
        return {
            'circuit_specs': circuit_specs,
            'target_properties': target_properties
        }
    
    def _compute_loss(self, predictions, targets):
        """SOTA ëª¨ë¸ ì†ì‹¤ ê³„ì‚°"""
        # SOTA ëª¨ë¸ì˜ ë‚´ì¥ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
        losses = self.model.compute_loss(predictions, targets)
        
        # ê°œë³„ ì†ì‹¤ ë° ë™ì  ê°€ì¤‘ì¹˜ ë¡œê¹…
        if hasattr(self, 'use_wandb') and self.use_wandb:
            import wandb
            log_dict = {}
            for loss_name, loss_value in losses.items():
                if loss_name != 'total' and isinstance(loss_value, torch.Tensor):
                    log_dict[f'train/{loss_name}_loss'] = loss_value.item()
            
            # ë™ì  ê°€ì¤‘ì¹˜ ë¡œê¹…
            if hasattr(self.model, 'loss_function') and hasattr(self.model.loss_function, 'log_vars'):
                log_vars = self.model.loss_function.log_vars.data
                for i, prop in enumerate(['entanglement', 'fidelity', 'expressibility']):
                    precision = torch.exp(-log_vars[i])
                    log_dict[f'train/dynamic_weight_{prop}'] = precision.item()
                    log_dict[f'train/uncertainty_{prop}'] = torch.exp(log_vars[i]).item()
            
            if log_dict:
                wandb.log(log_dict, step=self.current_step)
        
        # ì½˜ì†” ë¡œê¹…
        individual_losses = []
        for loss_name, loss_value in losses.items():
            if loss_name != 'total' and isinstance(loss_value, torch.Tensor):
                individual_losses.append(f"{loss_name}: {loss_value.item():.4f}")
        
        # ë™ì  ê°€ì¤‘ì¹˜ ë° ìŠ¤ì¼€ì¼ í‘œì‹œ
        if hasattr(self.model, 'loss_function') and hasattr(self.model.loss_function, 'log_vars'):
            log_vars = self.model.loss_function.log_vars.data
            loss_scales = self.model.loss_function.loss_scales
            
            weights_info = []
            scales_info = []
            for i, prop in enumerate(['ent', 'fid', 'exp']):
                weight = torch.exp(-log_vars[i]).item()
                scale = loss_scales[['entanglement', 'fidelity', 'expressibility'][i]]
                weights_info.append(f"{prop}:{weight:.3f}")
                scales_info.append(f"{prop}:{scale:.3f}")
            
            individual_losses.append(f"weights[{','.join(weights_info)}]")
            individual_losses.append(f"scales[{','.join(scales_info)}]")
        
        if individual_losses and self.current_step % 50 == 0:  # 50ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…
            print(f"ğŸ“Š Step {self.current_step} - Individual losses: {', '.join(individual_losses)}")
        
        return losses['total']
    
    def _forward_step(self, prepared_batch):
        """SOTA ëª¨ë¸ forward pass"""
        circuit_specs = prepared_batch['circuit_specs']
        target_properties = prepared_batch['target_properties']
        
        if circuit_specs is None or target_properties is None:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # íƒ€ê²Ÿì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        targets_dict = {
            'entanglement': target_properties[:, 0],
            'expressibility': target_properties[:, 1],
            'fidelity': target_properties[:, 2]
        }
        
        # Forward pass (targets ì „ë‹¬í•˜ì—¬ í†µê³„ ì—…ë°ì´íŠ¸)
        outputs = self.model(circuit_specs, targets=targets_dict)
        
        # Loss ê³„ì‚°
        loss = self._compute_loss(outputs, targets_dict)
        return loss
    
    def train_epoch(self, train_loader):
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(train_loader)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
        
        pbar = tqdm(train_loader, desc=f"Epoch {self.current_epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                self.optimizer.zero_grad()
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                prepared_batch = self._prepare_batch_data(batch)
                
                # Forward pass
                loss = self._forward_step(prepared_batch)
                
                # NaN/Inf ê²€ì‚¬
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸ Invalid loss at batch {batch_idx}, skipping...")
                    continue
                
                # Backward pass (ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ë™ì¼)
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                if hasattr(self.config, 'gradient_clipping') and self.config.gradient_clipping > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clipping)
                
                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                self.optimizer.step()
                self.optimizer.zero_grad()
                
                loss_value = loss.item()
                del loss
                
                # ì†ì‹¤ ëˆ„ì 
                total_loss += loss_value
                self.current_step += 1
                
                # Progress bar ì—…ë°ì´íŠ¸
                pbar.set_postfix({'loss': f'{loss_value:.4f}'})
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if batch_idx % 10 == 0:
                    torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"âš ï¸ CUDA memory error at batch {batch_idx}, clearing cache...")
                    torch.cuda.empty_cache()
                    gc.collect()
                    continue
                else:
                    raise e
        
        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss
    
    def _validation_step(self, prepared_batch):
        """ê²€ì¦ìš© ìŠ¤í… - í–¥ìƒëœ ë””ë²„ê¹… í¬í•¨"""
        circuit_specs = prepared_batch['circuit_specs']
        target_properties = prepared_batch['target_properties']
        
        if circuit_specs is None or target_properties is None:
            return 0.0
        
        # Forward pass
        outputs = self.model(circuit_specs)
        
        # íƒ€ê²Ÿì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        targets_dict = {
            'entanglement': target_properties[:, 0],
            'expressibility': target_properties[:, 1],
            'fidelity': target_properties[:, 2]
        }
        
        # Enhanced debugging for validation data
        if hasattr(self, '_val_step_counter'):
            self._val_step_counter += 1
        else:
            self._val_step_counter = 1
            
        # Debug validation data every 5 steps
        if self._val_step_counter % 5 == 0:
            print(f"\n[VALIDATION STEP {self._val_step_counter}] Data Analysis:")
            if isinstance(circuit_specs, list):
                print(f"  Circuit specs: list with {len(circuit_specs)} items")
            else:
                print(f"  Circuit specs shape: {circuit_specs.shape}")
            print(f"  Target properties shape: {target_properties.shape}")
            
            # Check target expressibility values
            exp_targets = targets_dict['expressibility']
            print(f"  Expressibility targets: min={exp_targets.min().item():.8f}, max={exp_targets.max().item():.8f}")
            print(f"  Expressibility targets: mean={exp_targets.mean().item():.8f}, std={exp_targets.std().item():.8f}")
            
            # Check model outputs
            if 'expressibility' in outputs:
                exp_outputs = outputs['expressibility']
                print(f"  Expressibility outputs: min={exp_outputs.min().item():.8f}, max={exp_outputs.max().item():.8f}")
                print(f"  Expressibility outputs: mean={exp_outputs.mean().item():.8f}, std={exp_outputs.std().item():.8f}")
        
        # ê²€ì¦ì—ì„œëŠ” ê°œë³„ ì†ì‹¤ë„ í•¨ê»˜ ë°˜í™˜
        losses = self.model.compute_loss(outputs, targets_dict)
        
        # Enhanced loss logging with detailed analysis
        if hasattr(self, 'use_wandb') and self.use_wandb:
            import wandb
            log_dict = {}
            
            # Debug loss computation
            if self._val_step_counter % 5 == 0:
                print(f"\n[VALIDATION LOSS] Computed losses:")
            
            for loss_name, loss_value in losses.items():
                if loss_name != 'total' and isinstance(loss_value, torch.Tensor):
                    loss_val = loss_value.item()
                    log_dict[f'val/{loss_name}_loss'] = loss_val
                    
                    # Debug specific loss values
                    if self._val_step_counter % 5 == 0:
                        print(f"  {loss_name}: {loss_val:.8f}")
                        
                        # Special attention to expressibility
                        if loss_name == 'expressibility':
                            if hasattr(self, '_prev_exp_loss'):
                                if abs(loss_val - self._prev_exp_loss) < 1e-10:
                                    print(f"  WARNING: Expressibility loss unchanged from previous: {self._prev_exp_loss:.8f}")
                                else:
                                    print(f"  Expressibility loss changed from: {self._prev_exp_loss:.8f} to {loss_val:.8f}")
                            self._prev_exp_loss = loss_val
            
            if log_dict:
                wandb.log(log_dict, step=self.current_step)
                
                # Debug wandb logging
                if self._val_step_counter % 5 == 0:
                    print(f"  Logged to wandb: {list(log_dict.keys())}")
        
        return losses['total'].item() if hasattr(losses['total'], 'item') else float(losses['total'])

    def validate(self, val_loader):
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        num_batches = len(val_loader)
        
        # ê°œë³„ ì†ì‹¤ ì§‘ê³„ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        accumulated_losses = {}
        valid_batches = 0
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc="Validation")
            for batch_idx, batch in enumerate(pbar):
                try:
                    # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                    prepared_batch = self._prepare_batch_data(batch)
                    
                    # Forward pass with detailed losses
                    circuit_specs = prepared_batch['circuit_specs']
                    target_properties = prepared_batch['target_properties']
                    
                    if circuit_specs is None or target_properties is None:
                        continue
                    
                    # Forward pass
                    outputs = self.model(circuit_specs)
                    
                    # íƒ€ê²Ÿì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
                    targets_dict = {
                        'entanglement': target_properties[:, 0],
                        'expressibility': target_properties[:, 1],
                        'fidelity': target_properties[:, 2]
                    }
                    
                    # ì†ì‹¤ ê³„ì‚°
                    losses = self.model.compute_loss(outputs, targets_dict)
                    total_loss_value = losses['total'].item() if hasattr(losses['total'], 'item') else float(losses['total'])
                    
                    # NaN/Inf ê²€ì‚¬
                    if torch.isnan(torch.tensor(total_loss_value)) or torch.isinf(torch.tensor(total_loss_value)):
                        print(f"âš ï¸ Warning: Invalid loss detected in validation batch {batch_idx}: {total_loss_value}")
                        continue
                    
                    total_loss += total_loss_value
                    valid_batches += 1
                    
                    # ê°œë³„ ì†ì‹¤ ì§‘ê³„
                    for loss_name, loss_value in losses.items():
                        if loss_name != 'total' and isinstance(loss_value, torch.Tensor):
                            loss_val = loss_value.item()
                            if loss_name not in accumulated_losses:
                                accumulated_losses[loss_name] = 0.0
                            accumulated_losses[loss_name] += loss_val
                    
                    pbar.set_postfix({'val_loss': f'{total_loss_value:.4f}'})
                    
                except Exception as e:
                    print(f"âŒ Error in validation batch {batch_idx}: {e}")
                    continue
        
        # í‰ê·  ê³„ì‚°
        avg_loss = total_loss / max(valid_batches, 1)
        
        # ê°œë³„ ì†ì‹¤ í‰ê·  ê³„ì‚° ë° wandb ë¡œê¹…
        if hasattr(self, 'use_wandb') and self.use_wandb and accumulated_losses:
            import wandb
            val_log_dict = {}
            
            for loss_name, total_loss_val in accumulated_losses.items():
                avg_loss_val = total_loss_val / max(valid_batches, 1)
                val_log_dict[f'val/{loss_name}_loss_epoch'] = avg_loss_val
            
            val_log_dict['val/total_loss_epoch'] = avg_loss
            val_log_dict['epoch'] = self.current_epoch
            
            wandb.log(val_log_dict, step=self.current_step)
            print(f"ğŸ“Š Validation losses logged to wandb: {list(val_log_dict.keys())}")
        
        return avg_loss
    
    def train(self, train_loader, val_loader=None, num_epochs=100):
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            
            # í•™ìŠµ
            train_loss = self.train_epoch(train_loader)
            
            # ê²€ì¦
            val_loss = None
            if val_loader is not None:
                val_loss = self.validate(val_loader)
                print(f"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")
                
                # WandB ë¡œê¹…
                if self.use_wandb:
                    wandb.log({
                        'train_loss': train_loss,
                        'val_loss': val_loss,
                        'epoch': epoch,
                        'learning_rate': self.optimizer.param_groups[0]['lr']
                    })
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ (val_loss: {val_loss:.4f})")
                    
                    # Save best model checkpoint (ìƒëŒ€ ê²½ë¡œ)
                    checkpoint_path = Path("./checkpoints/best_model.pt")
                    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
                    self.save_checkpoint(str(checkpoint_path))
                    print(f"âœ“ ìµœê³  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {checkpoint_path}")
                    print(f"âœ“ ì ˆëŒ€ ê²½ë¡œ: {checkpoint_path.resolve()}")
                    
                    if self.use_wandb:
                        wandb.log({"best_model_saved": True, "best_val_loss": val_loss})
            else:
                print(f"Epoch {epoch}: train_loss={train_loss:.4f}")
                
                if self.use_wandb:
                    wandb.log({
                        'train_loss': train_loss,
                        'epoch': epoch,
                        'learning_rate': self.optimizer.param_groups[0]['lr']
                    })
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            if self.scheduler:
                self.scheduler.step()
        
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        
        if self.use_wandb:
            wandb.finish()
        
        return {
            'best_val_loss': self.best_val_loss,
            'final_train_loss': train_loss,
            'final_val_loss': val_loss
        }
    
    def save_checkpoint(self, filepath):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ - ëª¨ë¸ ì„¤ì • í¬í•¨"""
        # ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ ì •ë³´
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'current_epoch': self.current_epoch,
            'current_step': self.current_step,
            'best_val_loss': self.best_val_loss,
        }
        
        # ëª¨ë¸ êµ¬ì„± ì •ë³´ ì €ì¥
        if hasattr(self.model, 'get_model_info'):
            # IntegratedPropertyPredictionTransformerì˜ get_model_info ë©”ì„œë“œ ì‚¬ìš©
            model_info = self.model.get_model_info()
            checkpoint['model_info'] = model_info
        
        # ëª¨ë¸ êµ¬ì„± í´ë˜ìŠ¤ íƒ€ì… ì €ì¥
        checkpoint['model_class'] = self.model.__class__.__name__
        
        # ì„¤ì • ì €ì¥ (dataclass ë˜ëŠ” dict í˜•íƒœ)
        if hasattr(self.config, '__dict__'):
            checkpoint['config'] = self.config.__dict__
        elif hasattr(self.config, '_asdict'):
            checkpoint['config'] = self.config._asdict()
        else:
            checkpoint['config'] = self.config if isinstance(self.config, dict) else {}
            
        # ëª¨ë¸ì˜ ì„¤ì • ê°ì²´ ì¢…ë¥˜ ì €ì¥
        if hasattr(self.config, '__class__'):
            checkpoint['config_class'] = self.config.__class__.__name__
        
        torch.save(checkpoint, filepath)
        print(f"âœ… ì„¤ì • í¬í•¨ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
        print(f"   ëª¨ë¸ í´ë˜ìŠ¤: {checkpoint['model_class']}")
        if 'config_class' in checkpoint:
            print(f"   ì„¤ì • í´ë˜ìŠ¤: {checkpoint['config_class']}")
        else:
            print(f"   ì„¤ì •: {type(checkpoint['config'])}")
            
        # ë²„í¼ í†µê³„ ì €ì¥ ë¡œê¹… (expressibility í†µê³„ í™•ì¸)
        if hasattr(self.model, 'prediction_head') and hasattr(self.model.prediction_head, 'exp_mean'):
            exp_mean = getattr(self.model.prediction_head, 'exp_mean', None)
            exp_std = getattr(self.model.prediction_head, 'exp_std', None)
            if exp_mean is not None and exp_std is not None:
                print(f"   Expressibility í†µê³„ - mean: {exp_mean.item():.6f}, std: {exp_std.item():.6f}")
        
    
    def load_checkpoint(self, filepath):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if checkpoint.get('scheduler_state_dict') and self.scheduler:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.current_epoch = checkpoint.get('current_epoch', 0)
        self.current_step = checkpoint.get('current_step', 0)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {filepath}")
        print(f"ì—í­ {self.current_epoch}ë¶€í„° ì¬ì‹œì‘")


def create_trainer(model, config, device="auto", use_wandb=True):
    """íŠ¸ë ˆì´ë„ˆ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return PropertyPredictionTrainer(
        model=model,
        config=config,
        device=device,
        use_wandb=use_wandb
    )