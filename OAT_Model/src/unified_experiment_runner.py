"""
Unified Experiment Runner
ìƒˆë¡œìš´ GNN ê¸°ë°˜ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì„ ì§€ì›í•˜ëŠ” í†µí•© ì‹¤í—˜ ì‹¤í–‰ê¸°
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import sys
from pathlib import Path
import argparse
import json
from tqdm import tqdm
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import wandb

# Add project paths
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent.parent.parent / "quantumcommon"))

# Import unified config
from config.unified_training_config import (
    UnifiedTrainingConfig, 
    ConfigManager,
    get_config_by_name
)

# Import new encoding system
from encoding.encoding_pipeline_factory import EncodingPipelineFactory
from encoding.modular_quantum_attention import AttentionMode
from encoding.property_prediction_integration import (
    EnhancedPropertyPredictionEncoder,
    create_integrated_property_predictor,
    DEFAULT_CONFIG
)

# Import models
from models.decision_transformer import DecisionTransformer
from models.unified_property_prediction_transformer import UnifiedPropertyPredictionTransformer
from config.unified_training_config import PropertyConfig as UnifiedPropertyPredictionConfig

# Import data
from data.quantum_circuit_dataset import DatasetManager, create_dataloaders
from training.dataset.property_prediction_dataset import PropertyPredictionDataset,collate_fn
from training.utils.early_stopping import EarlyStopping
from training.utils.checkpoint_manager import CheckpointManager
import torch.nn.functional as F
# Import gates
from gates import QuantumGateRegistry

class UnifiedExperimentRunner:
    """ìƒˆë¡œìš´ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì„ ì§€ì›í•˜ëŠ” í†µí•© ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config: UnifiedTrainingConfig, model_type: str = "property_predictor"):
        self.config = config
        self.model_type = model_type

        self.loss_fn = F.mse_loss
        
        # GPU ìµœì í™”: ë””ë°”ì´ìŠ¤ ì„¤ì •
        if hasattr(config.model, 'get_device'):
            device_str = config.model.get_device()
        else:
            # Fallback: ê¸°ë³¸ GPU ì‚¬ìš©
            device_str = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.device = torch.device(device_str)
        
        # CUDA ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì • (ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°)
        if self.device.type == 'cuda':
            torch.cuda.set_per_process_memory_fraction(0.8)  # GPU ë©”ëª¨ë¦¬ì˜ 80%ë§Œ ì‚¬ìš©
            torch.backends.cudnn.benchmark = False  # ë©”ëª¨ë¦¬ ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
            torch.backends.cudnn.deterministic = True
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì (Gradient Accumulation) ì‚¬ìš© ì„¤ì • - ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¦ê°€
        self.grad_accum_steps = getattr(self.config.training, 'grad_accum_steps', 8)  # 4ì—ì„œ 8ë¡œ ì¦ê°€
        # ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
        original_batch_size = self.config.training.train_batch_size
        self.config.training.train_batch_size = original_batch_size
        self.effective_batch_size = self.config.training.train_batch_size * self.grad_accum_steps
        print(f"ğŸ”¥ Gradient Accumulation: {self.grad_accum_steps} steps")
        print(f"ğŸ”¥ Effective Batch Size: {self.effective_batch_size}")
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
            print(f"ğŸš€ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
            print(f"ğŸš€ Using GPU: {torch.cuda.get_device_name()}")
            
            # ë©”ëª¨ë¦¬ í™œìš© ì •ë³´ í‘œì‹œ
            reserved_mem = torch.cuda.memory_reserved(0) / 1024**3
            allocated_mem = torch.cuda.memory_allocated(0) / 1024**3
            print(f"ğŸ’¾ Reserved Memory: {reserved_mem:.2f}GB | Allocated Memory: {allocated_mem:.2f}GB")
        
        print(f"ğŸš€ Initializing Enhanced {model_type} Runner")
        print(f"Device: {self.device}")
        print(f"Experiment: {config.experiment.experiment_name}")
        print(f"Embedding Mode: {getattr(config.model, 'embedding_mode', 'gnn')}")
        print(f"Attention Mode: {getattr(config.model, 'attention_mode', 'advanced')}")
        
        # ë””ë²„ê¹… í”Œë˜ê·¸
        self.debug_memory = False
        
        # wandb ì´ˆê¸°í™”
        wandb.init(
            project="quantum-circuit-property-prediction",
            name=config.experiment.experiment_name,
            config={
                "model_type": model_type,
                "embedding_mode": getattr(config.model, 'embedding_mode', 'gnn'),
                "attention_mode": getattr(config.model, 'attention_mode', 'advanced'),
                "learning_rate": config.training.learning_rate,
                "batch_size": config.training.train_batch_size,
                "d_model": config.model.d_model,
                "n_heads": config.model.n_heads,
                "n_layers": config.model.n_layers,
                "entanglement_weight": getattr(config.training, 'entanglement_weight', 10.0),
                "expressibility_weight": getattr(config.training, 'expressibility_weight', 1.0),
                "fidelity_weight": getattr(config.training, 'fidelity_weight', 0.1),
            }
        )
        
        # Setup directories and seed
        config.setup_directories()
        config.set_seed()
        
        # Initialize encoding pipeline factory
        self.encoding_factory = EncodingPipelineFactory()
        
        # Initialize early stopping and checkpoint manager
        self.early_stopping = EarlyStopping(
            patience=getattr(config.training, 'early_stopping_patience', 15),
            min_delta=getattr(config.training, 'early_stopping_delta', 0.001)
        )
        self.checkpoint_manager = CheckpointManager(
            save_dir=config.experiment.checkpoint_dir,
            device=self.device
        )
        
        # Initialize model with new embedding pipeline
        self.model = self._create_enhanced_model()
        
        # GPU ìµœì í™”: ëª¨ë¸ì„ GPUë¡œ ì´ë™ ë° ìµœì í™”
        self.model.to(self.device)
        
        # ê·¼ë³¸ì  í•´ê²°: ëª¨ë“  ì„œë¸Œëª¨ë“ˆì„ ê°•ì œë¡œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self._ensure_all_modules_on_device()
            
        # AMP (Automatic Mixed Precision) ì„¤ì • - ë©”ëª¨ë¦¬ ë¬¸ì œë¡œ ì¸í•´ ë¹„í™œì„±í™”
        self.use_amp = False  # getattr(config.model, 'use_amp', True) and self.device.type == 'cuda'
        # ìµœì‹  PyTorch ë²„ì „ì— ë§ê²Œ GradScaler ì‚¬ìš©
        if self.use_amp:
            if hasattr(torch.amp, 'GradScaler'):
                self.scaler = torch.amp.GradScaler('cuda')
            else:
                # ì´ì „ ë²„ì „ í˜¸í™˜ì„± ìœ ì§€
                self.scaler = torch.cuda.amp.GradScaler()
        else:
            self.scaler = None
        
        print(f"ğŸš€ AMP Enabled: {self.use_amp}")
        
        # Initialize optimizer and scheduler
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # Initialize loss function
        self.criterion = self._create_loss_function()
        
        # Training state
        self.current_epoch = 0
        self.current_step = 0
        self.best_val_loss = float('inf')
        
        print(f"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def _ensure_all_modules_on_device(self):
        """ê·¼ë³¸ì  í•´ê²°: ëª¨ë“  ì„œë¸Œëª¨ë“ˆì„ ê°•ì œë¡œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        print(f"ğŸ”§ Ensuring all modules are on {self.device}")
        
        # ëª¨ë“  ì„œë¸Œëª¨ë“ˆ ìˆœíšŒí•˜ì—¬ ë””ë°”ì´ìŠ¤ ì´ë™
        for name, module in self.model.named_modules():
            if hasattr(module, 'to'):
                module.to(self.device)
                print(f"  âœ… Moved {name} to {self.device}")
        
        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ íŠ¹ë³„ ì²˜ë¦¬
        if hasattr(self.model, 'embedding_pipeline'):
            if hasattr(self.model.embedding_pipeline, 'unified_facade'):
                self.model.embedding_pipeline.unified_facade.to(self.device)
                print(f"  âœ… Moved embedding_pipeline.unified_facade to {self.device}")
            
            # ê¸°íƒ€ ì„ë² ë”© ê´€ë ¨ ëª¨ë“ˆë“¤
            for attr_name in ['grid_encoder', 'circuit_processor', 'batch_processor']:
                if hasattr(self.model.embedding_pipeline, attr_name):
                    attr_obj = getattr(self.model.embedding_pipeline, attr_name)
                    if hasattr(attr_obj, 'to'):
                        attr_obj.to(self.device)
                        print(f"  âœ… Moved embedding_pipeline.{attr_name} to {self.device}")
        
        print(f"ğŸ¯ All modules moved to {self.device}")
        
    def _create_enhanced_model(self):
        """Create model with enhanced embedding pipeline - SOTA í†µí•© ëª¨ë¸ ì§€ì›"""
        if self.model_type == "property_predictor":
            # Create unified model with size-based configuration
            from config.experiment_configs import MODEL_SIZES
            
            # ëª¨ë¸ í¬ê¸° ê²°ì • (ì„¤ì •ì—ì„œ ì§€ì •í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
            model_size = getattr(self.config.model, 'model_size', 'medium')
            if model_size not in MODEL_SIZES:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ í¬ê¸°: {model_size}, ê¸°ë³¸ê°’ 'medium' ì‚¬ìš©")
                model_size = 'medium'
            
            size_config = MODEL_SIZES[model_size]
            print(f"ğŸ“ ëª¨ë¸ í¬ê¸°: {model_size} - d_model={size_config['d_model']}, n_layers={size_config['n_layers']}")
            
            # í†µí•©ëœ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±
            config_params = {
                'd_model': size_config['d_model'],
                'n_heads': size_config['n_heads'], 
                'n_layers': size_config['n_layers'],
                'd_ff': size_config['d_ff'],
                'dropout': size_config['dropout'],
                'max_qubits': getattr(self.config.model, 'max_qubits', 10),
                'max_gates': getattr(self.config.model, 'max_gates', 100),
                'attention_mode': getattr(self.config.model, 'attention_mode', 'advanced'),
                'use_rotary_pe': getattr(self.config.model, 'use_rotary_pe', True),
                'cross_attention_heads': getattr(self.config.model, 'cross_attention_heads', 4),
                'consistency_loss_weight': getattr(self.config.model, 'consistency_loss_weight', 0.1),
                'gradient_clipping': getattr(self.config.model, 'gradient_clipping', 1.0),
                'numerical_stability': getattr(self.config.model, 'numerical_stability', True),
                'learning_rate': getattr(self.config.training, 'learning_rate', 1e-4),
                'train_batch_size': getattr(self.config.training, 'train_batch_size', size_config['batch_size']),
                'val_batch_size': getattr(self.config.training, 'val_batch_size', size_config['batch_size'] * 2)
            }
            
            integrated_config = UnifiedPropertyPredictionConfig(**config_params)
            model = UnifiedPropertyPredictionTransformer(integrated_config)
            print("ğŸš€ SOTA í†µí•© ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            if self.device.type == 'cuda':
                model.half() if getattr(self.config.model, 'use_fp16', False) else model.float()
            
            return model
        elif self.model_type == "decision_transformer":
            # Create Decision Transformer with enhanced pipeline
            from models.decision_transformer import DecisionTransformer
            
            model_config = self.config.get_model_config_for_decision_transformer()
            model_config['device'] = self.device.type  # GPU ì„¤ì • ì¶”ê°€
            model = DecisionTransformer(**model_config)
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            if self.device.type == 'cuda':
                model.half() if getattr(self.config.model, 'use_fp16', False) else model.float()
            
            return model
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
    
    def _create_optimizer(self):
        """Create optimizer - GPU ìµœì í™”"""
        # GPUì—ì„œ ë” íš¨ìœ¨ì ì¸ AdamW ì‚¬ìš©
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config.training.learning_rate,
            weight_decay=self.config.training.weight_decay,
            eps=1e-8,  # GPU ì•ˆì •ì„±
            amsgrad=True  # GPUì—ì„œ ë” ì•ˆì •ì 
        )
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        if self.device.type == 'cuda':
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ GPUë¡œ ì´ë™
            for state in optimizer.state.values():
                for k, v in state.items():
                    if isinstance(v, torch.Tensor):
                        state[k] = v.to(self.device)
        
        return optimizer
    
    def _create_scheduler(self):
        """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±"""
        if self.config.training.scheduler_type == "cosine":
            return CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.training.num_epochs,
                eta_min=self.config.training.learning_rate * 0.01
            )
        else:
            return None
    
    def _create_loss_function(self):
        """ì†ì‹¤ í•¨ìˆ˜ ìƒì„± - SOTA í†µí•© ëª¨ë¸ ì§€ì›"""
        if self.model_type == "decision_transformer":
            return nn.CrossEntropyLoss()
        elif self.model_type == "property_predictor":
            # SOTA ëª¨ë¸ì¸ ê²½ìš° ë‚´ì¥ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©, ì•„ë‹ˆë©´ MSE ì‚¬ìš©
            use_sota = getattr(self.config.model, 'use_sota_architecture', True)
            if use_sota:
                return None  # SOTA ëª¨ë¸ì€ ë‚´ì¥ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
            else:
                return nn.MSELoss()
        else:
            return nn.MSELoss()
    
    def _prepare_batch_data(self, batch):
        """ì–‘ìíšŒë¡œ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„"""
        
        if self.model_type == "property_predictor":
            circuit_specs = batch.get('circuit_specs', [])
            
            if not circuit_specs:
                raise ValueError("Empty circuit_specs provided - cannot create valid batch")
            
            # Extract target properties from batch['targets'] created by property_prediction collate_fn
            targets_dict = batch.get('targets')
            
            if targets_dict:
                # Convert dict format to tensor format [entanglement, expressibility, fidelity]
                batch_size = len(circuit_specs)
                target_properties = torch.zeros(batch_size, 3, device=self.device)
                
                # Extract values from targets dict
                if 'entanglement' not in targets_dict or 'expressibility' not in targets_dict or 'fidelity' not in targets_dict:
                    raise ValueError(f"Missing required target properties: {list(targets_dict.keys())}")
                
                entanglement_vals = targets_dict['entanglement']
                expressibility_vals = targets_dict['expressibility']
                fidelity_vals = targets_dict['fidelity']
                
                # Stack into [batch_size, 3] tensor
                target_properties[:, 0] = entanglement_vals.to(self.device)
                target_properties[:, 1] = expressibility_vals.to(self.device)
                target_properties[:, 2] = fidelity_vals.to(self.device)

            # ì–‘ìíšŒë¡œ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±
            circuit_data_list = []
            
            # ê° circuit_specì— ëŒ€í•´ ê·¸ë˜í”„ ë°ì´í„° ìƒì„± (ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™”)
            with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì  ë¹„í™œì„±í™”
                for spec in circuit_specs:
                    if hasattr(spec, 'gates') and hasattr(spec, 'num_qubits'):
                        # ì–‘ìíšŒë¡œ ê·¸ë˜í”„ ë¹Œë” ì‚¬ìš© (ë³¸ì§ˆ ë³´ì¡´)
                        graph_builder = self.encoding_factory.graph_builder
                        graph_data = graph_builder.build_graph_from_circuit_spec(spec)
                        
                        # ì¦‰ì‹œ GPUë¡œ ì´ë™í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
                        circuit_data = {
                            'node_features': graph_data.node_features.to(self.device, non_blocking=True),
                            'grid_positions': graph_data.grid_positions.to(self.device, non_blocking=True),
                            'node_types': graph_data.node_types.to(self.device, non_blocking=True),
                            'attention_mask': None
                        }
                        circuit_data_list.append(circuit_data)
                        
                        # ì¤‘ê°„ ê°ì²´ ì¦‰ì‹œ ì •ë¦¬
                        del graph_data
            
            # ë°°ì¹˜ í…ì„œë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  íŒ¨ë”©)
            if circuit_data_list:
                batch_circuit_data = self._create_padded_batch_efficient(circuit_data_list)
            else:
                raise ValueError("No valid circuit data found in batch")
            
            return {
                'circuit_data': batch_circuit_data,
                'target_properties': target_properties
            }
            
        elif self.model_type == "decision_transformer":
            # Decision Transformerìš© ë°ì´í„° ì¤€ë¹„ (í–¥í›„ êµ¬í˜„)
            return batch
        else:
            return batch
    
    def _create_padded_batch_efficient(self, circuit_data_list):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íŒ¨ë”© ë°°ì¹˜ ìƒì„±"""
        batch_size = len(circuit_data_list)
        
        # ìµœëŒ€ ë…¸ë“œ ìˆ˜ ê³„ì‚°
        max_nodes = max(cd['node_features'].shape[0] for cd in circuit_data_list)
        max_nodes = min(max_nodes, 500)  # ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ ì œí•œ
        
        # ë¯¸ë¦¬ í• ë‹¹ëœ ë°°ì¹˜ í…ì„œ
        batch_node_features = torch.zeros(batch_size, max_nodes, 12, device=self.device)
        batch_grid_positions = torch.zeros(batch_size, max_nodes, 2, device=self.device)
        batch_node_types = torch.zeros(batch_size, max_nodes, dtype=torch.long, device=self.device)
        batch_attention_masks = torch.zeros(batch_size, max_nodes, device=self.device)
        
        # ê° ìƒ˜í”Œì— ëŒ€í•´ íš¨ìœ¨ì ì¸ íŒ¨ë”©
        for i, cd in enumerate(circuit_data_list):
            num_nodes = cd['node_features'].shape[0]
            actual_nodes = min(num_nodes, max_nodes)
            
            # ì§ì ‘ ë³µì‚¬ (íš¨ìœ¨ì )
            batch_node_features[i, :actual_nodes] = cd['node_features'][:actual_nodes]
            batch_grid_positions[i, :actual_nodes] = cd['grid_positions'][:actual_nodes]
            batch_node_types[i, :actual_nodes] = cd['node_types'][:actual_nodes]
            
            # attention mask ì„¤ì •
            batch_attention_masks[i, :actual_nodes] = 1.0
            
            # ì¤‘ê°„ ë°ì´í„° ì •ë¦¬
            del cd['node_features']
            del cd['grid_positions'] 
            del cd['node_types']
        
        return {
            'node_features': batch_node_features,
            'grid_positions': batch_grid_positions,
            'node_types': batch_node_types,
            'attention_mask': batch_attention_masks
        }
    
    def train_epoch(self, train_loader):
        """í•œ ì—í¬í¬ í•™ìŠµ - ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì ê³¼ ë©”ëª¨ë¦¬ ìµœì í™”"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(train_loader)
        
        # ë©”ëª¨ë¦¬ ìµœì í™”: ê°€ë¹„ì§€ ì½œë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        
        # ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…)
        if hasattr(self, 'debug_memory') and self.debug_memory:
            reserved_mem = torch.cuda.memory_reserved(0) / 1024**3
            allocated_mem = torch.cuda.memory_allocated(0) / 1024**3
            print(f"\nğŸ’¾ í•™ìŠµ ì‹œì‘ ë©”ëª¨ë¦¬: {reserved_mem:.2f}GB (ì˜ˆì•½) | {allocated_mem:.2f}GB (í• ë‹¹)")
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì  ì„¤ì •
        grad_accum_steps = getattr(self, 'grad_accum_steps', 4)
        
        pbar = tqdm(train_loader, desc=f"Epoch {self.current_epoch}")
        
        self.optimizer.zero_grad(set_to_none=True)  # í•œë²ˆë§Œ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        accum_loss = 0.0  # ì¶•ì ëœ ì†ì‹¤
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                prepared_batch = self._prepare_batch_data(batch)
                
                # ë¦¬ì†ŒìŠ¤ ì œí•œì„ ìœ„í•´ ë°°ì¹˜ë¥¼ í•˜ìœ„ ë°°ì¹˜ë¡œ ë¶„í•  (ì¶”ê°€ ìµœì í™”)
                sub_batch_factor = getattr(self.config.training, 'sub_batch_factor', 1)
                if sub_batch_factor > 1:
                    # TODO: í•˜ìœ„ ë°°ì¹˜ ë¶„í•  ë¡œì§ ì¶”ê°€ (í•„ìš”ì‹œ)
                    pass
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë””ë²„ê¹… - ë°°ì¹˜ ì§„ì… ì „
                if hasattr(self, 'debug_memory') and self.debug_memory and batch_idx < 30:
                    torch.cuda.synchronize()
                    reserved_mem = torch.cuda.memory_reserved(0) / 1024**3
                    allocated_mem = torch.cuda.memory_allocated(0) / 1024**3
                    print(f"\nğŸ’¾ ë°°ì¹˜ {batch_idx+1} ì‹œì‘ ë©”ëª¨ë¦¬: {reserved_mem:.2f}GB (ì˜ˆì•½) | {allocated_mem:.2f}GB (í• ë‹¹)")
                    
                    # ë©”ëª¨ë¦¬ ê²½ê³„ì¹˜ ê°ì§€ ë° í´ë¦¬ì–´ ì‹œë„ - ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    if allocated_mem > 15.0:  # 20GB GPUì—ì„œ 15GBì´ìƒì´ë©´ ìœ„í—˜
                        print(f"\nâš ï¸ ë©”ëª¨ë¦¬ ê²½ê³„ì¹˜ ë„ë‹¬: {allocated_mem:.2f}GB/20GB - ìºì‹œ í´ë¦¬ì–´ ì‹œë„...")
                        import gc
                        gc.collect()
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                
                # Forward pass with memory management
                # ê·¸ë˜í”„ ì‘ì—… ì „ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
                if batch_idx % 10 == 0:
                    # 10ë°°ì¹˜ë§ˆë‹¤ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                    import gc
                    gc.collect()
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                # Forward pass with AMP
                if self.use_amp:
                    with torch.amp.autocast(device_type='cuda'):
                        if self.model_type == "property_predictor":
                            loss = self._train_step_property_predictor(prepared_batch)
                        elif self.model_type == "decision_transformer":
                            loss = self._train_step_decision_transformer(prepared_batch)
                        else:
                            continue
                else:
                    if self.model_type == "property_predictor":
                        loss = self._train_step_property_predictor(prepared_batch)
                    elif self.model_type == "decision_transformer":
                        loss = self._train_step_decision_transformer(prepared_batch)
                    else:
                        continue
                        
                # ë©”ëª¨ë¦¬ ë¬¸ì œ ê°ì§€ (forward í›„ ì²´í¬)
                if hasattr(self, 'debug_memory') and self.debug_memory and batch_idx < 30:
                    torch.cuda.synchronize()
                    allocated_mem = torch.cuda.memory_allocated(0) / 1024**3
                    if allocated_mem > 17.0:  # 20GB GPUì—ì„œ 17GB ì´ìƒì´ë©´ ìœ„í—˜
                        print(f"\nâš ï¸ Forward í›„ ë©”ëª¨ë¦¬ ê²½ê³„ì¹˜ ì ‘ê·¼: {allocated_mem:.2f}GB/20GB")
                
                # Loss ìœ íš¨ì„± ê²€ì‚¬
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"Warning: Invalid loss detected at batch {batch_idx}, skipping...")
                    continue
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì ì„ ìœ„í•´ lossë¥¼ ì¶•ì  ìŠ¤í… ìˆ˜ë¡œ ë‚˜ëˆ”
                scaled_loss = loss / grad_accum_steps
                
                # Backward pass with gradient accumulation
                if self.use_amp:
                    # AMPë¥¼ ì‚¬ìš©í•œ backward pass
                    self.scaler.scale(scaled_loss).backward()
                    accum_loss += loss.item()  # ì›ë˜ loss ê°’ ì¶”ì 
                    
                    # ë©”ëª¨ë¦¬ ì²´í¬ - backward í›„
                    if hasattr(self, 'debug_memory') and self.debug_memory and batch_idx < 30:
                        torch.cuda.synchronize()  # ë¹„ë™ê¸° ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
                        allocated_mem = torch.cuda.memory_allocated(0) / 1024**3
                        if allocated_mem > 17.5:  # ê²½ê³„ ìƒíƒœ í™•ì¸
                            print(f"\nâš ï¸ Backward í›„ ë©”ëª¨ë¦¬ ìƒíƒœ: {allocated_mem:.2f}GB/20GB")
                    
                    # ì¶”ì  ë‹¨ê³„ê°€ ë‹¤ ì°¨ê±°ë‚˜ ë§ˆì§€ë§‰ ë°°ì¹˜ì¼ ê²½ìš°
                    if (batch_idx + 1) % grad_accum_steps == 0 or (batch_idx + 1) == num_batches:
                        # AMPì—ì„œëŠ” í•­ìƒ unscale_ì„ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•¨
                        self.scaler.unscale_(self.optimizer)
                        
                        # Gradient clipping
                        if self.config.training.gradient_clip_norm > 0:
                            torch.nn.utils.clip_grad_norm_(
                                self.model.parameters(), 
                                self.config.training.gradient_clip_norm
                            )
                        
                        # ì—°ì‚° ì¶”ì  ë° ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                        self.optimizer.zero_grad(set_to_none=True)  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì´ˆê¸°í™”
                        
                        # ë””ë²„ê¹… ë©”ëª¨ë¦¬ ì •ë³´ - ì˜µí‹°ë§ˆì´ì € ìŠ¤í… í›„
                        if hasattr(self, 'debug_memory') and self.debug_memory:
                            torch.cuda.synchronize()  # ë¹„ë™ê¸° ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
                            reserved_mem = torch.cuda.memory_reserved(0) / 1024**3
                            allocated_mem = torch.cuda.memory_allocated(0) / 1024**3
                            print(f"\nğŸ’¾ ë°°ì¹˜ {batch_idx+1} ì—…ë°ì´íŠ¸ í›„ ë©”ëª¨ë¦¬: {reserved_mem:.2f}GB (ì˜ˆì•½) | {allocated_mem:.2f}GB (í• ë‹¹)")
                            
                            # ë©”ëª¨ë¦¬ ìƒíƒœê°€ ìœ„í—˜ ìˆ˜ì¤€ì´ë©´ ê°•ì œ ì •ë¦¬
                            if allocated_mem > 16.0 or batch_idx % 10 == 0:
                                print(f"\nğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„ (ë°°ì¹˜ {batch_idx+1})")
                                import gc
                                gc.collect()
                                torch.cuda.empty_cache()
                                torch.cuda.synchronize()  # ì •ë¦¬ ì™„ë£Œ ëŒ€ê¸°
                                
                                # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                                new_mem = torch.cuda.memory_allocated(0) / 1024**3
                                reduction = allocated_mem - new_mem
                                print(f"   â†’ ë©”ëª¨ë¦¬ {reduction:.2f}GB ì •ë¦¬ë¨ (í˜„ì¬: {new_mem:.2f}GB)")
                else:
                    # ì¼ë°˜ backward pass
                    scaled_loss.backward()
                    accum_loss += loss.item()
                    
                    # ì¶”ì  ë‹¨ê³„ê°€ ë‹¤ ì°¨ê±°ë‚˜ ë§ˆì§€ë§‰ ë°°ì¹˜ì¼ ê²½ìš°
                    if (batch_idx + 1) % grad_accum_steps == 0 or (batch_idx + 1) == num_batches:
                        # Gradient clipping
                        if self.config.training.gradient_clip_norm > 0:
                            torch.nn.utils.clip_grad_norm_(
                                self.model.parameters(), 
                                self.config.training.gradient_clip_norm
                            )
                        
                        self.optimizer.step()
                        self.optimizer.zero_grad(set_to_none=True)
                
                # Update metrics - loss.item()ìœ¼ë¡œ ì‹¤ì œ ê°’ ì¶”ì¶œ
                current_loss = loss.item()
                total_loss += current_loss
                self.current_step += 1
                
                # Update progress bar
                pbar.set_postfix({
                    'loss': f'{current_loss:.4f}',
                    'avg_loss': f'{total_loss / (batch_idx + 1):.4f}'
                })
                
                # ë©”ëª¨ë¦¬ ìµœì í™”: ë” ìì£¼ ìºì‹œ ì •ë¦¬ (CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°©ì§€)
                if batch_idx % 5 == 0:  # 10ì—ì„œ 5ë¡œ ë³€ê²½í•˜ì—¬ ë” ìì£¼ ì •ë¦¬
                    import gc
                    torch.cuda.empty_cache()
                    gc.collect()
                    torch.cuda.synchronize()  # ë™ê¸°í™” ì¶”ê°€
                
                # ë°°ì¹˜ ì™„ë£Œ í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
                del prepared_batch
                if 'circuit_data' in locals():
                    del circuit_data
                if 'target_properties' in locals():
                    del target_properties
                torch.cuda.empty_cache()
                    
            except RuntimeError as e:
                if "out of memory" in str(e) or "CUDA" in str(e):
                    print(f"\nCUDA memory error at batch {batch_idx}: {e}")
                    print("Clearing cache and continuing...")
                    torch.cuda.empty_cache()
                    gc.collect()
                    continue
                else:
                    raise e
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        gc.collect()
        
        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss
    
    def _train_step_property_predictor(self, prepared_batch):
        """Property Predictor í•™ìŠµ ìŠ¤í… - SOTA í†µí•© ëª¨ë¸ ì§€ì›"""
        circuit_data = prepared_batch['circuit_data']
        target_properties = prepared_batch['target_properties']
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        for key, value in circuit_data.items():
            if isinstance(value, torch.Tensor):
                circuit_data[key] = value.to(self.device)
        
        if target_properties is not None:
            target_properties = target_properties.to(self.device)
        
        # SOTA í†µí•© ëª¨ë¸ ì—¬ë¶€ í™•ì¸
        use_sota = getattr(self.config.model, 'use_sota_architecture', True)
        
        if use_sota and isinstance(self.model, UnifiedPropertyPredictionTransformer):
            # SOTA í†µí•© ëª¨ë¸ ì‚¬ìš©
            outputs = self.model(circuit_data)
            
            # íƒ€ê²Ÿì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            if target_properties is not None:
                targets_dict = {
                    'entanglement': target_properties[:, 0],
                    'fidelity': target_properties[:, 2],  # fidelityê°€ 2ë²ˆì§¸ ì¸ë±ìŠ¤
                    'expressibility': target_properties[:, 1]  # expressibilityê°€ 1ë²ˆì§¸ ì¸ë±ìŠ¤
                }
                
                # SOTA ëª¨ë¸ì˜ ë‚´ì¥ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
                losses = self.model.compute_loss(outputs, targets_dict)
                
                # ì†ì‹¤ ì»´í¬ë„ŒíŠ¸ ì €ì¥
                self._last_loss_components = {
                    'entanglement_loss': losses.get('entanglement', torch.tensor(0.0)).item(),
                    'expressibility_loss': losses.get('expressibility', torch.tensor(0.0)).item(),
                    'fidelity_loss': losses.get('fidelity', torch.tensor(0.0)).item(),
                    'consistency_loss': losses.get('consistency', torch.tensor(0.0)).item(),
                    'total_loss': losses['total'].item()
                }
                
                return losses['total']
            else:
                return torch.tensor(0.0, device=self.device, requires_grad=True)
        else:
            # ê¸°ì¡´ ëª¨ë¸ ë¡œì§
            outputs = self.model(circuit_data, mode='unified')
        
        # # ë””ë²„ê¹…: ëª¨ë¸ ì¶œë ¥ í‚¤ í™•ì¸
        # print(f"Model output keys: {list(outputs.keys())}")
        # if 'unified' in outputs:
        #     print(f"Unified keys: {list(outputs['unified'].keys())}")
        
        # ì†ì‹¤ ê³„ì‚° - ê°œë³„ propertyë³„ ê°€ì¤‘ ì†ì‹¤
        if 'unified' in outputs:
            entanglement_pred = outputs['unified']['entanglement']
            expressibility_pred = outputs['unified']['expressibility'] 
            fidelity_pred = outputs['unified']['fidelity']
        else:
            # Property predictorëŠ” ì§ì ‘ property í‚¤ë¥¼ ë°˜í™˜
            entanglement_pred = outputs.get('entanglement', torch.zeros(1, 1, device=self.device))
            expressibility_pred = outputs.get('expressibility', torch.zeros(1, 1, device=self.device))
            fidelity_pred = outputs.get('fidelity', torch.zeros(1, 1, device=self.device))
        if target_properties is not None:
            # NaN ê²€ì‚¬ ë° ë””ë²„ê¹…
            has_nan = (torch.isnan(entanglement_pred).any() or torch.isnan(expressibility_pred).any() or 
                      torch.isnan(fidelity_pred).any() or torch.isnan(target_properties).any())
            
            if has_nan:
                print(f"\nâš ï¸ NaN ê°ì§€! ì˜ˆì¸¡ê³¼ íƒ€ê²Ÿ ë””ë²„ê¹…:")
                print(f"  entanglement_pred NaN: {torch.isnan(entanglement_pred).any()}")
                print(f"  expressibility_pred NaN: {torch.isnan(expressibility_pred).any()}")
                print(f"  fidelity_pred NaN: {torch.isnan(fidelity_pred).any()}")
                print(f"  target_properties NaN: {torch.isnan(target_properties).any()}")
                print(f"  entanglement_pred range: {entanglement_pred.min().item():.6f} - {entanglement_pred.max().item():.6f}")
                print(f"  expressibility_pred range: {expressibility_pred.min().item():.6f} - {expressibility_pred.max().item():.6f}")
                print(f"  fidelity_pred range: {fidelity_pred.min().item():.6f} - {fidelity_pred.max().item():.6f}")
                
                # NaN ë°œìƒ ì‹œ í›ˆë ¨ ì¤‘ë‹¨
                raise RuntimeError("NaN detected in predictions or targets - training cannot continue")
            
            # íƒ€ê²Ÿì„ ê°œë³„ propertyë¡œ ë¶„í•  (entanglement, expressibility, fidelity ìˆœì„œ)
            target_entanglement = target_properties[:, 0:1]
            target_expressibility = target_properties[:, 1:2] 
            target_fidelity = target_properties[:, 2:3]
            
            # ê°œë³„ property ì†ì‹¤ ê³„ì‚°
            entanglement_loss = self.loss_fn(entanglement_pred, target_entanglement)
            expressibility_loss = self.loss_fn(expressibility_pred, target_expressibility)
            fidelity_loss = self.loss_fn(fidelity_pred, target_fidelity)
            
            # ê°€ì¤‘ì¹˜ ì ìš© (configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            entanglement_weight = getattr(self.config.training, 'entanglement_weight', 10.0)
            expressibility_weight = getattr(self.config.training, 'expressibility_weight', 1.0)
            fidelity_weight = getattr(self.config.training, 'fidelity_weight', 0.1)
            
            # ê°€ì¤‘ ì†ì‹¤ ê³„ì‚°
            weighted_entanglement_loss = entanglement_weight * entanglement_loss
            weighted_expressibility_loss = expressibility_weight * expressibility_loss
            weighted_fidelity_loss = fidelity_weight * fidelity_loss
            
            # ì´ ì†ì‹¤
            total_loss = weighted_entanglement_loss + weighted_expressibility_loss + weighted_fidelity_loss
            #print(f"total_loss: {total_loss}")
            # ë¡œìŠ¤ ì»´í¬ë„ŒíŠ¸ ì €ì¥ (wandb ë¡œê¹…ìš©)
            self._last_loss_components = {
                'entanglement_loss': entanglement_loss.item(),
                'expressibility_loss': expressibility_loss.item(),
                'fidelity_loss': fidelity_loss.item(),
                'weighted_entanglement_loss': weighted_entanglement_loss.item(),
                'weighted_expressibility_loss': weighted_expressibility_loss.item(),
                'weighted_fidelity_loss': weighted_fidelity_loss.item()
            }
            
            # wandb ë¡œê¹… ë° ë””ë²„ê¹… ì •ë³´
            if hasattr(self, 'current_step'):
                # ì£¼ê¸°ì ìœ¼ë¡œ íƒ€ê²Ÿ ê°’ ë²”ìœ„ ë¡œê¹… (100ìŠ¤í…ë§ˆë‹¤)
                if self.current_step % 100 == 0:
                    wandb.log({
                        'debug/target_entanglement_mean': target_entanglement.mean().item(),
                        'debug/target_entanglement_std': target_entanglement.std().item(),
                        'debug/target_expressibility_mean': target_expressibility.mean().item(),
                        'debug/target_expressibility_std': target_expressibility.std().item(),
                        'debug/target_fidelity_mean': target_fidelity.mean().item(),
                        'debug/target_fidelity_std': target_fidelity.std().item(),
                        'debug/pred_entanglement_mean': entanglement_pred.mean().item(),
                        'debug/pred_expressibility_mean': expressibility_pred.mean().item(),
                        'debug/pred_fidelity_mean': fidelity_pred.mean().item(),
                        'step': self.current_step
                    })
                    
                # 50ë°°ì¹˜ë§ˆë‹¤ MAE ë¡œê¹…
                if self.current_step % 50 == 0:
                    # MAE ê³„ì‚° (Mean Absolute Error)
                    entanglement_mae = torch.nn.functional.l1_loss(entanglement_pred, target_entanglement).item()
                wandb.log({
                    'train/entanglement_loss': entanglement_loss.item(),
                    'train/expressibility_loss': expressibility_loss.item(),
                    'train/fidelity_loss': fidelity_loss.item(),
                    'train/weighted_entanglement_loss': weighted_entanglement_loss.item(),
                    'train/weighted_expressibility_loss': weighted_expressibility_loss.item(),
                    'train/weighted_fidelity_loss': weighted_fidelity_loss.item(),
                    'train/total_weighted_loss': total_loss.item(),
                    'train/entanglement_weight': entanglement_weight,
                    'train/expressibility_weight': expressibility_weight,
                    'train/fidelity_weight': fidelity_weight,
                    'step': self.current_step
                })

            loss = total_loss

            # ì†ì‹¤ ê°’ ê²€ì¦
            if torch.isnan(loss) or torch.isinf(loss):
                raise RuntimeError(f"Invalid loss detected: {loss.item()}")
            return loss
        else:
            print("No unified output")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def validate(self, val_loader):
        """ê²€ì¦ ìŠ¤í… ìˆ˜í–‰"""
        self.model.eval()
        total_losses = {'total_loss': 0.0, 'entanglement_loss': 0.0, 'expressibility_loss': 0.0, 'fidelity_loss': 0.0,
                      'weighted_entanglement_loss': 0.0, 'weighted_expressibility_loss': 0.0, 'weighted_fidelity_loss': 0.0}
        val_predictions = []
        num_batches = len(val_loader)
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                prepared_batch = self._prepare_batch_data(batch)
                
                if self.model_type == "property_predictor":
                    losses = self._val_step_property_predictor(prepared_batch)
                    
                    # ì˜ˆì¸¡ê°’ê³¼ íƒ€ê²Ÿê°’ ì €ì¥ (MAE ê³„ì‚°ìš©)
                    if 'entanglement_pred' in losses and 'entanglement_target' in losses:
                        val_predictions.append({
                            'entanglement_pred': losses['entanglement_pred'].detach(),
                            'expressibility_pred': losses['expressibility_pred'].detach(),
                            'fidelity_pred': losses['fidelity_pred'].detach(),
                            'entanglement_target': losses['entanglement_target'].detach(),
                            'expressibility_target': losses['expressibility_target'].detach(),
                            'fidelity_target': losses['fidelity_target'].detach()
                        })
                    
                    # ì†ì‹¤ê°’ ëˆ„ì 
                    for key, value in losses.items():
                        if key in total_losses:
                            if isinstance(value, torch.Tensor):
                                total_losses[key] += value.item()
                            elif isinstance(value, (int, float)):
                                total_losses[key] += value
                            
                elif self.model_type == "decision_transformer":
                    loss = self._train_step_decision_transformer(prepared_batch)
                    total_losses['total_loss'] += loss.item()
                else:
                    continue
        
        # í‰ê·  ê³„ì‚° ë° wandb ë¡œê¹…
        avg_losses = {key: value / max(num_batches, 1) for key, value in total_losses.items()}
        
        # MAE ê³„ì‚°ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ëˆ„ì  (validation ì¤‘ì— ìˆ˜ì§‘)
        all_entanglement_pred = torch.cat([item['entanglement_pred'] for item in val_predictions]) if val_predictions else torch.tensor([])
        all_expressibility_pred = torch.cat([item['expressibility_pred'] for item in val_predictions]) if val_predictions else torch.tensor([])
        all_fidelity_pred = torch.cat([item['fidelity_pred'] for item in val_predictions]) if val_predictions else torch.tensor([])
        
        all_entanglement_target = torch.cat([item['entanglement_target'] for item in val_predictions]) if val_predictions else torch.tensor([])
        all_expressibility_target = torch.cat([item['expressibility_target'] for item in val_predictions]) if val_predictions else torch.tensor([])
        all_fidelity_target = torch.cat([item['fidelity_target'] for item in val_predictions]) if val_predictions else torch.tensor([])
        
        # Validation MAE ê³„ì‚°
        val_mae = {}
        if len(all_entanglement_pred) > 0:
            val_mae['entanglement'] = self.loss_fn(all_entanglement_pred, all_entanglement_target).item()
            val_mae['expressibility'] = self.loss_fn(all_expressibility_pred, all_expressibility_target).item()
            val_mae['fidelity'] = self.loss_fn(all_fidelity_pred, all_fidelity_target).item()
            val_mae['total'] = (val_mae['entanglement'] + val_mae['expressibility'] + val_mae['fidelity']) / 3.0
        
        # ì½˜ì†” ì¶œë ¥ - íŠ¸ë ˆì´ë‹ê³¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ìƒì„¸ ë¡œê¹…
        print(f"\nğŸ“Š Validation Results (Epoch {self.current_epoch}):")
        print(f"   Total Loss: {avg_losses['total_loss']:.6f}")
        print(f"   Entanglement Loss: {avg_losses.get('entanglement_loss', 0):.6f}")
        print(f"   Expressibility Loss: {avg_losses.get('expressibility_loss', 0):.6f}")
        print(f"   Fidelity Loss: {avg_losses.get('fidelity_loss', 0):.6f}")
        print(f"   Weighted Entanglement Loss: {avg_losses.get('weighted_entanglement_loss', 0):.6f}")
        print(f"   Weighted Expressibility Loss: {avg_losses.get('weighted_expressibility_loss', 0):.6f}")
        print(f"   Weighted Fidelity Loss: {avg_losses.get('weighted_fidelity_loss', 0):.6f}")
        
        if val_mae:
            print(f"   MAE - Entanglement: {val_mae['entanglement']:.6f}")
            print(f"   MAE - Expressibility: {val_mae['expressibility']:.6f}")
            print(f"   MAE - Fidelity: {val_mae['fidelity']:.6f}")
            print(f"   MAE - Total: {val_mae['total']:.6f}")
        
        # wandb ê²€ì¦ ë¡œê¹…
        wandb_log_data = {
            'val/total_loss': avg_losses['total_loss'],
            'val/entanglement_loss': avg_losses.get('entanglement_loss', 0),
            'val/expressibility_loss': avg_losses.get('expressibility_loss', 0),
            'val/fidelity_loss': avg_losses.get('fidelity_loss', 0),
            'val/weighted_entanglement_loss': avg_losses.get('weighted_entanglement_loss', 0),
            'val/weighted_expressibility_loss': avg_losses.get('weighted_expressibility_loss', 0),
            'val/weighted_fidelity_loss': avg_losses.get('weighted_fidelity_loss', 0),
            'epoch': self.current_epoch
        }
        
        # MAE ë¡œê¹… ì¶”ê°€
        if val_mae:
            wandb_log_data.update({
                'val_mae/entanglement': val_mae['entanglement'],
                'val_mae/expressibility': val_mae['expressibility'],
                'val_mae/fidelity': val_mae['fidelity'],
                'val_mae/total': val_mae['total']
            })
        
        wandb.log(wandb_log_data)
        
        return avg_losses['total_loss']
    
    def train_decision_transformer(self, train_loader, val_loader, rtg_calculator):
        """Decision Transformer í•™ìŠµ ë©”ì„œë“œ - RTG ê°€ì´ë“œ í¬í•¨"""
        print(f"\nğŸš€ Starting Decision Transformer Training")
        print(f"   Epochs: {self.config.training.num_epochs}")
        print(f"   Learning Rate: {self.config.training.learning_rate}")
        print(f"   Batch Size: {self.config.training.train_batch_size}")
        
        for epoch in range(self.config.training.num_epochs):
            self.current_epoch = epoch
            
            print(f"\nğŸ“ˆ Epoch {epoch + 1}/{self.config.training.num_epochs}")
            
            # Training phase
            train_loss = self._train_epoch_decision_transformer(train_loader, rtg_calculator)
            
            # Validation phase
            val_loss = self._validate_epoch_decision_transformer(val_loader, rtg_calculator)
            
            # Scheduler step
            if self.scheduler:
                self.scheduler.step()
            
            # Early stopping check
            if self.early_stopping(val_loss):
                print(f"Early stopping triggered at epoch {epoch + 1}")
                break
            
            # Save best model
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                best_model_path = Path(self.config.experiment.checkpoint_dir) / "best_decision_transformer.pt"
                self.save_checkpoint(best_model_path)
                print(f"ğŸ’¾ New best model saved: {val_loss:.6f}")
            
            # Log to wandb
            wandb.log({
                'epoch': epoch,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'learning_rate': self.optimizer.param_groups[0]['lr']
            })
        
        print(f"\nâœ… Decision Transformer training completed!")
        print(f"   Best validation loss: {self.best_val_loss:.6f}")
    
    def _train_epoch_decision_transformer(self, train_loader, rtg_calculator):
        """Decision Transformer í•™ìŠµ ì—í¬í¬"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(train_loader)
        
        pbar = tqdm(train_loader, desc=f"Training Epoch {self.current_epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # ë°°ì¹˜ê°€ ì´ë¯¸ ì½œë ˆì´í„°ì— ì˜í•´ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ RTGë„ í¬í•¨ë˜ì–´ ìˆìŒ
                if isinstance(batch, dict) and 'rtg_rewards' in batch:
                    prepared_batch = batch
                else:
                    # RTG ê³„ì‚° í•„ìˆ˜
                    if rtg_calculator is None:
                        raise ValueError("RTG calculator required for Decision Transformer training")
                    rtg_values = self._calculate_batch_rtg(batch, rtg_calculator)
                    prepared_batch = self._prepare_decision_transformer_batch(batch, rtg_values)
                
                # ê·¼ë³¸ì  í•´ê²° í›„ ë¶ˆí•„ìš”í•œ ìˆ˜ë™ ë””ë°”ì´ìŠ¤ ì´ë™ ì œê±°
                # (ëª¨ë“  ëª¨ë“ˆì´ ì´ë¯¸ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ìˆìŒ)
                
                # Forward pass
                outputs = self.model(
                    input_sequence=prepared_batch['input_sequence'],
                    attention_mask=prepared_batch['attention_mask'],
                    action_prediction_mask=prepared_batch['action_prediction_mask'],
                    rtg_rewards=prepared_batch['rtg_rewards']
                )
                print(prepared_batch)
                # Loss ê³„ì‚°
                loss_dict = self.model.compute_loss(
                    predictions=outputs,
                    targets=prepared_batch['targets'],
                    action_prediction_mask=prepared_batch['action_prediction_mask']
                )
                
                loss = loss_dict['total_loss']
                
                # Backward pass
                loss.backward()
                
                if self.config.training.gradient_clip_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config.training.gradient_clip_norm
                    )
                
                # Optimizer step
                self.optimizer.step()
                self.optimizer.zero_grad()
                
                # Update metrics
                total_loss += loss.item()
                self.current_step += 1
                
                # Update progress bar
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{total_loss / (batch_idx + 1):.4f}'
                })
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"CUDA OOM at batch {batch_idx}, skipping...")
                    torch.cuda.empty_cache()
                    continue
                else:
                    raise e
        
        return total_loss / max(num_batches, 1)
    
    def _validate_epoch_decision_transformer(self, val_loader, rtg_calculator):
        """Decision Transformer ê²€ì¦ ì—í¬í¬"""
        self.model.eval()
        total_loss = 0.0
        num_batches = len(val_loader)
        
        with torch.no_grad():
            pbar_val = tqdm(val_loader, desc=f"Validation Epoch {self.current_epoch}")
            for batch_idx, batch in enumerate(pbar_val):
                try:
                    # ë°°ì¹˜ê°€ ì´ë¯¸ ì½œë ˆì´í„°ì— ì˜í•´ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ RTGë„ í¬í•¨ë˜ì–´ ìˆìŒ
                    if isinstance(batch, dict) and 'rtg_rewards' in batch:
                        prepared_batch = batch
                    else:
                        # RTG ê³„ì‚° í•„ìˆ˜
                        if rtg_calculator is None:
                            raise ValueError("RTG calculator required for Decision Transformer validation")
                        rtg_values = self._calculate_batch_rtg(batch, rtg_calculator)
                        prepared_batch = self._prepare_decision_transformer_batch(batch, rtg_values)
                    
                    # Forward pass
                    outputs = self.model(
                        input_sequence=prepared_batch['input_sequence'],
                        attention_mask=prepared_batch['attention_mask'],
                        action_prediction_mask=prepared_batch['action_prediction_mask'],
                        rtg_rewards=prepared_batch['rtg_rewards']
                    )
                    
                    # Loss ê³„ì‚°
                    loss_dict = self.model.compute_loss(
                        predictions=outputs,
                        targets=prepared_batch['targets'],
                        action_prediction_mask=prepared_batch['action_prediction_mask']
                    )
                    
                    loss = loss_dict['total_loss']
                    total_loss += loss.item()
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        torch.cuda.empty_cache()
                        continue
                    else:
                        raise e
        
        return total_loss / max(num_batches, 1)
    
    def _calculate_batch_rtg(self, batch, rtg_calculator):
        """ë°°ì¹˜ì— ëŒ€í•œ RTG ê°’ ê³„ì‚°"""
        # ë°°ì¹˜ê°€ ì´ë¯¸ ì½œë ˆì´í„°ì— ì˜í•´ ì²˜ë¦¬ëœ ê²½ìš°, RTGëŠ” ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆìŒ
        if isinstance(batch, dict) and 'rtg_rewards' in batch:
            return batch['rtg_rewards']
        
        # ì›ì‹œ CircuitData ê°ì²´ë“¤ì¸ ê²½ìš° (ì´ ê²½ìš°ëŠ” ë°œìƒí•˜ì§€ ì•Šì„ ê²ƒì„)
        rtg_values_list = []
        
        for circuit_data in batch:
            # CircuitDataì—ì„œ CircuitSpec ì¶”ì¶œ
            circuit_spec = circuit_data.circuit_spec if hasattr(circuit_data, 'circuit_spec') else circuit_data
            
            # íƒ€ê²Ÿ ì†ì„± ì¶”ì¶œ
            target_properties = {
                'entanglement': getattr(circuit_spec, 'target_entanglement', 0.8),
                'fidelity': getattr(circuit_spec, 'target_fidelity', 0.9),
                'expressibility': getattr(circuit_spec, 'target_expressibility', 0.7)
            }
            
            # RTG ê³„ì‚° (ë‹¨ì¼ íšŒë¡œì— ëŒ€í•´)
            rtg_values = rtg_calculator.calculate_rtg_sequence([circuit_spec], target_properties)
            rtg_values_list.append(rtg_values)
        
        return rtg_values_list
    
    def _prepare_decision_transformer_batch(self, batch, rtg_values_list):
        """Decision Transformerìš© ë°°ì¹˜ ì¤€ë¹„ - ëª¨ë¸ ë‚´ì¥ SAR ë¡œì§ í™œìš©"""
        debug_log("Preparing DT batch - delegating SAR creation to model")
        
        batch_size = len(batch)
        
        # 1. ì…ë ¥ ì‹œí€€ìŠ¤ íŒ¨ë”© (ëª¨ë¸ì´ SAR ë³€í™˜ ì²˜ë¦¬)
        input_sequences = [item['decision_transformer']['embeddings'] for item in batch]
        max_seq_len = max(seq.shape[1] for seq in input_sequences)
        embed_dim = input_sequences[0].shape[2]
        
        padded_input = torch.zeros(batch_size, max_seq_len, embed_dim, device=self.device)
        for i, seq in enumerate(input_sequences):
            seq_len = seq.shape[1]
            padded_input[i, :seq_len] = seq.squeeze(0).to(self.device)
        
        # 2. Attention mask íŒ¨ë”©
        attention_masks = [item['decision_transformer']['attention_mask'] for item in batch]
        padded_attention = torch.zeros(batch_size, max_seq_len, max_seq_len, dtype=torch.bool, device=self.device)
        for i, mask in enumerate(attention_masks):
            seq_len = mask.shape[1]
            padded_attention[i, :seq_len, :seq_len] = mask.squeeze(0).to(self.device)
        
        # 3. Action prediction mask íŒ¨ë”©
        action_masks = [item['decision_transformer']['action_prediction_mask'] for item in batch]
        padded_action_mask = torch.zeros(batch_size, max_seq_len, dtype=torch.bool, device=self.device)
        for i, mask in enumerate(action_masks):
            seq_len = mask.shape[1]
            padded_action_mask[i, :seq_len] = mask.squeeze(0).to(self.device)
        
        # 4. RTG íŒ¨ë”©
        padded_rtg = torch.zeros(batch_size, max_seq_len, device=self.device)
        for i, rtg in enumerate(rtg_values_list):
            if isinstance(rtg, torch.Tensor):
                rtg = rtg.to(self.device)
                rtg_len = min(rtg.shape[0], max_seq_len)
                padded_rtg[i, :rtg_len] = rtg[:rtg_len]
            else:
                padded_rtg[i, :] = float(rtg)
        
        # 5. Targets íŒ¨ë”©
        targets = []
        for item in batch:
            if 'decision_transformer' in item and 'targets' in item['decision_transformer']:
                targets.append(item['decision_transformer']['targets'])
            else:
                raise ValueError("Missing targets in decision transformer batch item")
        
        target_dim = targets[0].shape[2] if targets else 3
        padded_targets = torch.zeros(batch_size, max_seq_len, target_dim, device=self.device)
        for i, target in enumerate(targets):
            target = target.to(self.device)
            seq_len = min(target.shape[1], max_seq_len)
            padded_targets[i, :seq_len] = target.squeeze(0)[:seq_len]
        
        debug_log(f"DT batch - input: {padded_input.shape}, rtg: {padded_rtg.shape}, targets: {padded_targets.shape}")
        
        return {
            'input_sequence': padded_input,
            'attention_mask': padded_attention,
            'action_prediction_mask': padded_action_mask,
            'rtg_rewards': padded_rtg,
            'targets': padded_targets
        }
    
    def save_checkpoint(self, filepath):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        # PropertyConfig í˜¸í™˜ ì„¤ì •ë§Œ ì €ì¥
        property_config = {}
        if hasattr(self.config, 'model') and self.config.model:
            # PropertyConfig í•„ë“œë§Œ ì¶”ì¶œ
            property_fields = [
                'device', 'd_model', 'n_heads', 'n_layers', 'd_ff', 'dropout',
                'attention_mode', 'use_rotary_pe', 'cross_attention_heads',
                'property_dim', 'max_qubits', 'max_gates', 'learning_rate',
                'min_learning_rate', 'train_batch_size', 'val_batch_size',
                'grad_accum_steps', 'warmup_steps', 'weight_decay',
                'consistency_loss_weight', 'numerical_stability', 'gradient_clipping'
            ]
            
            for field in property_fields:
                if hasattr(self.config.model, field):
                    property_config[field] = getattr(self.config.model, field)
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'current_epoch': self.current_epoch,
            'current_step': self.current_step,
            'best_val_loss': self.best_val_loss,
            'config': property_config,  # PropertyConfig í˜¸í™˜ ì„¤ì •ë§Œ ì €ì¥
            'model_type': self.model_type
        }
        
        torch.save(checkpoint, filepath)
        print(f"Checkpoint saved to {filepath}")
    
    def train(self, train_loader, val_loader=None):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„ - property_predictorëŠ” ëª¨ë“ˆí™”ëœ íŠ¸ë ˆì´ë„ˆ ì‚¬ìš©"""
        if self.model_type == "property_predictor":
            # Property PredictorëŠ” ëª¨ë“ˆí™”ëœ íŠ¸ë ˆì´ë„ˆì— ì™„ì „ ìœ„ì„
            from training.property_prediction_trainer import create_trainer
            
            print(f"ğŸ¯ Starting modular training for {self.config.training.num_epochs} epochs")
            print(f"ğŸ“Š Model Type: SOTA í†µí•© ëª¨ë¸")
            
            # ëª¨ë“ˆí™”ëœ íŠ¸ë ˆì´ë„ˆ ìƒì„±
            trainer = create_trainer(
                model=self.model,
                config=self.config,
                device=str(self.device),
                use_wandb=True
            )
            
            # í•™ìŠµ ì‹¤í–‰ (ì™„ì „íˆ íŠ¸ë ˆì´ë„ˆì—ê²Œ ìœ„ì„)
            results = trainer.train(
                train_loader=train_loader,
                val_loader=val_loader,
                num_epochs=self.config.training.num_epochs
            )
            
            print("ğŸ‰ Modular training completed!")
            return results

    def train_step(self, batch):
        """Single training step with enhanced pipeline - GPU ìµœì í™”"""
        self.model.train()
        self.optimizer.zero_grad(set_to_none=True)  # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        
        # GPUë¡œ ë°ì´í„° ì´ë™
        circuit_specs = batch['circuit_specs']
        targets = {k: v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else v 
                  for k, v in batch['targets'].items()}
        
        # AMPë¥¼ ì‚¬ìš©í•œ Forward pass
        if self.use_amp:
            with torch.cuda.amp.autocast():
                outputs = self.model(circuit_specs)
                loss = self._compute_loss(outputs, targets)
        else:
            outputs = self.model(circuit_specs)
            loss = self._compute_loss(outputs, targets)
        
        # AMPë¥¼ ì‚¬ìš©í•œ Backward pass
        if self.use_amp:
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device.type == 'cuda' and torch.cuda.memory_allocated() > 0.8 * torch.cuda.max_memory_allocated():
            torch.cuda.empty_cache()
        
        return {
            'loss': loss.item(),
            'outputs': outputs
        }
    
    def _val_step_property_predictor(self, prepared_batch):
        """Property Predictor ê²€ì¦ ìŠ¤í… - í•™ìŠµ ìŠ¤í…ê³¼ ë™ì¼í•˜ì§€ë§Œ ê·¸ë˜ë””ì–¸íŠ¸ ì—†ìŒ"""
        circuit_data = prepared_batch['circuit_data']
        target_properties = prepared_batch['target_properties']
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        for key, value in circuit_data.items():
            if isinstance(value, torch.Tensor):
                circuit_data[key] = value.to(self.device)
        
        if target_properties is not None:
            target_properties = target_properties.to(self.device)
        
        # Forward pass
        outputs = self.model(circuit_data, mode='unified')
        
        # ì†ì‹¤ ê³„ì‚° - ê°œë³„ propertyë³„ ê°€ì¤‘ ì†ì‹¤
        if 'unified' in outputs:
            entanglement_pred = outputs['unified']['entanglement']
            expressibility_pred = outputs['unified']['expressibility'] 
            fidelity_pred = outputs['unified']['fidelity']
        else:
            # Property predictorëŠ” ì§ì ‘ property í‚¤ë¥¼ ë°˜í™˜
            entanglement_pred = outputs.get('entanglement', torch.zeros(1, 1, device=self.device))
            expressibility_pred = outputs.get('expressibility', torch.zeros(1, 1, device=self.device))
            fidelity_pred = outputs.get('fidelity', torch.zeros(1, 1, device=self.device))
        
        if target_properties is not None:
            # NaN ê²€ì‚¬ ë° ë””ë²„ê¹…
            has_nan = (torch.isnan(entanglement_pred).any() or torch.isnan(expressibility_pred).any() or 
                      torch.isnan(fidelity_pred).any() or torch.isnan(target_properties).any())
            
            if has_nan:
                print(f"\nâš ï¸ Validation NaN ê°ì§€! ì˜ˆì¸¡ê³¼ íƒ€ê²Ÿ ë””ë²„ê¹…:")
                print(f"  entanglement_pred NaN: {torch.isnan(entanglement_pred).any()}")
                print(f"  expressibility_pred NaN: {torch.isnan(expressibility_pred).any()}")
                print(f"  fidelity_pred NaN: {torch.isnan(fidelity_pred).any()}")
                print(f"  target_properties NaN: {torch.isnan(target_properties).any()}")
                print(f"  entanglement_pred range: {entanglement_pred.min().item():.6f} - {entanglement_pred.max().item():.6f}")
                print(f"  expressibility_pred range: {expressibility_pred.min().item():.6f} - {expressibility_pred.max().item():.6f}")
                print(f"  fidelity_pred range: {fidelity_pred.min().item():.6f} - {fidelity_pred.max().item():.6f}")
                
                # NaNì„ 0.5ë¡œ ëŒ€ì²´í•˜ì—¬ ì†ì‹¤ ê³„ì‚° ê³„ì†
                entanglement_pred = torch.nan_to_num(entanglement_pred, nan=0.5)
                expressibility_pred = torch.nan_to_num(expressibility_pred, nan=0.5)
                fidelity_pred = torch.nan_to_num(fidelity_pred, nan=0.5)
                target_properties = torch.nan_to_num(target_properties, nan=0.5)
                print(f"  NaNì„ 0.5ë¡œ ëŒ€ì²´í•˜ì—¬ ì†ì‹¤ ê³„ì‚° ê³„ì†")
            
            # íƒ€ê²Ÿì„ ê°œë³„ propertyë¡œ ë¶„í•  (entanglement, expressibility, fidelity ìˆœì„œ)
            target_entanglement = target_properties[:, 0:1]
            target_expressibility = target_properties[:, 1:2] 
            target_fidelity = target_properties[:, 2:3]
            
            # ê°œë³„ property ì†ì‹¤ ê³„ì‚°
            entanglement_loss = self.loss_fn(entanglement_pred, target_entanglement)
            expressibility_loss = self.loss_fn(expressibility_pred, target_expressibility)
            fidelity_loss = self.loss_fn(fidelity_pred, target_fidelity)
            
            # ê°€ì¤‘ì¹˜ ì ìš© (configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            entanglement_weight = getattr(self.config.training, 'entanglement_weight', 10.0)
            expressibility_weight = getattr(self.config.training, 'expressibility_weight', 1.0)
            fidelity_weight = getattr(self.config.training, 'fidelity_weight', 0.1)
            
            # ê°€ì¤‘ ì†ì‹¤ ê³„ì‚°
            weighted_entanglement_loss = entanglement_weight * entanglement_loss
            weighted_expressibility_loss = expressibility_weight * expressibility_loss
            weighted_fidelity_loss = fidelity_weight * fidelity_loss
            
            # ì´ ì†ì‹¤
            total_loss = weighted_entanglement_loss + weighted_expressibility_loss + weighted_fidelity_loss
            
            # ì†ì‹¤ ê°’ ê²€ì¦
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                raise RuntimeError(f"Invalid validation loss detected: {total_loss.item()}")
            
            return {
                'total_loss': total_loss.item(),
                'entanglement_loss': entanglement_loss.item(),
                'expressibility_loss': expressibility_loss.item(),
                'fidelity_loss': fidelity_loss.item(),
                'weighted_entanglement_loss': weighted_entanglement_loss.item(),
                'weighted_expressibility_loss': weighted_expressibility_loss.item(),
                'weighted_fidelity_loss': weighted_fidelity_loss.item(),
                'entanglement_pred': entanglement_pred,
                'expressibility_pred': expressibility_pred,
                'fidelity_pred': fidelity_pred,
                'entanglement_target': target_entanglement,
                'expressibility_target': target_expressibility,
                'fidelity_target': target_fidelity
            }
        else:
            print(f"âš ï¸ Warning: target_properties is None in validation batch - this should not happen!")
            print(f"   prepared_batch keys: {list(prepared_batch.keys())}")
            if 'target_properties' in prepared_batch:
                print(f"   target_properties type: {type(prepared_batch['target_properties'])}")
                print(f"   target_properties value: {prepared_batch['target_properties']}")
            
            # targetì´ ì—†ìœ¼ë©´ validationì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì—ëŸ¬ ë°œìƒ
            raise ValueError("Validation batch missing target_properties - cannot compute validation loss")

    def create_dataloaders(self, dataset_manager):
        """Create optimized dataloaders - GPU ìµœì í™”"""
        train_dataset, val_dataset, test_dataset = dataset_manager.create_datasets()
        
        # GPU ìµœì í™” DataLoader ì„¤ì •
        dataloader_kwargs = {
            'pin_memory': getattr(self.config.model, 'pin_memory', True) and self.device.type == 'cuda',
            'num_workers': getattr(self.config.model, 'num_workers', 4),
            'prefetch_factor': getattr(self.config.model, 'prefetch_factor', 2),
            'persistent_workers': True if getattr(self.config.model, 'num_workers', 4) > 0 else False
        }
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.training.train_batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            **dataloader_kwargs
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.training.val_batch_size,
            shuffle=False,
            collate_fn=collate_fn,
            **dataloader_kwargs
        )
        
        print(f"ğŸš€ DataLoader GPU optimizations: pin_memory={dataloader_kwargs['pin_memory']}, num_workers={dataloader_kwargs['num_workers']}")
        
        return train_loader, val_loader

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Unified Enhanced Experiment Runner")
    parser.add_argument("--config", type=str, default="medium", 
                       help="Configuration name (small, medium, large) or path to config file")
    parser.add_argument("--model", type=str, choices=["decision_transformer", "property_predictor"],
                       default="property_predictor", help="Model type to train")
    parser.add_argument("--embedding-mode", type=str, choices=["gnn", "hybrid"],
                       default="gnn", help="Embedding pipeline mode")
    parser.add_argument("--attention-mode", type=str, choices=["standard", "advanced"],
                       default="standard", help="Attention mechanism mode")
    parser.add_argument("--experiment-name", type=str, help="Custom experiment name")
    parser.add_argument("--data-path", type=str, help="Path to training data")
    
    args = parser.parse_args()
    
    # Load or create configuration
    config_manager = ConfigManager()
    
    if Path(args.config).exists():
        config = UnifiedTrainingConfig.load(args.config)
    else:
        config = get_config_by_name(args.config)
    
    # Apply command line overrides
    if args.experiment_name:
        config.experiment.experiment_name = args.experiment_name
    else:
        # ìë™ ì‹¤í—˜ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        config.experiment.experiment_name = f"enhanced_{args.model}_{args.embedding_mode}_{args.attention_mode}_{timestamp}"
    
    if args.data_path:
        config.data.data_path = args.data_path
    
    # ìƒˆë¡œìš´ ì„¤ì • ì¶”ê°€
    config.model.embedding_mode = args.embedding_mode
    config.model.attention_mode = args.attention_mode
    config.model.model_size = args.config  # Command line config maps to model size
    
    # Save the configuration
    config_path = Path(config.experiment.output_dir) / f"{config.experiment.experiment_name}_config.json"
    config.save(config_path)
    
    print(f"ğŸ“‹ Enhanced configuration saved to {config_path}")
    print(f"ğŸ¯ Training {args.model} with enhanced pipeline")
    print(f"ğŸ“Š Experiment: {config.experiment.experiment_name}")
    
    # Create enhanced runner
    runner = UnifiedExperimentRunner(config, args.model)
    
    # Load data and start training
    print(f"Loading data from {config.data.data_path}")
    
    if args.model == "property_predictor":
        # Create dataset manager and load data
        dataset_manager = DatasetManager(unified_data_path=config.data.data_path)
        
        # Split dataset
        train_quantum_dataset, val_quantum_dataset, test_quantum_dataset = dataset_manager.split_dataset(
            train_ratio=config.data.train_split,
            val_ratio=config.data.val_split,
            test_ratio=config.data.test_split
        )
        
        # Create datasets
        train_dataset = PropertyPredictionDataset(train_quantum_dataset)
        val_dataset = PropertyPredictionDataset(val_quantum_dataset)
        
        print(f"ğŸ“Š Data loaded: {len(train_dataset)} training samples, {len(val_dataset)} validation samples")
        
        # Create data loaders
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=config.training.train_batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=0
        )
        
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=config.training.val_batch_size,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=0
        )
        
        print(f"\nğŸš€ Starting enhanced training with {len(train_loader)} batches per epoch")
        
        # Start training
        runner.train(train_loader, val_loader)
        
    elif args.model == "decision_transformer":
        # Decision Transformer í•™ìŠµ ì„¤ì •
        from rtg.core.rtg_calculator import create_rtg_calculator
        
        # Property model weights ê²½ë¡œ ì„¤ì • (best model)
        property_model_path = "weights/best_model.pt"
        
        # RTG ê³„ì‚°ê¸° ìƒì„± (property model weights ì‚¬ìš©)
        rtg_calculator = create_rtg_calculator(
            checkpoint_path=property_model_path,
            property_weights={
                'entanglement': 1.0,
                'fidelity': 1.0, 
                'expressibility': 1.0
            },
            device=config.model.get_device() if hasattr(config.model, 'get_device') else 'cuda'
        )
        
        # Dataset manager ìƒì„±
        dataset_manager = DatasetManager(unified_data_path=config.data.data_path)
        
        # Decision Transformerìš© ë°ì´í„°ì…‹ ë¶„í• 
        train_quantum_dataset, val_quantum_dataset, test_quantum_dataset = dataset_manager.split_dataset(
            train_ratio=config.data.train_split,
            val_ratio=config.data.val_split,
            test_ratio=config.data.test_split
        )
        
        print(f"ğŸ“Š Decision Transformer Data: {len(train_quantum_dataset)} train, {len(val_quantum_dataset)} val")
        
        # Decision Transformer ì „ìš© ë°ì´í„°ë¡œë” ìƒì„±
        from training.trainer import QuantumCircuitCollator
        from data.embedding_pipeline import create_embedding_pipeline, EmbeddingConfig
        
        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì„¤ì • (í†µí•© ì„ë² ë”© ì‹œìŠ¤í…œ ì‚¬ìš©)
        embed_config = EmbeddingConfig(
            d_model=config.model.d_model,
            n_gate_types=20,  # Decision Transformerìš©
            max_seq_len=2000
        )
        embedding_pipeline = create_embedding_pipeline(embed_config)
        
        # ì½œë ˆì´í„° ì„¤ì • (RTG ê³„ì‚° í¬í•¨)
        collator = QuantumCircuitCollator(embedding_pipeline, rtg_calculator)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader = torch.utils.data.DataLoader(
            train_quantum_dataset,
            batch_size=config.training.train_batch_size,
            shuffle=True,
            collate_fn=collator,
            num_workers=0
        )
        
        val_loader = torch.utils.data.DataLoader(
            val_quantum_dataset,
            batch_size=config.training.val_batch_size,
            shuffle=False,
            collate_fn=collator,
            num_workers=0
        )
        
        print(f"\nğŸš€ Starting Decision Transformer training with RTG guidance")
        print(f"   Property model weights: {property_model_path}")
        print(f"   Training batches: {len(train_loader)}")
        
        # Decision Transformer í•™ìŠµ ì‹œì‘
        runner.train_decision_transformer(train_loader, val_loader, rtg_calculator)
    
    print("\nâœ… Enhanced training complete!")

if __name__ == "__main__":
    main()
