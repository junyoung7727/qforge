"""
Device Debugging Utilities
ë””ë°”ì´ìŠ¤ ë¯¸ìŠ¤ë§¤ì¹˜ ì—ëŸ¬ë¥¼ ì •í™•í•˜ê²Œ ë””ë²„ê¹…í•˜ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹°ë“¤
"""

import torch
import traceback
import functools
from typing import Dict, Any, Optional, Union, List
from contextlib import contextmanager


class DeviceTracker:
    """ë””ë°”ì´ìŠ¤ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device_log = []
        self.enabled = True
    
    def log_tensor_device(self, name: str, tensor: torch.Tensor, location: str = ""):
        """í…ì„œì˜ ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë¡œê¹…"""
        if not self.enabled:
            return
            
        device_info = {
            'name': name,
            'device': str(tensor.device),
            'shape': tuple(tensor.shape),
            'dtype': str(tensor.dtype),
            'location': location,
            'stack_trace': traceback.format_stack()[-3:-1]  # í˜¸ì¶œ ìœ„ì¹˜ ì¶”ì 
        }
        self.device_log.append(device_info)
        print(f"ğŸ” DEVICE DEBUG: {name} -> {tensor.device} {tuple(tensor.shape)} at {location}")
    
    def log_model_device(self, model: torch.nn.Module, name: str = "model"):
        """ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë¡œê¹…"""
        if not self.enabled:
            return
            
        try:
            model_device = next(model.parameters()).device
            print(f"ğŸ—ï¸ MODEL DEVICE: {name} -> {model_device}")
            return model_device
        except StopIteration:
            print(f"âš ï¸ MODEL DEVICE: {name} -> No parameters found")
            return None
    
    def clear_log(self):
        """ë¡œê·¸ ì´ˆê¸°í™”"""
        self.device_log.clear()
    
    def print_summary(self):
        """ë””ë°”ì´ìŠ¤ ë¡œê·¸ ìš”ì•½ ì¶œë ¥"""
        if not self.device_log:
            print("ğŸ“‹ No device operations logged")
            return
            
        print("\nğŸ“‹ DEVICE OPERATION SUMMARY:")
        print("=" * 60)
        
        devices = set()
        for entry in self.device_log:
            devices.add(entry['device'])
            print(f"  {entry['name']:20} | {entry['device']:10} | {entry['location']}")
        
        print(f"\nğŸ¯ Devices used: {', '.join(devices)}")
        if len(devices) > 1:
            print("âš ï¸ WARNING: Multiple devices detected - potential mismatch!")


# ì „ì—­ ë””ë°”ì´ìŠ¤ íŠ¸ë˜ì»¤
device_tracker = DeviceTracker()


def debug_tensor_device(tensor: torch.Tensor, name: str, location: str = "") -> torch.Tensor:
    """í…ì„œì˜ ë””ë°”ì´ìŠ¤ë¥¼ ë””ë²„ê¹…í•˜ê³  ë°˜í™˜"""
    device_tracker.log_tensor_device(name, tensor, location)
    return tensor


def debug_model_device(model: torch.nn.Module, name: str = "model") -> Optional[torch.device]:
    """ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤ë¥¼ ë””ë²„ê¹…í•˜ê³  ë°˜í™˜"""
    return device_tracker.log_model_device(model, name)


@contextmanager
def device_debug_context(name: str = "operation"):
    """ë””ë°”ì´ìŠ¤ ë””ë²„ê¹… ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    print(f"\nğŸš€ Starting device debug context: {name}")
    device_tracker.clear_log()
    
    try:
        yield device_tracker
    except RuntimeError as e:
        if "device" in str(e).lower():
            print(f"\nğŸ’¥ DEVICE ERROR in {name}: {e}")
            device_tracker.print_summary()
            print("\nğŸ” Error occurred at:")
            traceback.print_exc()
        raise
    finally:
        print(f"\nâœ… Ending device debug context: {name}")
        device_tracker.print_summary()


def validate_tensor_devices(*tensors, expected_device: Optional[torch.device] = None, 
                          names: Optional[List[str]] = None) -> bool:
    """ì—¬ëŸ¬ í…ì„œì˜ ë””ë°”ì´ìŠ¤ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦"""
    if not tensors:
        return True
    
    if names is None:
        names = [f"tensor_{i}" for i in range(len(tensors))]
    
    devices = []
    for i, tensor in enumerate(tensors):
        if isinstance(tensor, torch.Tensor):
            devices.append(tensor.device)
            device_tracker.log_tensor_device(names[i], tensor, "validation")
        else:
            print(f"âš ï¸ {names[i]} is not a tensor: {type(tensor)}")
    
    # ëª¨ë“  ë””ë°”ì´ìŠ¤ê°€ ê°™ì€ì§€ í™•ì¸
    if len(set(str(d) for d in devices)) > 1:
        print(f"âŒ DEVICE MISMATCH detected!")
        for name, device in zip(names, devices):
            print(f"  {name}: {device}")
        return False
    
    # ì˜ˆìƒ ë””ë°”ì´ìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    if expected_device is not None and devices:
        if str(devices[0]) != str(expected_device):
            print(f"âŒ DEVICE MISMATCH: Expected {expected_device}, got {devices[0]}")
            return False
    
    print(f"âœ… All tensors on same device: {devices[0] if devices else 'None'}")
    return True


def device_safe_operation(func):
    """ë””ë°”ì´ìŠ¤ ì•ˆì „ ì—°ì‚°ì„ ìœ„í•œ ë°ì½”ë ˆì´í„°"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        func_name = f"{func.__module__}.{func.__name__}"
        
        # ì…ë ¥ í…ì„œë“¤ì˜ ë””ë°”ì´ìŠ¤ ë¡œê¹…
        tensor_args = [arg for arg in args if isinstance(arg, torch.Tensor)]
        tensor_kwargs = {k: v for k, v in kwargs.items() if isinstance(v, torch.Tensor)}
        
        print(f"\nğŸ”§ Calling {func_name}")
        for i, tensor in enumerate(tensor_args):
            device_tracker.log_tensor_device(f"arg_{i}", tensor, func_name)
        
        for name, tensor in tensor_kwargs.items():
            device_tracker.log_tensor_device(f"kwarg_{name}", tensor, func_name)
        
        try:
            result = func(*args, **kwargs)
            
            # ê²°ê³¼ í…ì„œì˜ ë””ë°”ì´ìŠ¤ ë¡œê¹…
            if isinstance(result, torch.Tensor):
                device_tracker.log_tensor_device("result", result, func_name)
            elif isinstance(result, (list, tuple)):
                for i, item in enumerate(result):
                    if isinstance(item, torch.Tensor):
                        device_tracker.log_tensor_device(f"result_{i}", item, func_name)
            elif isinstance(result, dict):
                for key, value in result.items():
                    if isinstance(value, torch.Tensor):
                        device_tracker.log_tensor_device(f"result_{key}", value, func_name)
            
            return result
            
        except RuntimeError as e:
            if "device" in str(e).lower():
                print(f"\nğŸ’¥ DEVICE ERROR in {func_name}: {e}")
                device_tracker.print_summary()
            raise
    
    return wrapper


def enable_device_debugging():
    """ë””ë°”ì´ìŠ¤ ë””ë²„ê¹… í™œì„±í™”"""
    device_tracker.enabled = True
    print("ğŸ” Device debugging enabled")


def disable_device_debugging():
    """ë””ë°”ì´ìŠ¤ ë””ë²„ê¹… ë¹„í™œì„±í™”"""
    device_tracker.enabled = False
    print("ğŸ”‡ Device debugging disabled")


# í¸ì˜ í•¨ìˆ˜ë“¤
def check_device(tensor: torch.Tensor, name: str = "tensor") -> str:
    """í…ì„œì˜ ë””ë°”ì´ìŠ¤ë¥¼ í™•ì¸í•˜ê³  ì¶œë ¥"""
    device = str(tensor.device)
    print(f"ğŸ“ {name}: {device} {tuple(tensor.shape)}")
    return device


def move_to_device(tensor: torch.Tensor, device: torch.device, name: str = "tensor") -> torch.Tensor:
    """í…ì„œë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•˜ë©° ë””ë²„ê¹…"""
    old_device = tensor.device
    new_tensor = tensor.to(device)
    print(f"ğŸšš Moving {name}: {old_device} -> {device}")
    return new_tensor


def ensure_same_device(*tensors, target_device: Optional[torch.device] = None, 
                      names: Optional[List[str]] = None) -> List[torch.Tensor]:
    """ëª¨ë“  í…ì„œë¥¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
    if not tensors:
        return []
    
    if names is None:
        names = [f"tensor_{i}" for i in range(len(tensors))]
    
    # íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤ ê²°ì •
    if target_device is None:
        target_device = tensors[0].device
    
    result = []
    for tensor, name in zip(tensors, names):
        if tensor.device != target_device:
            print(f"ğŸšš Moving {name}: {tensor.device} -> {target_device}")
            result.append(tensor.to(target_device))
        else:
            result.append(tensor)
    
    return result
